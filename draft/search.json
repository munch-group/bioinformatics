[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bioinformatics and Programming",
    "section": "",
    "text": "Welcome\nThis is the homepage for the AU course Bioinformatics and programming (Bioinformatik og programmering). You will find all course content here. The Brightspace course page is only used for communication, and assignments.\n\n\nSchedule\n\nWeek 1Week 2Week 3Week 4Week 5Week 6Week 7Week 8Week 9Week 10Week 11Week 12Week 13Week 14\n\n\nReading:\n\nLecture notes: Chapter 2, Chapter 3, Chapter 4, Chapter 5, Chapter 6, Chapter 7\n\nMake sure you have installed Python and VScode for the first lecture.\nLectures:\n\nIn the first lecture, I will outline how the course is organized and how you will get the most out of your efforts in learning programming.\nIn the first lecture, I will also talk about how a Python program works and about values, math, and logic. \nThe second lecture is replace by the video below:\n\n\nExercises:\nIf you have yet to do so at home, you will install Python and the text editor. To do this, follow the instructions in Chapter 3. Then, start doing the exercises in Chapter 4, Chapter 5, and Chapter 6. You will also have time to do these exercises in the TA session of week two, so go slow. It is important to properly absorb the basic concepts at the beginning of the course; otherwise, it will become too difficult later on. Have a look\nAnd make sure to familiarize yourself with the Myiagi game and the pysteps tool described in Chapter 7. These are useful companions in the course.\n\n\nWeek 2 went up in flames\n\n\n\nReading:\n\nLecture notes: Chapter 8, Chapter 9\n\nI will cover chapters 8-9 in the lecture notes.\nLectures:\n\nIn the first lecture, I will talk about how to use logic to control which statements in your program that get executed.\nIn the first lecture, I will also talk about how you can efficiently organize your code using functions.\nIn the second lecture, I will talk more about functions.\n\nExercises:\nThe topics for this week’s exercises are statements, variables, operators, expressions, substitution, reduction, and logic. You will work on the rest of the exercises in Chapter 4, Chapter 5, Chapter 6, and Chapter 7. Do what you can at home and do the rest at the TA session.\n\n\n\nReading:\n\nLecture notes: Chapter 10, Chapter 11, Chapter 12, Chapter 13\n\nLectures:\n\nIn the first lecture, I will talk about objects and methods.\nIn the first lecture, I will also talk about lists.\nIn the second lecture, I will talk about dictionaries and a bit about tuples.\n\nExercises:\n\n\n\n\n\n\nImportantLab week\n\n\n\nOnly MO and Bio classes attend the exercises this week. MM classes do not. The exercise is repeated next week for the MM classes to attend\n\n\nYou are meant to work on the exercises in Chapter 8, Chapter 9, Chapter 10, Chapter 11, Chapter 12, and Chapter 13. Do what you can at home and do the rest at the TA session.\n\n\n\nReading:\n\nLecture notes: Chapter 14, Chapter 15\n\nLectures:\n\nIn the first lecture, I will talk about iteration and lists.\nIn the second lecture, I will talk about how your code can interact with files on your computer.\n\nExercises:\n\n\n\n\n\n\nImportantLab week\n\n\n\nOnly MM classes attend the exercises this week. MO and Bio classes do not. The exercise is repeated from last week*\n\n\n\n\n\nReading:\n\nLecture notes: Chapter 16, Chapter 17, Chapter 18\nChapter 11: Genomewide Association Studies\nBenefits and limitations of genomewide association studies\n\nLectures:\n\nIn the first lecture, I will talk about databases, genotyping arrays, and genomewide association studies (GWAS).\nIn the first lecture, I will also talk about building simple data strutures in Python.\nIn the second lecture, I will talk about how to use recursion in Python.\n\nExercises:\nThe topics for this week are iteration and data structures. You are meant to complete all the exercises in Chapter 14 and Chapter 15.\n\n\n\nReading:\n\nLecture notes: Chapter 28\nUnderstanding Bioinformatics 127-141\nLecture notes: Chapter 37\n\nLectures:\n\nIn the first lecture, I will talk about global pairwise alignment In the first lecture\nIn the first lecture, I will also talk about the weekly programming project.\nIn the second lecture, I will talk about local pairwise alignment and more realistic gap scoring.\n\nExercises:\n\nThe programming project described in Chapter 28.\n\nFrom the project files page, you can download the files you need for programming projects. There will be lots of work, so do what you can at home and do the rest at the TA session.\n\n\n\n\n\n\nTipMandatory assignment\n\n\n\nChapter 28 is a mandatory assignment. The deadline for handing it in (on Brightspace) is the night before your exercise class in week 43.\n\n\n\n\n\nReading:\n\nLecture notes: Chapter 34\nUnderstanding Bioinformatics: 117-127\nAlignment methods: strategies, challenges, benchmarking, and comparative overview (don’t do the exercises).\n\nLectures:\n\nIn the first lecture, I will talk about protein substitution matrices and how to score protein alignments. - In the first lecture, I will also talk Python classes and the weekly programming project.\nIn the second lecture, I will talk about multiple alignment.\n\nExercises:\n\nThe web exercise: GWAS candidates\nThe programming project described in Chapter 34 (not a mandatory assignment).\n\nFrom the project files page, you can download the files you need for both programming projects and web exercises. There will be lots of work, so do what you can at home and do the rest at the TA session.\n\n\n\nReading:\n\nLecture notes: Chapter 39\nBioinformatics Explained: BLAST\nBiological Sequence Analysis pp. 192-197\n\nLectures:\n\nIn the first lecture, I will talk about how to search for matches in a sequence database and how to asses alignment significance.\nIn the first lecture, I will also talk about programming topics and the weekly programming project.\nIn the second lecture, I will talk about models of DNA evolution and how to measure evolutionary distance between sequences.\n\nExercises:\n\nThe web exercise: CCR5-delta32\nThe programming project described in Chapter 39 (not a mandatory assignment).\n\nFrom the project files page, you can download the files you need for both programming projects and web exercises. There will be lots of work, so do what you can at home and do the rest at the TA session.\n\n\n\nReading:\n\nLecture notes: ?sec-codonbiasproject\nBiological Sequence Analysis pp. 165-179 \n\nLectures:\n\nIn the first lecture, I will talk about methods for sequence clustering.\nIn the first lecture, I will also talk about the programming project.\nIn the second lecture, I will talk about bioinformatics code libraries for Python, such as BioPython, and the Master in Bioinformatics that we offer at the Bioinformatics Centre.\n\nExercises:\n\nThe web exercise: MRSA\nThe programming project described in ?sec-codonbiasproject (not a mandatory assignment).\n\nFrom the project files page, you can download the files you need for both programming projects and web exercises. There will be lots of work, so do what you can at home and do the rest at the TA session.\n\n\n\nReading:\n\nLecture notes: Chapter 44\nBiological Sequence Analysis pp. 192-202 \n\nLectures:\n\nIn the first lecture, I will talk about phylogenetic trees.\nIn the first lecture, I will also talk about Python topics and the weekly programming project.\nIn the second lecture, I will talk about gene prediction in prokaryotes.\n\nExercises:\n\nThe web exercise: Aardvark?\nThe programming project described in Chapter 44.\n\nFrom the project files page, you can download the files you need for both programming projects and web exercises. There will be lots of work, so do what you can at home and do the rest at the TA session.\n\n\n\n\n\n\nTipMandatory assignment\n\n\n\nChapter 44 is a mandatory assignment. The deadline for handing it in is the night before your exercise class in week 47.\n\n\n\n\n\nReading:\n\nLecture notes: Chapter 47\nBiological Sequence Analysis pp. 46-66 \n\nLectures:\n\nIn the first lecture, I will talk about hidden Markov models (HMMs).\nIn the first lecture, I will also talk about python topics and the weekly programming project.\nIn the second lecture, I will talk about more about HMMs\n\nExercises:\n\nThe programming project described in Chapter 47 (not a mandatory assignment).\n\nFrom the project files page, you can download the files you need for both programming projects and web exercises. There will be lots of work, so do what you can at home and do the rest at the TA session.\n\n\n\nReading:\n\nLecture notes: Chapter 31\nUnderstanding Bioinformatics pp. 438-448 \nAutomatic generation of gene finders for Eukaryotic species\nThe Theory and Practice of Genome Sequence Assembly \n\n\n0.0.0.0.1 Lectures\n\nIn the first lecture, I will talk about applications of hidden Markov models gene finding and protein annotation.\nIn the first lecture, I will also talk about Python topics and the weekly programming project.\nIn the second lecture, I will talk genome assembly.\n\nExercises:\n\nThe web exercise: Plasmid ORFs\nThe programming project described in Chapter 31 (not a mandatory assignment).\n\nFrom the project files page, you can download the files you need for both programming projects and web exercises. There will be lots of work, so do what you can at home and do the rest at the TA session.\n\n\n\nReading:\n\nLecture notes: Chapter 36\nUnderstanding Bioinformatics pp. 494-496\nExploring Bioinformatics pp. 242-248\n\nLectures:\n\nIn the first lecture, I will talk about neural networks\nIn the first lecture, I will also talk about the programming project. \n\nExercises:\n\nThe web exercise: Read mapping\nThe programming project described in Chapter 36 (not a mandatory assignment).\nIn the last lecture, we will evaluate the course and review the exam’s practicalities.\n\nFrom the project files page, you can download the files you need for both programming projects and web exercises. There will be lots of work, so do what you can at home and do the rest at the TA session.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Exam information</span>"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Course description\nAfter the course, the participants will have basic knowledge of computer methods and applications for analyzing biological sequence data and insight into principles and techniques for constructing simple programs. Participants will acquire practical experience with analyzing problems in bioinformatics and related fields and implementing programs to solve such problems using the Python programming language.\nThe participants must, at the end of the course, be able to:",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#course-description",
    "href": "about.html#course-description",
    "title": "About",
    "section": "",
    "text": "Apply fundamental constructs of a programming language.\nAnalyse data and construct data structures for the representation of data.\nAnalyse simple computational problems and construct programs for their solution.\nDescribe and relate essential methods in bioinformatics analysis.\nApply bioinformatics software to biological data.\nJudge the reliability of results obtained using Bioinformatics software.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#course-contents",
    "href": "about.html#course-contents",
    "title": "About",
    "section": "Course contents",
    "text": "Course contents\nThe course introduces programming and its practical applications in bioinformatics. The course also outlines and discusses bioinformatics algorithms, and the most common tools for bioinformatics analysis of sequence data are presented and demo nstrated. The participant will acquire and train basic programming skills during the first seven weeks. The last seven weeks introduce key topics in bioinformatics, focusing on applying bioinformatical software and developing programming skills. Subjects for lectures and exercises include bioinformatics databases, sequence alignment, genome annotation, sequence evolution, and phylogenetic analysis.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "curriculum.html",
    "href": "curriculum.html",
    "title": "Curriculum",
    "section": "",
    "text": "The curriculum for programming is the lecture notes on these pages (a PDF version is available by clicking the PDF icon in the top left corner). I will develop my own lecture notes and add them to this page as the course progress.\nBioinformatics is a rapidly developing field, so textbooks often have parts that need updating. So, in this course, the curriculum for bioinformatics is put together from several sources. We use the best excerpts from the textbooks Understanding Bioinformatics by Jeremy Baum and Marketa J. Zvelebil, Biological Sequence Analysis by Richard Durbin et al., and Exploring Bioinformatics by Caroline St. Clair and Jonathan E. Visick. I have collected the sections from each textbook in separate compendia. You will also be reading short, up-to-date publications on selected topics in bioinformatics.\nBelow are links to material covering the bioinformatics topics we treat in the course. In the weekly notes, you can see what you need to read to prepare for each lecture. You can also find any curriculum related to the exercises in the weekly note.\nCompendia you can download here:\n\nCompendium of selections from Understanding Bioinformatics\nCompendium of selections from Biological Sequence Analysis\nCompendium of selections from Exploring Bioinformatics\n\nLinks to material you need to download yourself (you may need to be on campus or use your student VPN to download from publisher websites):\n\nChapter 11: Genome-Wide Association Studies\nBenefits and limitations of genome-wide association studies\nThe Theory and Practice of Genome Sequence Assembly\nAlignment methods: strategies, challenges, benchmarking, and comparative overview\nBioinformatics explained: BLAST (CLCbio)\nAutomatic generation of gene finders for Eukaryotic species\n\nCurriculum on webpages (not PDF format):\nUsing neural nets to recognize handwritten digits (if you have trouble accessing the page, download this zip file with the HTML content, unzip it, and view it locally).",
    "crumbs": [
      "Curriculum"
    ]
  },
  {
    "objectID": "chapters/python/preface.html",
    "href": "chapters/python/preface.html",
    "title": "2  Preface",
    "section": "",
    "text": "These lecture notes are inspired by the many books and resources I have used in this course. “Learning Python the Hard Way” and “How to Think Like a Computer Scientist” have inspired my own notes on programming. These implement the following ideas, which I think best support learning in an introduction to programming:\n\nEach topic and concept is introduced so that it can be applied immediately on top of what you know.\nThe introduction of each topic covers only the most basic facts and rules required to learn the rest through practical exercises.\n\nI would like to improve these notes as much as I can. If you find errors in exercises, poorly explained concepts, redundant information, missing items, or exercises that would work better in a different order, please let me know. You can do this by reporting an issue.\nHappy coding, Kasper Munch",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "chapters/python/before_you_begin.html",
    "href": "chapters/python/before_you_begin.html",
    "title": "3  Before you begin",
    "section": "",
    "text": "Install Python\nThis chapter serves to get the practicalities out of the way so you can start programming. Read the whole chapter once carefully before you install anything\nIn this course, we use the Python programming language and you need to install the Python program to run the code we will write. I have automated the installation procedure, which include a few other tools that you will also need later in the course:\nIf you see any red text during the installation, the installation was not successful. In that case take a screenshot and send it to kaspermunch@birc.au.dk.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Before you begin</span>"
    ]
  },
  {
    "objectID": "chapters/python/before_you_begin.html#install-python",
    "href": "chapters/python/before_you_begin.html#install-python",
    "title": "3  Before you begin",
    "section": "",
    "text": "Mac Windows\n\n\n\nClick the clipboard icon at the right end of the box below to copy the command to your clipboard.\n\n\ncurl -fsSL http://munch-group.org/franklin-cli/installers/scripts/install-pixi.sh | bash -s -- --force\n\nOpen your Terminal application.\nPaste the command into the Terminal window and press Enter. You will be prompted several times for either your user password or permissions of the installed apps.\nRestart your Terminal app and then run this command:\n\npixi global install -c conda-forge -c munch-group -c sepandhaghighi python pythonsteps\n\n\n\nClick the clipboard icon at the right end of the box below to copy the command to your clipboard.\n\n powershell -ExecutionPolicy ByPass -c \"irm -useb  http://munch-group.org/franklin-cli/installers/scripts/Install-Pixi.ps1\" &gt; installer.ps1 ; dir \"$env:USERPROFILE/Downloads/*.ps1\" | Unblock-File ; powershell -ExecutionPolicy ByPass -File \"installer.ps1\" -Force -Quiet\n\nFind Windows PowerShell and open it by right-clicking it and select “Run as administrator”.\nPaste the command into the PowerShell window and press Enter. You may be prompted several times to allow the app to make changes to your computer.\nOnce the Pixi installation has completed successfully, you need to restart the Windows Powershell application.\nYou can install Python by opening Windows Powershell and paste the command below into the window and press enter.\n\npixi global install -c conda-forge -c munch-group -c sepandhaghighi python pythonsteps",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Before you begin</span>"
    ]
  },
  {
    "objectID": "chapters/python/before_you_begin.html#the-text-editor",
    "href": "chapters/python/before_you_begin.html#the-text-editor",
    "title": "3  Before you begin",
    "section": "The text editor",
    "text": "The text editor\nYou will also need a text editor. A text editor is where you write your Python code. For this course, we will use Visual Studio Code - or VScode for short. You can download it from this page. If you open VScode, you should see something like Figure 3.1. You may wonder why we cannot use Word to create and edit files with programming code. The reason is that a text editor made for programming, such as VScode, only saves the actual characters you type. So, unlike Word, it does not silently save all kinds of formatting, like margins, boldface text, headers, etc. With VScode, what you type is exactly in the file when you save it. In addition, where Word is made for prose, VScode is made for programming and has many features that will make your programming life easier.\n\n\n\n\n\n\nFigure 3.1: Visual Studio Code (VScode)",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Before you begin</span>"
    ]
  },
  {
    "objectID": "chapters/python/before_you_begin.html#the-terminal",
    "href": "chapters/python/before_you_begin.html#the-terminal",
    "title": "3  Before you begin",
    "section": "The terminal",
    "text": "The terminal\nThe last thing you need is a tool to make Python run the programs you write. Fortunately, that is already installed. On OSX, this is an application called Terminal. You can find it by typing “Terminal” in Spotlight Search. When you start, you will see something like Figure 3.2. You may be presented with the following text:\nThe default interactive shell is now zsh.\nTo update your account to use zsh, please run `chsh -s /bin/zsh`.\nFor more details, please visit https://support.apple.com/kb/HT208050.\n\n\n\n\n\n\nFigure 3.2: The Terminal app on Mac\n\n\n\n\n\n\n\n\n\nFigure 3.3: Windows Powershell app on Windows\n\n\n\nDo not update your account. If you do, Terminal will not be able to find the Python you install (If you did so by mistake, you change back using this command: chsh -s /bin/bash).\nOn Windows, the tool you need is called the Windows Powershell. You should be able to find it from the Start menu. Ensure you open Windows Powershell and not some other shell (The name should be exactly Windows Powershell). When you do, you should see something like Figure 3.3.\nWhat is Windows Powershell and this Terminal thing? Both programs are what we call terminal emulators. They are used to run other programs, like the ones you will write yourself. I will informally refer to both Terminal and Windows Powershell as “the terminal.” So if I write something like “open the terminal,” you should open Windows Powershell if you are running Windows and the Terminal application if you are running OS X.\nThe terminal is a very useful tool. However, to use it, you need to know a few basics. First of all, a terminal lets you execute commands on your computer. You type the command you want and then hit enter. The place where you type is called a prompt (or command prompt), and it may look a little different depending on which terminal emulator you use. In this book, we represent the prompt with the character $. So, a command in the examples below is the line of text to the left of the $. When you open the terminal, you’ll be redirected to a folder. You can see which folder you are in by typing pwd, and then press Enter on the keyboard. When you press Enter, you tell the terminal to execute your written command. In this case, the command you typed tells you the path to the folder we are in. If I do it, I get:\n\n\nTerminal\n\n$ pwd\n/Users/kasper/programming\n\nIf I had been on a Windows machine, it would have looked something like this:\n\n\nTerminal\n\n$ cd\nC:\\Users\\kasper\\programming\n\nSo, right now, I am in the programming folder. /Users/kasper/programming is the folder’s path or “full address” with dashes (or backslashes on Windows) separating nested folders. So programming is a subfolder of kasper, a subfolder of Users. That way, you know which folder you are in and where that folder is. Let us see what is in this folder. You can use the ls command (l as in Lima and s as in Sierra). When I do that and press Enter I get the following:\n\n\nTerminal\n\n$ ls\nnotes projects\n\nThere seem to be two other folders, one called notes and another called projects. If you are curious about what is inside the notes folder, you can “walk” into the folder with the cd command. To use this command, you must specify which folder you want to walk into (in this case, notes). We do this by typing cd, then a space, and then the folder’s name. When I press Enter I get the following:\n\n\nTerminal\n\n$ cd notes\n$\n\nIt seems that nothing really happened, but if I run the pwd command now to see which folder I am in, I get the following:\n\n\nTerminal\n\n$ pwd\n/Users/kasper/programming/notes\n\nJust to keep track of what is happening: before we ran the cd command, we were in the directory /Users/kasper/programming folder, and now we’re in /Users/kasper/programming/notes. This means that we can now use the ls command to see what is in the notes folder:\n\n\nTerminal\n\n$ ls\n$\n\nAgain, it seems like nothing happened. Well, ls and dir do not show anything if the folder we are in is empty. So notes must be empty. Let us go back to where we came from. To walk “back” or “up” to /Users/kasper/programming, we again use the cd command, but we do not need to name a folder this time. Instead, we use the special name .. to say that we wish to go to the parent folder called programming, i.e., the folder we just came from:\n\n\nTerminal\n\n$ cd ..\n$ pwd\n/Users/kasper/programming\n\nWhen we run the pwd command, we see that we are back where we started. Let us see if the two folders are still there:\n\n\nTerminal\n\n$ ls\nnotes projects\n\nThey are!\nHopefully, you can now navigate your folders and see what is in them. You will need this later to access the folders with the code you write for the exercises and projects during the course.\n\n\n\nAction\nOSX\n\n\n\n\nShow current folder\npwd\n\n\nList folder content\nls\n\n\nGo to subfolder “notes”\ncd notes\n\n\nGo to parent folder\ncd ..",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Before you begin</span>"
    ]
  },
  {
    "objectID": "chapters/python/before_you_begin.html#you-are-all-set",
    "href": "chapters/python/before_you_begin.html#you-are-all-set",
    "title": "3  Before you begin",
    "section": "You are all set ",
    "text": "You are all set \nWell done! You are all set to start the course. Have a cup of coffee, and look forward to your first program. While you sip your coffee, I need you to take an oath (one of three you will take during this course). Raise your right hand! (put your coffee down first).\n\n\n\n\n\n\nImportantOath 1\n\n\n\nI swear never to copy and paste code examples from this book into my text editor. I will always read the examples and then type them into my editor.\n\n\nThis serves three purposes (as if one was not enough):\n\nYou will be fully aware of every bit of each example.\nYou will learn to write code correctly and without omissions and mistakes.\nYou will get Python “into your fingers”. It sounds silly, but it will get into your fingers.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Before you begin</span>"
    ]
  },
  {
    "objectID": "chapters/python/writing_a_program.html",
    "href": "chapters/python/writing_a_program.html",
    "title": "4  Writing a program",
    "section": "",
    "text": "Hello World\nLet’s get you started…\nDive in and make your first program. Create a new file in your editor (VScode) and save it as hello.py. The .py suffix tells your editor that this file contains Python code. As you will see, this makes your life a whole lot easier. Such a file with Python code is usually called a script, but we can also call it a program.\nNow write exactly this in the file (hello.py):\nYour editor will color your code differently, but that is unimportant. Save your file with the added code and have your first program! Of course, there is not much point in having a program if it just sits there on your computer. To run your program, do the following:\nYou should see something like Figure 4.1.\nThis is where you shout, “It’s alive!” toss your head back, and do the insane scientist laugh.\nOkay, what just happened? You wrote a program by creating a file in which you wrote one line of code. You then ran the program using Python, and it wrote (printed) Hello world in the terminal. Do not worry about the parentheses and quotes for now; just enjoy your new life as a programmer.\nMaybe you wonder why we write print and not write or something else. That goes back to when computers were enormous, clunky things with no screens attached. They could only interact with the user by printing things on an actual physical paper printer. Back then, the output you now see on the screen was printed onto a piece of paper that the programmer could then look at. These days, print shows up in the terminal, but the story should help you remember that print spits text out of your program just like a printer.\nNow try to add another line of code like this:\nSave the file and run it again by typing python hello.py into your terminal and hitting Enter.\nYou should see this:\nNow your program prints Hello world and then prints I am your first program. The then-part is important. That is how a Python program works. Python (the python you write in front of hello.py) reads your hello.py file and then executes the code, one line after another, until it reaches the end of the file. This is essential, so reread from the beginning of the paragraph. Now, read it once more. It may seem trivial, but it is fundamental to remember that this is how Python runs your code. So here is Oath 2:\nWhen you write Python code, you always follow this workflow:\nMake sure you get the hang of this in the following exercises.\nImportant: The examples and exercises in this course are designed to work if you execute your scripts from their stored folder. So, you’ll need to go ahead and navigate the relevant folder before you’re done with your script. If your script is called hello.py, you must always execute it precisely like this: python hello.py. If you do it any other way, you may use a different Python version than you think. On some computers, it is possible to type hello.py without python in front of it. Don’t do that. Do not “drag” the script file into the Terminal either. In VScode, you can press a small play icon on the top right to execute the code. Don’t do that either. There are many other things you should not do, but you get the drift.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Writing a program</span>"
    ]
  },
  {
    "objectID": "chapters/python/writing_a_program.html#hello-world",
    "href": "chapters/python/writing_a_program.html#hello-world",
    "title": "4  Writing a program",
    "section": "",
    "text": "print(\"Hello world\")\n\n\nOpen the terminal and navigate to the folder (directory) where you saved hello.py. Use the cd command to do so. If you do not remember how, reread the previous chapter.\nType python hello.py in the terminal and hit Enter.\n\n\n\n\n\n\n\n\nFigure 4.1: Hello world\n\n\n\n\n\n\n\nprint(\"Hello world\")\nprint(\"I am your first program\")\n\n\nHello world\nI am your first program\n\n\n\n\n\n\n\nImportantOath 2\n\n\n\nI swear to always remember that each line of code in my script is executed one after another, starting from the first line and ending at the last line.\n\n\n\n\nChange the code in the file.\nSave the file.\nExecute the code in the file using the terminal.\nStart over.\n\n\n\n\nExercise 4-1\nTry swapping the two lines of code in the file and rerun the program. What does it print now?\n\n\nExercise 4-2\nTry to make the program print a greeting to yourself. Something like this:\nHello Sarah!\n– if your name is Sarah, of course.\n\n\nExercise 4-3\nAdd more lines of code to your program to make it print something else. Can you make your program print the same thing ten times?",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Writing a program</span>"
    ]
  },
  {
    "objectID": "chapters/python/writing_a_program.html#error-messages",
    "href": "chapters/python/writing_a_program.html#error-messages",
    "title": "4  Writing a program",
    "section": "Error Messages",
    "text": "Error Messages\nDid you get everything right with your first program, or did you get error messages when executing your code with python? Maybe you wrote the following code (adding an extra closing parenthesis):\nprint(\"Hello world\"))\n– and then got an error like this:\n  File \"hello.py\", line 1\n    print('Hello world'))\n                        ^\nSyntaxError: invalid syntax\nThis is Python’s way of telling you that the hello.py script has an error in line 1. If you write something that does not conform to the proper syntax for Python code, you will get a SyntaxError. Python will do its best to figure out where the problem is and point to it with a ^ character.\nYou will see many error messages in your new life as a programmer. So you must practice reading them. At first, they will be hard to decipher, but once you get used to them, they will help you quickly identify where the problem is. If there is an error message that you do not understand, the internet is your friend. Just paste the error message into Google’s search field, and you will see that you are not the only one on the internet getting started on Python programming. It is okay if you do not know how to fix the problem right now, but it is essential to remember that these error messages are Python’s way of helping you understand what you did wrong.\n\nExercise 4-4\nTry to break your new shiny program and make it produce an error message when you run it. An easy way of doing this is to remove or change random characters from the program. If you run this (with a missing end-parenthesis:\nprint(\"Hello world\"\nprint(\"I am your first program\")\nYou will get this error:\n  File \"hello.py\", line 2\n    print(\"I am your first program\")\n        ^\nSyntaxError: invalid syntax\nThe ^ character tells you when your code stops making sense to Python. Sometimes, that is a bit after where you made your mistake.\nTry to make other kinds of errors. Which error messages do you see? Do you always see the same error message, or are they different? Try googling the error messages you get. Can you figure out why the change you made broke the program? How many other error messages can you produce?",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Writing a program</span>"
    ]
  },
  {
    "objectID": "chapters/python/writing_a_program.html#strings",
    "href": "chapters/python/writing_a_program.html#strings",
    "title": "4  Writing a program",
    "section": "Strings",
    "text": "Strings\nIn programs, text values are called strings, and you have already used strings a lot in your first program. A string is simply a text, but we call it a string because it is a “string of characters”. In Python, we represent a string like this:\n\"this is a string\"\nor like this:\n'Hello world'\nThat is, we take the text we wish to use as our value and put quotes around it. We can use double quotes (the first example) or single quotes (the second example). We can mix them like this:\nprint('this is \"some text\" with a quote')\n– but not like this:\nprint(\"this is a broken string')\nCan you see why and how handy it is to have single and double quotes? If we did not have both, we could not have text with quotes. However, you must use the same kind of quotes at each end of the string. Running the latter example gives an error message telling you that Python cannot find the quote that was supposed to end the string:\n  File \"broken.py\", line 1\n    print(\"this is a broken string')\n                                   ^\nSyntaxError: EOL while scanning string literal\nIt is Python’s way of saying, ” I reached the end of the line (EOL) without finding a matching end quote.”",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Writing a program</span>"
    ]
  },
  {
    "objectID": "chapters/python/writing_a_program.html#comments",
    "href": "chapters/python/writing_a_program.html#comments",
    "title": "4  Writing a program",
    "section": "Comments",
    "text": "Comments\nYou have already learned that Python reads and executes one line of code at a time until your program has no more lines of code.\nHowever, we can make a line invisible to Python by putting a # symbol in front of it, like this:\n# print(\"Hello world\")\nprint(\"Greetings from your first program\")\nWhen you do that, Python does not read that line. It is not part of the program. Rerun the program. You will notice that now only the second line is part of your program:\n$ python hello.py\nGreetings from your first program\nThis is useful in two ways:\n\nIt lets you disable certain lines of your code by keeping Python from reading them. For example, see what happens if that line of code is not executed to understand how your program works.\nIt allows you to write notes in your Python code to help you remember how it works.\n\nLines with a # in front are called “comments” because we usually use them to write comments about our code. If you ask for help with some problem, you will often hear your instructor say: try to “comment out” in line two. When your instructor says that, it simply means that you should add a # in front of line two to see what then happens.\n\nExercise 4-5\nWhat happens if I put a # in the middle of a line of code? Try it out!\n\n\nExercise 4-6\nTry this:\n# Note to self: the lines below print stuff\nprint(\"Hello world\")\nprint(\"Greetings from your first program\")\n\n\nExercise 4-7\nTry this:\nprint(\"Hello world\") # actually, everything after a # is ignored\nprint(\"Greetings from your first program\")\nWhat did you learn? Which parts of each line are considered part of the program?\n\n\nExercise 4-8\nNow try this:\nprint(\"Hello # world\")\nprint(\"Greetings from your first program\")\nWhat did you learn about # characters in strings? #### Exercise Try this:\nprint(\"Hello world\"#)\nprint(\"Greetings from your first program\")\nDid you expect this to work? Why? Why not? What error message did you get?",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Writing a program</span>"
    ]
  },
  {
    "objectID": "chapters/python/dealing_with_values.html",
    "href": "chapters/python/dealing_with_values.html",
    "title": "5  Dealing with values",
    "section": "",
    "text": "Math\nThis chapter is about values and variables, the two most central concepts in programming.\nMuch programming is done to compute stuff. In Python, the usual math operations are done using these arithmetic operators:\nYou are probably quite familiar with these - except perhaps for integer division, exponentiation, and modulo. Let us take some of the operators for a spin. Remember to carefully write the whole thing in an empty file in VScode. Do not copy-paste. Then save the file as mathandlogic.py and run it from the terminal. Do not call your file math.py. It may bite you later. Just trust me on that one.\nNotice how you can print more than one thing at a time if you put commas between the values you want to print? We can group computations using parentheses, just like in normal math. Try this:\nIn addition to the regular math operators, there are a few extra operators that we call comparison operators because they are used to compare two values, e.g., two numbers.\nTry this:\nAs you may have noticed, running this code and comparing things using these operators, we always produce either True or False. E.g., the following\nprints the value True because 5 is smaller than 7. True and False are special values in Python that we can use (and print if we like) just like any other Python value:",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dealing with values</span>"
    ]
  },
  {
    "objectID": "chapters/python/dealing_with_values.html#math",
    "href": "chapters/python/dealing_with_values.html#math",
    "title": "5  Dealing with values",
    "section": "",
    "text": "Operator\nOperation\n\n\n\n\n+\nplus\n\n\n-\nminus\n\n\n/\ndivision\n\n\n//\ninteger division\n\n\n*\nmultiplication\n\n\n**\nexponentiation\n\n\n%\nmodulo (remainder)\n\n\n\n\nprint(\"Four times two is\",  4 * 2)\n\nprint(\"10 / (2 + 3) is\", 10 / (2 + 3))\nprint(\"(10 / 2) + 3 is\", (10 / 2) + 3)\n\n\n\n\nOperator\nOperation\n\n\n\n\n&lt;\nless-than\n\n\n&gt;\ngreater-than\n\n\n&lt;=\nless-than-or-equal\n\n\n&gt;=\ngreater-than-or-equal\n\n\n==\nequal\n\n\n!=\nnot equal\n\n\n\n\nprint(\"Is 5 greater than -2?\", 5 &gt; -2)\nprint(\"Is 5 greater or equal to -2?\", 5 &gt;= -2)\nprint(\"Is 5 less or equal to -2?\", 5 &lt;= -2)\nprint(\"Is 5 less than 7 - 2?\", 5 &lt; 7 - 2)\nprint(\"Is 5 equal to 7 - 2?\", 5 == 7 - 2)\n\nprint(5 &lt; 7)\n\nprint(True)\nprint(False)\n\nExercise 5-1\nTry to write and run the code below. Compare each line to what is printed when you run the code. Make sure you understand why.\nprint(\"I have\", 25 + 30 / 6, \"of something\")\nprint(\"I have\", 100 - 25 * 3 % 4, \"of something else\")\n\nprint(\"Is it true that 3 + 2 &lt; 5 - 7?\")\nprint(3 + 2.1 &lt; 5.4 - 7)\n\nprint(\"3 + 2.1 is\", 3 + 2.1)\nprint(\"5.4 - 7 is\", 5.4 - 7)\n\nprint(\"Oh, that's why it's False.\")\n\n\nExercise 5-2\nAn additional comparison operator even tests if something is a part of something else. That operator is called in. One use of it is to test if one string is part of another string. Try this to figure out how it works:\nprint(\"Hell\" in \"Hello world\")\nprint(\"Hello world\" in \"Hello world\")\nprint(\"Hello world\" in \"Hell\")\nprint(\"lo wo\" in \"Hello world\")\nprint(\"Artichoke\" in \"Hello world\")\n\n\nExercise 5-3\nSay the supermarket has chocolate bars for 7 kr. Write a small Python program (in a file called chocolate.py) that prints how many chocolate bars you can get for your 30 kr. You should run it like this;\nFor example, it could output something like this:\n$ python chocolate.py\nto have it print something like this:\nI can buy 4.285714285714286 chocolate bars!\n\n\nExercise 5-4\nWe mentioned a special operator called modulo. Google it if you do not remember what it does. How about integer division. Explain both to a fellow student (or to yourself out loud).\n\n\nExercise 5-5\nYou obviously cannot go buy 4.3 chocolate bars in a store. You will have to settle for 4. Can you change the program you made in Section 5.0.0.3 to print the number of bars you can buy and the change you then have left? Use the modulo and integer division operators. Something like:\nI can buy 4 chocolate bars, leaving me with 2 kr in change.\n\n\nExercise 5-6\nWhat happens if you try to run the following program?\nprint( 1 / 0 )\nIf you get an error? What kind of error? Why do you think you get that error? Do you think it makes sense?\n\n\nExercise 5-7\nYou probably know the Pythagorean theorem for computing a right-angled triangle’s hypotenuse (the longest side). The Pythagorean theorem looks like this: \\(a^2 + b^2 = c^2\\). Here \\(c\\) is the length of the hypotenuse, and \\(a\\) and \\(b\\) are the lengths of the triangle’s two legs. So if we have a triangle where \\(a = 5\\) and \\(b = 2\\) and we want to find \\(c^2\\) we can do this in Python:\nprint(\"The squared length of the hypotenuse is:\", 5**2 + 2**2)\n\n\nExercise 5-8\nHowever, we are rarely interested in the squared length of the hypotenuse. Can you modify the code you wrote in Section 5.0.0.7 so you compute \\(c\\) instead of \\(c^2\\)? Taking the square root of a number is the same as taking that number and exponentiating it to 0.5, so the square root of \\(x\\) is \\(x^{0.5}\\). Do you know of a Python operator that does exponentiation?",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dealing with values</span>"
    ]
  },
  {
    "objectID": "chapters/python/dealing_with_values.html#logic",
    "href": "chapters/python/dealing_with_values.html#logic",
    "title": "5  Dealing with values",
    "section": "Logic",
    "text": "Logic\nNow you know how to use the comparison operations to produce a True or False value. There are three additional operators that let you express more elaborate “True/False” statements than with the comparison operators alone. These are the logical operators: and, or and not.\n\nExercise 5-9\nGo through the code below and see if you can figure out what each line does. Then, write the code into your editor and run it to see what happens.\nprint(2 &lt; 3)\nprint(10 &lt; 12)\nprint(8 &gt; 100)\nprint(2 &lt; 3 and 10 &lt; 12)\nprint(8 &gt; 100 and 2 &lt; 3)\nprint(8 &gt; 100 or 2 &lt; 3)\nprint(not 8 &gt; 100 and 2 &lt; 3)\nprint(not 8 &gt; 100 and not 2 &gt; 3)\nDid it do what you expected? Can you explain each line?\n\n\nExercise 5-10\nWhen exposed to the operators and, or and not, some values are considered true and others are considered false. What happens when you put not in front of something that is considered true or false? Decide what you think and why before you write the code and try it out.\nprint(not True)\nprint(not False)\nprint(not 0)\nprint(not -4)\nprint(not 0.0000000)\nprint(not 3.14159265359)\nprint(not \"apple\")\nprint(not \"\")\nFrom the code above, try to find out which values Python considers true and which it considers false. Can you come up with a rule?\n\n\nExercise 5-11\nThe logical operator and takes two values (the one to the left of the operator and the one to the right) and figures out whether both the left and the right expression are true. It boils down to this:\n\n\n\nLeft expression\n Right expression\n Result\n\n\n\n\nTrue\nTrue\n True\n\n\nTrue\n False\n False\n\n\nFalse\n True\n False\n\n\nFalse\nFalse\n False\n\n\n\nWrite some code to confirm that the table above is correct using Python. For example, to test the first case, do this:\nprint(True and True)\n\n\nExercise 5-12\nPython will only do the necessary work to determine if a logical expression is true. That means that if the value left of and is considered false by Python, then there is no reason to look at the right value since it is already established that they are not both considered true. In this case the expression reduces to the left value. I.e. False and True reduces to False.\nIf Python considers the value left of and true, then It needs to look at the right value, too, to establish if they are both considered true. In this case, the expression reduces to the value on the right. I.e., True and False reduces to False.\nA rule of thumb is that the whole expression reduces to the last value that Python needs to consider to decide if the whole expression is true or false. Use that rationale to explain how the two last combinations in Section 5.0.0.11 are evaluated.\n\n\nExercise 5-13\nLike the and operator, the or operator also takes two values. However, the or operator determines whether one of the two values is true. Thus, the or operator boils down to this:\n\n\n\nLeft expression\n Right expression\n Result\n\n\n\n\nTrue\nTrue\n True\n\n\nTrue\n False\n True\n\n\nFalse\n True\n True\n\n\nFalse\nFalse\n False\n\n\n\nWrite some code using Python to confirm that the table above is correct. For example, to test the first case, do this:\nprint(True or True)\n\n\nExercise 5-14\nAs with the and operator, Python will not do any more work than absolutely necessary when evaluating an expression with ‘or’. So if the value left of or is considered true by Python, then there is no reason to look at the right value since it is already established that at least one of them is considered true. In this case the expression reduces to the left value. I.e. True or False reduces to True.\nIf the value left of or is considered false by Python, then Python still needs to look at the right value to establish if at least one of them is considered true. In this case, the expression reduces to the right value. I.e., False or True reduces to True.\nAgain, the whole expression reduces to the last value that Python needs to consider to decide if the whole expression is true or false. Use that same rationale to explain to yourself how the two last combinations in Section 5.0.0.13 are evaluated.\n\n\nExercise 5-15\nRemember what you learned in Section 5.0.0.10 about which values are considered true and which are considered false. Combine that with what you learned in Section 5.0.0.11 and Section 5.0.0.13 about what logical expressions reduce to and see if you can figure out what is printed below and why. Use the rule-of-thumb from Section 5.0.0.14. Decide what you think before you write the code and try it out.\nprint(True and 4)\nprint(0 and 7)\nprint(-27 and 0.5)\nprint(42 and 0)\nprint(\"apple\" and \"orange\")\nprint(\"apple\" and \"\")\nprint(42 or 0)\nprint(\"apple\" and \"\")\nprint(\"apple\" or \"\")\nIf you were surprised by what was printed, maybe go back and look at Section 5.0.0.11 and Section 5.0.0.13 again.\n\n\nExercise 5-16\nRecall the in operator from Section 5.0.0.2? There is also an operator called not in. I guess you can imagine what that tests. Try it out.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dealing with values</span>"
    ]
  },
  {
    "objectID": "chapters/python/dealing_with_values.html#variables",
    "href": "chapters/python/dealing_with_values.html#variables",
    "title": "5  Dealing with values",
    "section": "Variables",
    "text": "Variables\n\nBy now, you probably feel the first signs of brain overload. If you do not take breaks, your brain may overheat and explode - we have seen that happen. One of the nice things about the brain is that it works when you rest. Archiving and understanding a lot of new information takes time, and force-feeding your brain will not help. The last part of this chapter is very important so now might be a good time for a good long break.\n\nThis section is about variables and is where the fun begins. A variable is a way of assigning a name to a value. 8700000 is just a value, but if we assign a name to it, then it gets a special meaning:\nnumber_of_species = 8700000\nprint(number_of_species)\nIn this case, the variable number_of_species represents the estimated number of eukaryotic species on the planet, which is 8700000. So 8700000 is the value, and “number_of_species” is the variable name. Write the code above into a file and run it. Notice how this lets us refer to the value using the variable name. What appears in the terminal when you do that? Do you see number_of_species or 8700000?\nAs you can see in the small program above, one of two different things happens when a variable name appears in Python code:\n\nAssignment: When a variable name appears to the left of an equal sign, a value is assigned to the variable. This happens in the first line where number_of_species is assigned the value 8700000.\nSubstitution: In all other contexts, the variable is substituted for its value. This happens in the second line where Python substitutes the variable name number_of_species for its value 8700000 and then prints that.\n\nThat is it, but let us take the example further and create another variable to which we assign the value 1200000. That is the number of species discovered so far. Now, let us add this to the program and use it to compute the number of species we have yet to identify. Start by reading the code below super carefully. Remember that a variable is either assigned a value or substituted for the value it represents. For each occurrence of the variables below, determine if they are being assigned a value or if they are substituted for their value.\nnumber_of_species = 8700000\nnumber_discovered = 1200000\nnumber_unidentified = number_of_species - number_discovered\nprint(number_unidentified)\nNow write the code into a file and run it. Take some time to let it sink in that variables are extremely useful for two reasons:\n\nVariables give meaning to a value. Without the variable name, the value of 1200000 could just as well be the number of people that live in Copenhagen. However, by giving the value a meaningful name, it becomes clear what it represents.\nWe can assign new values to variables (that is why they are called variables). For example, we can change the value of number_discovered as new species are discovered.\n\nYour variable names can be pretty much anything, but they have to start with a letter or an underscore (_), and the rest of the name has to be either letters, numbers, or underscores. To be clear, a space is not any of those things, so do not use spaces in variable names. Above all, be careful in your choice of variable names. Variable names are case-sensitive, meaning that count and Count are different variables. Stick to lowercase variable names. That makes your code easier to read.\n\nExercise 5-17\nFor each occurrence of the variables below, determine if they are being assigned a value or if they are substituted for their value.\nbreeding_birds = 4\nprint(breeding_birds)\nbreeding_birds = 5\nprint(breeding_birds)\n\n\nExercise 5-18\nFor each occurrence of the variables below, determine if they are being assigned a value or if they are substituted for their value.\nbreeding_birds = 4\nprint(breeding_birds)\nbreeding_birds = breeding_birds + 1\nprint(breeding_birds)\n\n\nExercise 5-19\nWhat happens if you take the first example in this section and swap the two lines? So, going from this:\nnumber_of_species = 8700000\nprint(number_of_species)\nto this:\nprint(number_of_species)\nnumber_of_species = 8700000\nExplain to yourself what happens in each case. What kind of error do you get with version two, and why? Remember (important-oath2?)!\n\n\nExercise 5-20\nWrite the following code in a file, save it, and run it.\nincome = 45000\ntaxpercentage = 0.43\ntax_amount = tax_percentage * income\nincome_after_tax = income - tax_amount\nprint('Income after tax is', income_after_tax)\nYou should get an error that looks at lot like this one:\nTraceback (most recent call last):\n  File \"tax.py\", line 3, in &lt;module&gt;\n    tax_amount = tax_percentage * income\nNameError: name 'tax_percentage' is not defined\nIt says that the error is on line 3. Can you figure out what is wrong? Hopefully, you will now appreciate how much attention to detail is required when programming. Every tiny, little symbol or character in your code is essential.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dealing with values</span>"
    ]
  },
  {
    "objectID": "chapters/python/dealing_with_values.html#different-types-of-values",
    "href": "chapters/python/dealing_with_values.html#different-types-of-values",
    "title": "5  Dealing with values",
    "section": "Different types of values",
    "text": "Different types of values\nBy now, you probably have a pretty good idea of what a value in Python is. So far, you have seen text like 'Banana', integers like 7, and numbers with a fractional part like 4.25.\nIn Python, a text value is a type of value called a string, which Python denotes as str (abbreviation for “string”). So 'Banana' is a string, and so is 'Banana split'. There are two types of numbers in Python. Integers (7, 42, and 3) are called int. Numbers with a fractional part (like 3.1254 and 4.0) are that are called float (an abbreviation for “floating-point number”).\nAs I mentioned earlier, True and False are Python values too. They are called booleans or bool, named after an English mathematician called George Boole famous for his work on logic.\nSo the different types of values we know so far are:\n\n\n\nName\nType in Python\nExamples\n\n\n\n\nString\nstr\n\"hello\", '9'\n\n\nInteger\nint\n0, 2721, 9\n\n\nFloating-point\nfloat\n1.0, 4.4322\n\n\nBoolean\nbool\nTrue, False\n\n\nNone\nNoneType\nNone\n\n\n\nIn case you did not notice, I added a special type at the end that can only have the value None. I may sound a little weird, but in programming, we sometimes need a value representing nothing or None. For now, just make a mental note that None is also a Python value.\nWhen you do computations in Python, it is no problem to mix integers and floating-point numbers. Try this:\nprint(\"What is 0.5 * 2?\", 0.5 * 2)\nprint(\"What is 3 / 2?\", 3 / 2)\nAs you can see we can also make computations using only integers that result in floating-point numbers.\nSome of the math operators not only work on numbers, but they also work on strings. That way, you can add two strings together. It is no longer math, of course - but quite handy.\nfruit = 'Ba' + 'na' + 'na'\nprint(fruit)\n\nExercise 5-21\nIf you try to combine different types of values in ways that are not allowed in Python, you will get an error. Try each of the following weird calculations, and read each error message carefully.\nx = 3 - '1.5'\nprint(x)\nx = None - 4\nprint(x)\n\n\nExercise 5-22\nWrite these two examples and compare the resulting values of x\nx = '9' + '4'\nprint(x)\nx = 9 + 4\nprint(x)\n\n\nExercise 5-23\nTry these two examples. What happens in each case? Does it make sense?\nx = '72' * 3\nx = '72' * '3'\n\n\nExercise 5-24\nWill this work? Use what you have learned from the other exercises and try to predict what will happen here. Then, write the code and try it out.\nx = 'Ba' + 'na' * 2\nprint(x)\n\n\nExercise 5-25\nSometimes, you may need to change a string to a number. You can do that like this:\nsome_value = \"42\"\nother_value = int(some_value)\nWrite some code that converts strings to numbers and numbers to strings. Remember that numeric values are either integers or float. Use int, float as in the example above. You will notice that only meaningful conversions work. E.g., this will not work: number = int('four'). To convert a number to a string, you can use str.\nHaving completed the above exercises, you should take note of the following four important points:\n\nAll Python values have a type. You know about strings, integers, floating-points, and booleans so far.\nMath operators let you do cool things like concatenating two strings by adding them together.\nThe flip side of that cool coin is that Python will assume you know what you are doing if you add two strings ('4' + '4' is '44' not 8) or multiply a string with an integer ('4' * 4 is '4444' not 16).\nYou can change the type of a value, e.g., '4' to 4 or 1 to 1.0.\nPython will throw a TypeError if you try to combine types values of values in ways that are not allowed.\n\n\nEscape characters: An escape character is a backslash \\ followed by a single character. \\n and \\t are the most commonly used ones.\n\n\n\nExercise 5-26\nWhat do you think is printed here?\nmain_course = 'Duck a la Banana\\n'\ndessert = 'Banana split\\n'\nmenu = main_course + dessert\nprint(menu)\nCan you figure out what the special character \\n represents?\n\n\nExercise 5-27\nWhat do you think is printed here?\n\ndish_one = 'Banana\\t\\tsplit'\ndish_two = 'Chocolate\\tcake'\nprint(dish_one)\nprint(dish_two)\nCan you figure out what the special character \\t represents?",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dealing with values</span>"
    ]
  },
  {
    "objectID": "chapters/python/dealing_with_values.html#mixed-exercises",
    "href": "chapters/python/dealing_with_values.html#mixed-exercises",
    "title": "5  Dealing with values",
    "section": "Mixed exercises",
    "text": "Mixed exercises\nEach chapter in the book ends with a set of mixed exercises meant to allow you to combine what you have learned so far. In this case, they are meant to train your familiarity with the following topics:\n\nStrings\nMath\nLogic\nTypes of values\nVariables\n\n\nExercise 5-28\nWhat happens if you try to run the following program?\nprint(\"What happens now?\", 1 / )\nIf you get an error, why do you think you get that error?\n\n\nExercise 5-29\nWhat happens if you try to run the following program?\nprint(\"What happens now?\", 1 / 3\nIf you get an error, why do you think you get that error? Can you fix it? (Hint: EOF is short for End Of File)\n\n\nExercise 5-30\nDetermine, for each of the eight occurrences of the variable x below, where it is being assigned a value and when it is substituted for its value:\nx = 1\nx = x + 1\nx = x + 1\nx = x + 1\nprint(x)\nThen, figure out what is printed and why (remember (important-oath2?)). What value does x represent at each occurrence in the code?\n\n\nExercise 5-31\nSome comparison operators also work with strings. Consider this code:\nprint(\"apples\" == \"pears\")\nWhat is printed here? Write the code and see for yourself once you think you know. If you were wrong, make sure you understand why.\n\n\nExercise 5-32\nConsider this code:\nprint('aaaaaa' &lt; 'b')\nprint('a' &lt; 'b')\nprint('aa' &lt; 'ab')\nprint('99' &gt; '100')\nprint('four bananas' &gt; 'one banana')\nWhat is printed here? Write the code and see for yourself once you think you know. By what rule does Python decide if one string is smaller than another? You may have a clue if you have looked something up in an encyclopedia recently. Also, try to google “ASCII table”.\n\n\nExercise 5-33\nConsider this code:\nprint('banana' &lt; 'Banana')\nWhat is printed here? Write the code and see for yourself once you think you know.\n\n\nExercise 5-34\nDo you think it is allowed to use relational operators on values of different types? Try these out and see for yourself:\nprint('Banana' &gt; 4)\nprint('42' == 43) # this one is dangerous...\nprint(4 in '1234')\nPractice reading this kind of error (TypeError).\n\n\nExercise 5-35\nCan you use the in operator to test if this mini gene is part of the DNA string?\nmini_gene = 'ATGTAG'\ndna_string = 'GCTATGTAGGTA'\n\n\nExercise 5-36\nSay you have two strings \"4\" and \"2\". What happens if you add them like this: \"4\" + \"2\". Can you convert each one to integers so you get 6 when you add them? (have a look at Section 5.0.0.25 if you do not remember).\n\n\nExercise 5-37\nWhat happens if you run this code? Do you get an error? Do you remember why?\n1value = 42\n\n\nExercise 5-38\nWhat happens if you run this code?\nprint('Hi')\nprint('Hi')\nprint('Hi')\nCompare this to what happens when you run this code:\nprint('Hi\\nHi\\nHi')\nDo you remember what \\n represents? What does it tell about what is added at the end every time you print something?\n\n\nExercise 5-39\nMake three exercises for your fellow students. See if you can make them so they test the understanding of (almost) all you have learned so far.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dealing with values</span>"
    ]
  },
  {
    "objectID": "chapters/python/the_order_of_events.html",
    "href": "chapters/python/the_order_of_events.html",
    "title": "6  The order of events",
    "section": "",
    "text": "Precedence of Operators\nThis chapter is about how Python interprets (or evaluates) the code you write. It has a few fancy long words that may seem foreign to you. Do not let that throw you off. They are all just fancy names for something straightforward.\nFear not. Precedence is just a nasty word for something we have already talked about. Precedence just means that some things are done before others or, more correctly, that some operations are performed before others. You already know that multiplication is done before addition. Another way of saying that is that multiplication takes precedence over addition. The expression below obviously reduces to 7 in two steps:\n\\[ 1 + 3 * 2 \\]\nFirst, \\(3 * 2\\) reduces to 6, and then \\(1 + 6\\) reduces to 7. If we wanted to add 1 and 3 first, we would need to enforce this by adding parentheses:\n\\[ (1 + 3) * 2 \\]\nThis is because the multiplication operator (*) has higher precedence than the addition operator (+). Here is the list of the most common operators and their precedence in Python:\nSometimes, a statement contains adjacent operators with the same precedence. In this case, Python evaluates the expression from left to right. I.e., This following expression first reduces to \\(0.5 * 2\\) and then to \\(1\\)\nThe following one first reduces to \\(1 * 4\\) and then to \\(4\\):\nIf you want Python to order the operations in any other way, you need to use parentheses (E.g., 2 / (2 * 4)).",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The order of events</span>"
    ]
  },
  {
    "objectID": "chapters/python/the_order_of_events.html#precedence-of-operators",
    "href": "chapters/python/the_order_of_events.html#precedence-of-operators",
    "title": "6  The order of events",
    "section": "",
    "text": "Level\nCategory\nOperators\n\n\n\n\nHighest\nexponent\n**\n\n\n\npositive / negative\n+x, -x\n\n\n\nmultiplication\n*, /, //, %\n\n\n\naddition\n+, -\n\n\n\nrelational\n!=, ==, &lt;=, &gt;=, &lt;, &gt;, in, not in\n\n\n\nlogical\nnot\n\n\n\nlogical\nand\n\n\nLowest\nlogical\nor\n\n\n\n\n2 / 4 * 2\n\n2 / 2 * 4\n\n\nExercise 6-1\nLook at each expression in the exercises below and use the table above to decide if it evaluates to True or False. Then, write the code and test if you were right. If not, figure out why.\n2 + 4 * 7 == 2 + (4 * 7) \n\n\nExercise 6-2\nDoes this reduce to True or False?\n4 &gt; 3 and 2 &lt; 1 or 7 &gt; 2\n\n\nExercise 6-3\nDoes this reduce to True or False?\n4 &gt; 3 and (2 &lt; 1 or 7 &gt; 2)\n\n\nExercise 6-4\nDoes this reduce to True or False?\n2 * 4 ** 4 + 1 == (2 * 4) ** (4 + 1)",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The order of events</span>"
    ]
  },
  {
    "objectID": "chapters/python/the_order_of_events.html#statements-and-expressions",
    "href": "chapters/python/the_order_of_events.html#statements-and-expressions",
    "title": "6  The order of events",
    "section": "Statements and Expressions",
    "text": "Statements and Expressions\nTo talk concisely about programming (and to receive more useful help from your instructors), you need to have a bit of vocabulary. Statements and expressions are two words you need to know. Distinguishing between statements and expressions will help us discuss our code.\n\nA statement is a line of code that performs an action. Python evaluates each statement until it reaches the end of the file (remember (important-oath2?)?). print(y * 7) is a statement, and so is x = 14. They each represent a full line of code and perform an action.\nAn expression is any code that reduces to one value. y * 7 is an expression, and so are y * 7 + 14 - x and 4 &gt; 5.\n\nWe will talk more about how Python handles expressions in the next section, but right now, you must understand that statements do something while expressions are things that reduce to a value. Hopefully, this distinction will be more clear after completing the following exercises.\n\nExercise 6-5\nDid you notice in the above examples that print(y * 7) is a statement and y * 7 is an expression? Yes, expressions can be part of statements. In fact, they most often are. Similarly, expressions are often made up of other smaller expressions. E.g., y * 7 is part of the longer expression y * 7 + 14 - x.\nTake a look at this code:\nx = 5\ny = 20\nz = (x + y) / 2 + 20\nprint(z * 2 + 1)\nh = 2 * x - 9 * 48\nprint(h)\nWrite down the code on a piece of paper. Now, mark all statements and all expressions. Remember that expressions are often made up of smaller expressions so that you can find a lot of them. E.g. (x + y), 2 + 20, and (x + y) / 2 + 20 are all expressions. A single variable (like x) is also a small expression. Discuss with a fellow student. Do you agree on what to find?\n\n\nExercise 6-6\nConsider the following code:\ngreeting = 'Hello' + ' my '\nprint(greeting + 'friend')\nHow many statements are there in this piece of code? How many expressions?",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The order of events</span>"
    ]
  },
  {
    "objectID": "chapters/python/the_order_of_events.html#substitution-and-reduction",
    "href": "chapters/python/the_order_of_events.html#substitution-and-reduction",
    "title": "6  The order of events",
    "section": "Substitution and Reduction",
    "text": "Substitution and Reduction\nAlthough substitution and reduction may not sound like your new best friends, they truly are! If you remember to think about your Python code in terms of substitution and reduction, then programming will make a lot of sense. Understanding and using these simple rules will allow you to read and understand any code. If you do not, you may get by for a while - only to find yourself in big trouble later when things become more complicated.\nYou should remember, from the section on variables in the previous chapter, that variables in Python are either assigned a value or substituted for the value they represent.\nIn the first two lines of code below, the variables x and y are each assigned a value. Now consider the last line in the example:\nx = 4\ny = 3\nz = x * y + 8\nHere, x is substituted by the value 4, and y is substituted by the value 3. So now the expression after the equals sign reads 4 * 3 + 8. Because we multiply before we add, 4 * 3 reduces to 12 so that the expression now reads 12 + 8. Finally, this reduces to the value 20. The very last thing that happens is that the variable z is assigned the value 20.\nYou should do these steps every time you see an expression. You may think this is overdoing things a bit, but it is not. This kind of explicit thinking is what programming is all about, and it will become increasingly important as the course progresses. So make sure you make it a habit while it still seems trivial. Then, over time, it will become second nature.\nNow raise your right hand and read the third and last oath out loud:\n\n\n\n\n\n\nImportantOath 3\n\n\n\nI swear to consciously consider every single substitution and reduction in every Python expression that I read or write from this moment on.\n\n\nThis was the last of the three oaths, but it is by far the most important one. You can take your hand down now.\nNB: You may not realize this at this point, but the last two subsections are the most important ones in the book. Go back and read them many times as you proceed through the course. If you explicitly think in terms of substitution and reduction, you will be fine. If you do not, you are entering a minefield with snowshoes on.\n\nExercise 6-7\nDo the substitution and reduction steps with pen and paper, then run it to check yourself by inserting a print statement at the end.\nx = 7\ny = 4 + x\n2 + x * x**2 + y - x\n\n\nExercise 6-8\nDo the substitution and reduction steps with pen and paper, then run it to check yourself by inserting a print statement at the end.\na = 4\nb = a\nc = 2\nc = a + b + c\n\n\nExercise 6-9\nDo the substitution and reduction steps with pen and paper, then run it to check yourself by inserting a print statement at the end.\nx = 1\nx = x\n\n\nExercise 6-10\nDo the substitution and reduction steps with pen and paper, then run it to check yourself by inserting a print statement at the end.\nmicrosatellite = \"GTC\" * 41\nSurprised?\n\n\nExercise 6-11\nDo the substitution and reduction steps with pen and paper, then run it to check yourself by inserting a print statement at the end.\nmini_gene = \"ATG\" + \"GCG\" + \"TAA\"\nWhat did you do first here? Does the order of additions matter? What operations does Python perform first when operators have the same precedence? (left to right or right to left)\n\n\nExercise 6-12\nDo the substitution and reduction steps with pen and paper, then run it to check yourself by inserting a print statement at the end.\nnumber = 1 / 1 * 4\nIn what order are reductions made? Does the order of operations matter, and in what order does Python do the reductions?\n\n\nExercise 6-13\nDo the substitution and reduction steps with pen and paper, then run it to check yourself by inserting a print statement at the end.\nx = 4\ny = x + x\n\n\nExercise 6-14\nDo the substitution and reduction steps with pen and paper, then run it to check yourself by inserting a print statement at the end.\nx = 4\nx = x + 1\n\n\nExercise 6-15\nDo the substitution and reduction steps with pen and paper, then run it to check yourself by inserting a print statement at the end.\nx = 4\nx += 1\nCompare the final value of x to that in Section 6.0.0.14. Can you see what += is a shorthand for? Nifty, right?",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The order of events</span>"
    ]
  },
  {
    "objectID": "chapters/python/the_order_of_events.html#general-exercises",
    "href": "chapters/python/the_order_of_events.html#general-exercises",
    "title": "6  The order of events",
    "section": "General exercises",
    "text": "General exercises\nThe following exercises are meant to train your familiarity with the topics we have treated so far – in this case especially:\n\nSubstitution\nReduction\nAssignment\nSimple precedence rules\nComparison operators\nLogical operators\nDistinction between text and numbers\n\nRead each exercise and think hard about the questions before you code anything. Then write the code and try it out. Remember that it is crucial that you type it in – as super dull as it may be (remember oath one). This trains your accuracy and attention to detail, and it builds programming into your brain. Play around with each bit of code. Make small changes and see how it behaves.\nThere is a reason why there are lots of questions in this exercise but no answers. You are supposed to find them yourself if it takes you quite a while. That is the way you build understanding. Some of the questions may seem trivial, but do them anyway. If you only understand these concepts superficially, they will come back and bite you in the ass when things get more complicated.\n\nExercise 6-16\nConsider this code:\n1.2 * 3 + 4 / 5.2\nWhat does that expression evaluate to? Try to explicitly make all the reductions on paper before you write and run the code.\n\n\nExercise 6-17\nConsider this code:\n1.2 * (3 + 4) / 5.2\nWhat does that expression evaluate to? Try to explicitly make all the reductions on paper before you write and run the code.\n\n\nExercise 6-18\nConsider this code:\n10 % 3 - 2\nWhat does that expression evaluate to? Try to explicitly make all the reductions on paper before you write and run the code.\n\n\nExercise 6-19\nConsider this code:\n11 % (7 - 5)**2\nWhat does that expression evaluate to? Try to explicitly make all the reductions on paper before you write and run the code.\n\n\nExercise 6-20\nConsider this code:\na = 5\nx = 9\nbanana = 7\nx + 4 * a &gt; banana\nWhat does the last expression evaluate to? Try to explicitly make all the substitutions and reductions on paper before writing the code. What happens if you write and run the code? Why?\n\n\nExercise 6-21\nConsider this code:\ndance = 'can'\ndance = dance + dance\nprint(\"Do the\", dance)\nWhat is printed? Try to explicitly make all the substitutions and reductions on paper before writing the code. What happens if you write and run the code? Why?\n\n\nExercise 6-22\nConsider this code:\nfoo = 30\nbar = 50\nbaz = bar + foo\nprint(baz)\nbar = 10\nprint(baz)\nThere are two print statements. The first print statement prints 80. But what about the second print statement? Does that print 80 or 40? Find out and make sure you understand why it prints what it prints. If not, reread the section on substitution.\n\n\nExercise 6-23\nConsider this code:\n1 == '1'\nand this:\n1 == 1.0\nWhat does this reduce to? Try printing it and seeing once you think you know. If you were wrong, make sure you figure out why.\n\n\nExercise 6-24\nConsider this code:\na = '1'\nb = '2'\nc  = a + b\nprint(a, b, c)\nprint(a + b == 3)\nWhat is printed here? Write the code and see for yourself once you think you know. If you were wrong, make sure you understand why.\n\n\nExercise 6-25\nConsider this code:\na = 1\nb = 2\nc  = a + b\nprint(a, b, c)\nprint(a + b == 3)\nWhat is printed here? Compare to Section 6.0.0.24. Write the code and see for yourself once you think you know. If you were wrong, make sure you understand why.\n\n\nExercise 6-26\nConsider this code:\nx = 4\nprint(x + 2 and 7)\nprint(x + 2 or 7)\nx = -2\nprint(x + 2 and 7)\nprint(x + 2 or 7)\nWhat is printed here? Write the code and see for yourself once you think you know. If you were wrong, make sure you understand why.\n\n\nExercise 6-27\nI have shuffled the statements in the code below. Put them in the right order to make the code print 100.\nx = x + 4\nprint(x)\nx = x * 5\nx = x * x\nx = 4\n\n\nExercise 6-28\nI have shuffled the statements in the code below. Put them in the right order to make the code print the string `“Banana.”\ny = 'n'\nx = 'B' + y + x\nprint(x)\nx = 'a'\ny = (x + y) * 2\n\n\nExercise 6-29\nMake a puzzle exercise, like the two previous ones, for a fellow student.\n\n\nExercise 6-30\nRemind yourself of the different types of Python values you know. For example, one of them is an integer (int). Make a list.\n\n\nExercise 6-31\nYou already know about several types of data values in Python. Two are integers called int, and decimal numbers (or floating points) called float. When you use an operator like + or &gt; it produces a value. No matter what you put on either side of &gt; it produces a boolean value (bool), True or False. For other operators, the type of value produced depends on which values the operator works on. Try this and see if you print an integer or a float (8 or 8.0):\nx = 4\ny = 2\nresult = x * y\nprint(result)\nNow try to replace 4 with 4.0. What type is the result now? Try to also replace 2 with 2.0. What type is the result now? Can you extract a rule for what the * operator produces depending on what types the two values have?\n\n\nExercise 6-32\nIn Section 6.0.0.31, you investigated what types of values the * operator produces. Redo that exercise with the operators: +, -, /, **, //, and %. What are the rules for what is returned if both values are integers, one value is a float, or both values are floats?\n\n\nExercise 6-33\nMake a list of all the operators you know so far in order of precedence (without looking in the notes). Then check yourself.\n\n\nExercise 6-34\nWhat does his expression reduce to, and what type of value is it?\n3 &gt; 2\n\n\nExercise 6-35\nWhat does his expression reduce to, and what type of value is it? Do all the reduction steps in your head.\n2 - 4 * 5 - 2 * 1/3\n\n\nExercise 6-36\nWhat does his expression reduce to, and what type of value is it? Do all the reduction steps in your head.\n3 &gt; 2 and 2 - 4 * 5 - 2 * 9\n\n\nExercise 6-37\nWhat is printed here and why?:\nprint(True and \"banana\" or \"orange\")\nTry to change the True value to False and see what happens. Can you explain it? If not, look at Section 5.0.0.14 again.\n\n\nExercise 6-38\nWhat does his expression reduce to? Do all the reduction steps in your head.\n0 and 1 or 2\n\n\nExercise 6-39\nWhat does his expression reduce to? Do all the reduction steps in your head.\n4 and 1 or 2\n\n\nExercise 6-40\nIf you understood Section 6.0.0.37, then you should also understand this one:\nweather = 'rain'\nwhat_to_do = weather == 'rain' and 'watch movies' or 'go swimming'\nprint(what_to_do)\nWhat happens if you change 'rain' in the first line to something else (like 'sun')?\n\n\nExercise 6-41\nWhat is the value of results once the code below has run? Do the substitutions, reductions, and assignments in your head before you run the code.\nx = 7\ny = 13\nz = x + y\nx = 0\nresult = x + y + z\n\n\nExercise 6-42\nWhat is the value of results once the code below has run? Do the substitutions, reductions, and assignments in your head before you run the code.\nx = 5\ny = x + 1\nx = y + 1\ny = x + 1\nresult = x + y\n\n\nExercise 6-43\nIn the code below I have shuffled the statements. Put them in the right order to make the code print 9. To do that you must think about which values each variable will in each statement depending on the how you order the statements.\nx = x + 1\ny = 5\ny = y - 1\nprint(y)\nx = 1\ny = y * x\n\n\nExercise 6-44\nIn the code below, I have shuffled the statements. Put them in the right order to make the code print 'Mogens'\nc = b\nprint(c)\na = b + a\nb = 'og'\nb = c + a\nc = 'M'\na = 'ens'\n\n\nExercise 6-45\nMake three exercises that require the knowledge of programming so far. Have your fellow students solve them.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The order of events</span>"
    ]
  },
  {
    "objectID": "chapters/python/course_tools.html",
    "href": "chapters/python/course_tools.html",
    "title": "7  Course tools",
    "section": "",
    "text": "Wax on, wax off\nSeeing the sequence of substitutions and reductions in a Python expression will become natural over time. Until it does, you are in troubled waters, and if you do not practice in time, you may only realize this too late. Considering how simple this is to practice and how crucial it is to your progress, I have written a small companion program called myiagi where you can train this particular skill daily. The program is installed in the conda environment you created for this source, so make sure it is activated as described. To run the program, you execute this command in the terminal:\nIt should look like Figure 7.1, and the simple game is as follows. The program generates a Python expression. From that expression, all the substitution and reduction steps are performed. Each substitution or reduction results in an intermediate expression until only a single Python value remains. Here is an example where the expression is 4 * y + x and the value it reduces to is 37:\nYou do not know what values the y and x variables point to, but you can deduce it from the sequence of expressions that they are 24 and 13. In the game, you are given a series of numbered expressions in the wrong order like this:\nYour task is to put them in the right order so that the original expression is at the top and the single Python value it reduces to is at the bottom. Now, you might grab line 2 by tabbing 2 on your keyboard (the number turns red so you can see it is active). Then, you move the line using the up/down arrow keys. If you move it to the bottom, the list then looks like this:\nNow, you repeat this process until the order is correct (the program will let you know when it is). The fewer lines you grab to produce the right order, the more points you earn. Problems with longer lists of expressions also earn you more points. As problems become harder and include more aspects of Python, solving them also awards more points. Each week has a score goal to guide your effort. Reaching this goal ensures that you practice as much as you should. Practicing a bit every day daily is more effective than practicing a lot a few days a week. To provide an incentive, the points you earn slowly expire, so the easiest way to maintain your score is to practice a bit every day.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Course tools</span>"
    ]
  },
  {
    "objectID": "chapters/python/course_tools.html#wax-on-wax-off",
    "href": "chapters/python/course_tools.html#wax-on-wax-off",
    "title": "7  Course tools",
    "section": "",
    "text": "Mr. Miyagi: First, wash all car. Then wax. Wax on…\nDaniel: Hey, why do I have to…?\nMr. Miyagi: Ah ah! Remember deal! No questions!\nDaniel: Yeah, but…\nMr. Miyagi: Hai! Wax on, right hand. Wax off, left hand. Wax on, wax off. Breathe in through nose, out of mouth. Wax on, wax off. Don’t forget to breathe, very important. [walks away, still making circular motions with hands] Wax on… wax off. Wax on… wax off.\n\n\nmyiagi\n\n\n\n\n\n\nFigure 7.1: Visual Studio Code (VScode)\n\n\n\n\n1.   4 * y + x\n2.   4 * 8 + x\n3.   24 + x\n4.   24 + 13\n5.   37\n\n1.   4 * y + x\n2.   37\n3.   4 * 8 + x\n4.   24 + x\n5.   24 + 13\n\n1.   4 * y + x\n2.   4 * 8 + x\n3.   24 + x\n4.   24 + 13\n5.   37",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Course tools</span>"
    ]
  },
  {
    "objectID": "chapters/python/course_tools.html#a-helping-hand",
    "href": "chapters/python/course_tools.html#a-helping-hand",
    "title": "7  Course tools",
    "section": "A helping hand",
    "text": "A helping hand\nUsing the myiagi, you train your ability to read and understand Python expressions. Seeing a similar breakdown of a Python expression in your code may also be helpful. For that purpose, I have written another tool called pysteps. Say you have some code like the one below and need clarification on how the single value assigned to z is produced (here, you are probably not).\nx = 7\ny = 5\nz = x * y + 4\nAll you need to do is then to add # PRINT STEPS comment to the end of the line like this:\nx = 7\ny = 5\nz = x * y + 4 # PRINT STEPS\nSay your file is called myfile.py, you would normally run the code like this:\n\n\nTerminal\n\npython myfile.py\n\nBut to see the breakdown of expressions marked by # PRINT STEPS, you need to run your code with the pysteps program instead:\n\n\nTerminal\n\npysteps myfile.py\n\nThe command prints the following in the terminal:\nLine 4 in test_studentfile.py:\nAs written:      z = x * y + 4\nSubstitution:    z = 7 * y + 4\nSubstitution:    z = 7 * 5 + 4\nReduction:       z = 35 + 4\nReduction:       z = 39\nYou can even mark more than one line like this and have pysteps break down all of them for you:\nx = 7\ny = 5\nz = x * y + 4 # PRINT STEPS\nk = z * 42 # PRINT STEPS\nlike this:\nLine 3 in myfile.py:\nAs written:      z = x * y + 4\nSubstitution:    z = 7 * y + 4\nSubstitution:    z = 7 * 5 + 4\nReduction:       z = 35 + 4\nReduction:       z = 39\n\nLine 4 in myfile.py:\nAs written:      k = z * 42\nSubstitution:    k = 39 * 42\nReduction:       k = 1638\nHowever, it would be best if you used this helping hand sparingly. It is much better to train your ability to do this in your head with the help of Mr. Myagi. Trust me, it works.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Course tools</span>"
    ]
  },
  {
    "objectID": "chapters/python/controlling_behavior.html",
    "href": "chapters/python/controlling_behavior.html",
    "title": "8  Controlling behavior",
    "section": "",
    "text": "If-statement\nThis chapter is about how you make your program do different things under different circumstances. Making functionality dependent on data is what makes programs useful.\nSo far, the small programs you have written run the same sequence of statements (lines). Imagine if you could control which statements were run depending on the circumstances. Then, you would be able to write more flexible and useful programs. Cue the music - and let me introduce the “if-statement”.\nWrite the following carefully into a file. It is a small program that monitors bus passenger status. Notice the colon ending the if-statements. Also, note that the lines below each if-statement are indented with precisely four spaces. While writing the program, figure out what the if-statement does. Then, run the code and see what happens.\nTry changing the values of bus_seats, passengers, and bags and see how the program executes.\nYou have probably realized that the if-statements control which prints statements that are evaluated. A statement nested under an if-statement is only evaluated if the expression between the if keyword and the : reduces to a value Python considers as true. This does not happen if the expression between the if and : reduces to a value Python considers false.\nWhen asked to evaluate something as true or false, Python will interpret zero and empty values (like 0 and '') as False and all other non-zero and non-empty values as True.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Controlling behavior</span>"
    ]
  },
  {
    "objectID": "chapters/python/controlling_behavior.html#if-statement",
    "href": "chapters/python/controlling_behavior.html#if-statement",
    "title": "8  Controlling behavior",
    "section": "",
    "text": "bus_seats = 32\npassengers = 20\nbags = 20\n\nprint(passengers, \"people ride the bus\")\n\nif bus_seats &gt;= passengers + bags:\n    print(\"Smiles, everyone has room for bags\")\n\nif bus_seats &gt;= passengers:\n    print('Everyone gets to sit down, no complaints')\n\nif bus_seats &lt; passengers:\n    print('Some passengers standing, annoyed')\n    \nif bus_seats &lt; passengers / 3:\n    print(\"General dissatisfaction, some swearing too\")\n\n\n\n\nExercise 8-1\nWhich of the following letters are printed: A, B, C, D, E, F, G. Make up your mind before you write and run the code.\nif 0:\n    print('A')\n\nif \"Banana\":\n    print('B')\n\nif 3.14159265359:\n    print('C')\n\nif False:\n    print('D')\n\nif 9 &gt; 5 and 4 &lt; 7:\n    print('E')\n\nif '':\n    print('F')\n\nif False or \"banana\":\n    print('G')\n\n\nExercise 8-2\nWhat happens if you forget to write the : in the if-statement?\nif 4 &gt; 2\n    print('Hi!')\n\n\nExercise 8-3\nWhat happens if you do not indent the code under the if-statement?\nif 4 &gt; 2:\nprint('Hi!')\n\nBy now, you probably know that your text editor is intelligent regarding indentation. If you hit Enter after a statement ending with :, it will indent the next line with four spaces. Also, if you use the tab in Python code, it will produce four spaces for you.\n\n\n\nFAQ\nQ: Isn’t “If” a poem by Rudyard Kipling?\nA: Yes.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Controlling behavior</span>"
    ]
  },
  {
    "objectID": "chapters/python/controlling_behavior.html#else-statement",
    "href": "chapters/python/controlling_behavior.html#else-statement",
    "title": "8  Controlling behavior",
    "section": "Else-statement",
    "text": "Else-statement\nSometimes you not only want your program to do something if an expression reduces to True, you also want it to do something else if it is False. It is as simple as it looks:\ncookies = 3\n\nif cookies &gt; 0:\n    print(\"Uh, I wonder if we have some milk too...\")\nelse:\n    print(\"Sigh!\")\nRemember to put a : after the else keyword. Write the code and change the value of cookies to 0.\n\nExercise 8-4\nTest your understanding about which expressions that reduce to a True or False value. Write the code below and then see how it responds to different values of x. Try to come up with other variations yourself.\nx = 0.0\n# x = '0'\n# x = '   '\n# x = ''\n# x = not 0\n# x = 'zero'\n\nif x:\n    print('x is substituted with True in the if-statement')\nelse:\n    print('x is substituted with False in the if-statement)\n\n\nFAQ\nQ: Isn’t “Else” a poem by Rudyard Kipling?\nA: No.\n\nExercise 8-5\nWhat do you think this code prints? Notice how you can nest if and else-statements under other if and else-statements. This way, you can make your program include only some statements when certain combinations of conditions are met. Just remember that the code below each if or else is indented by four spaces. Try to change the True/False values of milk and cookies.\nmilk = False\ncookies = True\nif milk:\n    if cookies:\n        status = 'Good times!'\n    else:\n        status = 'Not thirsty, thanks or asking'\nelse:\n    if cookies:\n        status = 'How does something like this happen?'\n    else:\n        status = 'Whatever...'\n        \nprint(status)",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Controlling behavior</span>"
    ]
  },
  {
    "objectID": "chapters/python/controlling_behavior.html#blocks-of-code",
    "href": "chapters/python/controlling_behavior.html#blocks-of-code",
    "title": "8  Controlling behavior",
    "section": "Blocks of code",
    "text": "Blocks of code\nIn the examples above, some lines are indented more than others, and you probably already have some idea of how Python interprets this. Indentation defines blocks of code. The if and else statements control each code block’s evaluation when your code runs. The following three rules define individual blocks of code:\n\nAll statements in a code block have the same indentation. That is, they line up vertically.\nA block of code begins at the first line of code at a line that is indented more than the one before it.\nA block ends when it is followed by a less indented line or at the last line of code.\n\nThis way, a block can be nested inside another block by indenting it further to the right, as shown in Figure 8.1. Compare the example in Figure 8.1 to the code example above. Note how a colon at the end of a statement means “this applies the block of code below”. Make sure you understand which print statements are controlled by which if and else statements.\n\n\n\n\n\n\nFigure 8.1: The amount of indentation defines blocks of code",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Controlling behavior</span>"
    ]
  },
  {
    "objectID": "chapters/python/controlling_behavior.html#elif-statement",
    "href": "chapters/python/controlling_behavior.html#elif-statement",
    "title": "8  Controlling behavior",
    "section": "Elif-statement",
    "text": "Elif-statement\nSay you need to test several mutually exclusive scenarios. For example, if a base is equal to A, T, C, or G. You can do that like in the example below, but it is very verbose and shifts your code further and further to the right.\nbase = 'G'\n\nif base == 'A':\n    print('This is adenine')\nelse:\n    if base == 'T':\n        print('This is thymine')\n    else:\n        if base == 'C':\n            print('This is cytosine')\n        else:\n            print('This is guanine')\nThis is where an elif statement can be helpful. It is basically short for “else if.” The correspondence is hopefully obvious if you compare it to the example below.\nbase = 'G'\n\nif base == 'A':\n    print('This is adenine')\nelif base == 'T':\n    print('This is thymine')\nelif base == 'C':\n    print('This is cytosine')\nelse:\n    print('This is guanine')\nHere, we put an else-statement at the end to capture all cases not covered by the if-statement and the two elif-statements.\n\nExercise 8-6\nYou can use logical operators (and, or, not) in the expressions tested in an if-statement. Can you change the program from Section 8.0.0.5 so that there are no nested if-statements - in a way that the program still does the same? You can use if, elif, and else and test if, e.g., milk and cookies are true using and.\n\n\nExercise 8-7\nThe snippet of code below has three blocks with three statements. Which statements belong to which block? Which statements are executed?\nx = 5\nif x &gt; 4:\n    y = 3\n    if x &lt; 1:\n        x = 2\n        y = 7\n        z = 1\n    x = 1\nz = 4\n\n\nExercise 8-8\nCan you see four blocks of code? If not, read the three rules above again. Which statements are executed?\nx = 5\nif x &gt; 4:\n    y = 3\n    if x &lt; 1:\n        x = 2\n        y = 7\n    else:\n        x = 1\n        y = 9\nz = 4",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Controlling behavior</span>"
    ]
  },
  {
    "objectID": "chapters/python/controlling_behavior.html#general-exercises",
    "href": "chapters/python/controlling_behavior.html#general-exercises",
    "title": "8  Controlling behavior",
    "section": "General exercises",
    "text": "General exercises\n\nExercise 8-9\nWill this print You are a superstar!?\nif -4 and 0 or 'banana' and not False:\n    print(\"You are a super star!\")\n\n\nExercise 8-10\nWill this print You are a superstar!?\nif -1 + 16 % 5 == 0 :\n    print(\"You are a super star!\")\n\n\nExercise 8-11\nAssign values to two variables, x and y. Then, write code that prints OK if (and only if) x is smaller than five and y is larger than five. Do it using two if statements:\nx = 3 # or something else\ny = 7 # or something else\n\n# rest of code here...\nNow solve the same problem using only one if statement.\n\n\nExercise 8-12\nAssign values to two variables, x and y. Then, write some code that prints OK if and only if x is smaller than five or y is larger than five. Do it using two if statements:\nx = 3 # or something else\n7 = 7 # or something else\n\n# rest of code here...\nNow solve the same problem using only one if statement and one elif statement.\n\n\nExercise 8-13\nAssign values to two variables, x and y. Then, write some code that prints OK if either x or y is zero but not if both are zero (this is tricky).\nx = 3 # or something else\ny = 7 # or something else\n\n# rest of code here...\n\n\nExercise 8-14\nWhich value of x makes the code below print Banana?\nx = \ns = ''\nif x**2 == 16:\n    s = s + 'Ba'\nif x + 6 == 2:\n    s = s + 'na'\nif 7 == x - 3:\n    s = 'na' * 2\nelse:\n    s = s + 'na'\nprint(s)\n\n\n\nExercise 8-15\nMake three exercises that require the knowledge of programming so far. Have your fellow students solve them.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Controlling behavior</span>"
    ]
  },
  {
    "objectID": "chapters/python/organizing_your_code.html",
    "href": "chapters/python/organizing_your_code.html",
    "title": "9  Organizing code",
    "section": "",
    "text": "Functions\nThis chapter is about organizing your code into chunks that you can call upon to perform well-defined tasks in your program.\nBuckle down for the most powerful and useful thing in programming. Functions! Functions serve as mini-programs that perform small, well-defined tasks in your program.\nI have started to write a song about functions:\nI will add many more verses, and I do not want to write the entire chorus every time. So what would be more natural than to make a function named chorus that takes care of that for us? That way, we can write our song the way lyrics with a chorus are usually written:\nFirst, let us break down the function definition in the top part of this code:\nWhen Python runs this code, each line is executed one by one, starting from the first line (remember oath two?). So, in this case, python first executes the definition of the chorus function. The only thing that happened after Python had executed the first five lines of code was that it assigned the name chorus to the four indented statements. So Python now “knows” about the chorus function (like it “knows” about a variable x after we do x = 4).\nTo use the function, we “call” it by writing its name followed by parentheses: chorus(). When it comes to functions, “use”, “call” and “run” means the same thing. As you can see, we call the function twice in the rest of the code. Each time we do, the following happens:\nSo, the key properties of functions are:",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Organizing code</span>"
    ]
  },
  {
    "objectID": "chapters/python/organizing_your_code.html#functions",
    "href": "chapters/python/organizing_your_code.html#functions",
    "title": "9  Organizing code",
    "section": "",
    "text": "print(\"Functions are super, Functions are cool\")\nprint(\"When writing a program they are a great tool\")\nprint(\"La la dim du da da di\")\nprint(\"Skubi dubi dumdi di\")\nprint(\"Bing di dubi dum da di\")\n\nprint(\"Functions are used to package some code\")\nprint(\"They are not so strange that your head will explode\")\nprint(\"La la dim du da da di\")\nprint(\"Skubi dubi dumdi di\")\nprint(\"Bing di dubi dum da di\")\n\ndef chorus():\n    line1 = \"La la dim du da da di\"\n    line2 = \"Skubi dubi dumdi di\"\n    line3 = \"Bing di dubi dum da di\"\n    return line1 + '\\n' + line2 + '\\n' + line3\n\nprint(\"Functions are super, Functions are cool\")\nprint(\"When writing a program they are a great tool\")\nprint( chorus() )\n\nprint(\"Functions are useful to wrap up some code\")\nprint(\"They are not so strange that your head will explode\")\nprint( chorus() )\n\n\nWe define a function with the def keyword (which is short for “define” in case you wonder).\nAfter def, we write the function chorus. We could name it something else, but like good variable names, good function names can help you remember what your code does.\nAfter the name, you put two parentheses, ().\nThen a colon, :.\nThe statements that are part of the function are nested under the def statement and are indented with four spaces exactly like we do under if-statements.\nThe return statement ends the function. After the return keyword, the expression reduces to a value that the function returns.\n\n\n\n\nWhen a function is called, each statement in the definition is executed one after the other. If you look at the function definition, you can see that our chorus function has four statements.\nThe first statement assigns a string value to the line1 variable.\nThe second statement assigns a string value to the line2 variable.\nThe third statement assigns a string value to the line3 variable.\nThe fourth statement is a return statement. The expression after the return keyword in the final statement is reduced to a value, and this value is substituted for the function call. In this case, that value is the following string:\n\n\"La la dim du da da di\\nSkubi dubi dumdi di\\nBing di dubi dum da di\"\n\n\nA function names a piece of code (some statements) just like variables name values like strings and numbers.\nWe call a function by writing the function name followed by parentheses: chorus(). Just writing the function name will not call the function.\nWhen a function is called, it is substituted by the value that the function returns – exactly like a variable in an expression is substituted by its value. It is crucial that you remember this.\n\n\nExercise 9-1\nNow that we have a chorus function, that part is out of the way, and we can concentrate on our song without worrying about remembering how many “la la”s it has and so on. Try to change the “lyrics” in the chorus a little bit. Notice how you only need to make the change in one place to change all the choruses in the song – cool, right? Without the function, you would have to rely on correctly changing the code in many different places.\n\n\nExercise 9-2\nTry to delete the return statement in the chorus function (the last line in the function) and run the code again. You should see something like this:\nFunctions are super, Functions are cool\nWhen writing a program they are a great tool\nNone\nFunctions are used to wrap up some code\nThey are not so strange that your head will explode\nNone\nThe function call (chorus()) is now substituted with None. How can that be when we did not return anything? The reason is that when you do not specify a return statement, the function returns None by default. This is to honour the rule that variables and a function calls are substituted by a value, and None is simply the value that Python uses to represent “nothing”. None is a value denoting the lack of value. As you just saw, it represents that no value is returned from a function. It can also be assigned to a variable as a placeholder value until another value is assigned:\nx = None\nx = 4\nAlso, None is considered false in a logical context:\nprint(not None)\n\n\nExercise 9-3\nTry this variant to the chorus function. Go through the code slowly and repeat all the steps to break down what happens when a function is called. Remember also to do each substitution and reduction carefully.\ndef chorus():\n    line1 = 'La la'\n    line2 = 'Du bi du'\n    return line1 + '\\n' + line2\nDo the same for this variant:\ndef chorus():\n    line1 = 'La la'\n    line2 = 'Du bi du'\n    chrous_text = line1 + '\\n' + line2\n    return chrorus_text\nand for this variant:\ndef chorus():\n    return \"La la\\nDu bi du\"\n\n\nExercise 9-4\nWhat do you think happens if you move the definition of chorus to the bottom of your file? Decide what you think will happen and why (maybe you remember what happens when you try to use a variable in an expression before defining it?). Then try it out.\nprint(\"Functions are super, Functions are cool\")\nprint(\"When writing a program they are a great tool\")\nprint( chorus() )\n\nprint(\"Functions are useful to wrap up some code\")\nprint(\"They are not so strange that your head will explode\")\nprint( chorus() )\n\ndef chorus():\n    line1 = \"La la dim du da da di\"\n    line2 = \"Skubi dubi dumdi di\"\n    line3 = \"Bing di dubi dum da di\"\n    return line1 + '\\n' + line2 + '\\n' + line3\nEnsure you understand how the error you get relates to how Python runs your script (remember oath two?). If you still need help understanding, do the next exercise and then return to this one.\n\n\nExercise 9-5\nWhich error do you get here, and why? How is that similar to the error in the previous exercise?\nprint(x)\nx = 7\n\n\nExercise 9-6\nConsider the code below. Do all the substitution and reduction steps in your head. Remember that each function call is substituted by the value that the function returns. Then run it.\ndef lucky_number():\n    return 7\n\nx = lucky_number()\ny = lucky_number()\ntwice_as_lucky = x + y\nprint(twice_as_lucky)\nNow, change the code to that below. The code makes the same computation but in fewer steps. Do all the substitution and reduction steps.\ndef lucky_number():\n    return 7\n\ntwice_as_lucky = lucky_number() + lucky_number()\nprint(twice_as_lucky)\nNow, change the code to the one below. The code makes the exact computation but in fewer steps. Do all the substitution and reduction steps.\ndef lucky_number():\n    return 7\n\nprint(lucky_number() + lucky_number())",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Organizing code</span>"
    ]
  },
  {
    "objectID": "chapters/python/organizing_your_code.html#functions-can-take-arguments",
    "href": "chapters/python/organizing_your_code.html#functions-can-take-arguments",
    "title": "9  Organizing code",
    "section": "Functions can take arguments",
    "text": "Functions can take arguments\nThe functions we have written so far are not very flexible because they return the same thing every time they are called. Now write and run this beauty:\ndef square(number):\n    squared_number = number**2\n    return squared_number \n\nresult = square(3)\nprint(result)\nNotice how we put a variable (number) between parentheses in the function definition. This variable is assigned the value we put between the parentheses (3) when we call the function. So when we call like that (square(3)), Python automatically makes the assignment number = 3.\nHere is another example:\ndef divide(numerator, denominator):\n    result = numerator / denominator \n    return result\n\ndivision_result = divide(44, 77)\nprint(division_result)\nWhen the function call divide(44, 77), these two things implicitly happen: numerator = 44 and denominator = 77.\nTake note of the following three important points: 1. The values that we pass to the function in the function call (like 3, 44, and 77) are called arguments. It is crucial to remember that it is values and not variables that are passed to functions. 2. The variables in the definition line of a function, like number, numerator and denominator, are called parameters. They hold the values passed to the function when it is called (the arguments). 3. You can define functions with any number of parameters if you use the same number of arguments when you call the function.\n\nExercise 9-7\nTry to call your divide function like this divide(77, 44). What does it return, and what do you learn from it? Does the order of arguments and parameters correspond?\n\n\nExercise 9-8\nTry to call your divide function like this divide(44). Do you get an error, and what do you learn from that?\n\n\nExercise 9-9\nTry to call your divide function like this divide(44, 77, 33). Do you get a different error message, and what do you learn from that?\n\n\nExercise 9-10\nRead this code and do all substitution and reduction steps from beginning to end.\ndef square(x):\n    return x ** 2\n\nresult = square(9) + square(5)\nprint(\"The result is:\", result)\nNow replace the line return x ** 2 with print(x ** 2). What is printed now and why?\n\n\nExercise 9-11\nAs described above, a return statement ends the function by producing the value that replaces the function call. If a function has more than one return statement, then the function ends when the first one is executed.\ndef assess_number(x):\n    if x &lt; 3:\n        return 'quite a few'\n    if x &lt; 100:\n        return 'a lot'\n    return 'a whole lot'\n\nnr_apples = 2\nprint(nr_apples, \"apples is\", assess_number(nr_apples))\nWhat happens when x is 2, 3, 50, 200? Think about it first.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Organizing code</span>"
    ]
  },
  {
    "objectID": "chapters/python/organizing_your_code.html#functions-and-variables",
    "href": "chapters/python/organizing_your_code.html#functions-and-variables",
    "title": "9  Organizing code",
    "section": "Functions and variables",
    "text": "Functions and variables\nA function call is the temporary little world that only exists when the function is called and until it returns its value. It did not exist before the function was called, and it does not exist after the function returns its value. By necessity, the variables defined in your function are also temporary.\nThis means variables defined inside a function are private to each function call. It also means that variables defined inside functions are unavailable to code outside the function. Running the following example should help you understand this:\ndef make_greeting():\n    greeting = 'Guten tag'\n    name = 'Heinz' \n    message = greeting + \" \" + name\n    return message\n\ngreeting = 'Buongiorno'\nname = 'Giovanni'\nprint( make_greeting() )\nprint(greeting + \" \" + name)\nNotice how Heinz and Giovanni are greeted in their native languages. This means that the variable definitions inside the function do not overwrite the Italian versions already defined outside the function. This is because the variables defined in the function are temporary and private to the function, even if they have the same names as variables outside the function. This is why the function call make_greeting() in the print statement does not change the variables’ values printed in the last line.\nNow try to “comment out” the line greeting = 'Guten tag' and run the example again. All of a sudden, Heinz is greeted in Italian! The reason is that now Python cannot find a definition of greeting inside the function. It then looks outside the function for a definition and finds the Italian version.\nNow try to “comment out” the line greeting = 'Buongiorno' and run the example again. You get an error, but which one? Python complains that it cannot find a definition of greeting. The reason is that once the last print statement is executed, the small world of the function call in the previous line no longer exists.\nYou should learn two rules from the above example:\n\nAll variables you define inside a function are private to the function. If a variable in a function has the same name as a variable in the main script (like greeting above), then these are two separate variables that just happen to have the same name.\nWhen you use a variable like greeting in the function (E.g., message = greeting + \" \" + name), Python checks if the variables have been defined in the function. If that is not the case, then it will look for it outside the function. In the above example, name is found in the function, and greeting is found outside the function. It is good practice to make your functions “self-contained” in the sense that Python should not have to look outside the function for variables.\n\n\nExercise 9-12\nTry this version of the example above. Now, name is defined as a function parameter, but it is still a function variable, just like greeting.\ndef make_greeting(name):\n    greeting = \"Guten tag\"\n    message = greeting + \" \" + name\n    return message\n\ngreeting = 'Buongiorno'\nname = 'Giovanni'\nprint( make_greeting(\"Heinz\") )\nprint(greeting + \" \" + name)\n\n\nExercise 9-13\nConsider the following example:\ndef double(z):\n    return z * 2\n   \nx = 7\nresult = double(x)\nprint(result)\nWhen the function is called (double(x)), the x is substituted by its value 7. That value is passed as the argument and assigned to the function parameter z (z = 7). z is a private function variable and does not exist before or after the function call. Does this change in any way if we use the variable name x instead of z, as shown below?\ndef double(x):\n    return x * 2\n   \nx = 7\nresult = double(x)\nprint(result)\nDo all substations and reductions for each line of code from top to bottom. Keep the sequence of events in mind and remember that a function definition is merely a template describing a mini-world that is created anew every time the function is called.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Organizing code</span>"
    ]
  },
  {
    "objectID": "chapters/python/organizing_your_code.html#builtin-functions",
    "href": "chapters/python/organizing_your_code.html#builtin-functions",
    "title": "9  Organizing code",
    "section": "Builtin functions",
    "text": "Builtin functions\nSo far we have only talked about functions you write yourself, but Python also has built-in functions that are already available to you. They work just like a function you would write yourself. You already know the print function quite well; that is an example of a function that prints something but returns None. There are many other useful built-in functions, but I will tell you about another two: len and type.\n\nExercise 9-14\nTry these examples:\nx = 'Banana'\nprint(\"The value of variable x is of type\", type(x))\nprint(\"The value of variable x has length\", len(x))\nAs you can see, type returns the type of the value passed as the argument, and len returns the length of the value passed as the argument. The type function is handy if you wonder what type a value has, but it is not a function we will use in this course. The len function, however, is your new best friend. You will see why soon enough.\n\n\nExercise 9-15\nTry to change the value of the x in Section 9.0.0.14 to an integer or a float and see what happens when you run it. Do you get an error? Does it make sense that not all types of values can meaningfully be said to have a length?\n\n\nExercise 9-16\nWhat happens if you pass an empty string (\"\") as the argument to the len function?\n\n\nExercise 9-17\nWhat is printed here? Think about it first, and then try it out. Remember to do the substitution and reduction steps.\nreturn_value = print(\"Hello world\")\nprint(return_value)\n\n\nExercise 9-18\nWhat is printed here? Think about it firs,t and then try it out. Remember to do the substitution and reduction steps.\nprint(print(\"Hello world\"))",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Organizing code</span>"
    ]
  },
  {
    "objectID": "chapters/python/organizing_your_code.html#general-exercises",
    "href": "chapters/python/organizing_your_code.html#general-exercises",
    "title": "9  Organizing code",
    "section": "General exercises",
    "text": "General exercises\nThe following exercises treat the areas we have worked on in this and previous chapters. They are meant to train your familiarity with if-statements and functions. Remember that the purpose of the exercises is not to answer the questions but to train the chain of thought that allows you to answer them. Play around with the code for each example and see what happens if you change it a bit.\n\nExercise 9-19\nConsider this function definition that takes a single number as the argument:\ndef square(n):\n    return n**2\nWhat does it do? What does it return? What number does square(2) then represent?\nBelow, I have used it in some printed expressions. Make sure you understand what each expression evaluates to. Make the explicit substitutions and reductions on paper before you run it. Remember that we substitute a function call (like square(2)) for the value it returns, just like a variable x substitutes for the value it points to.\nprint(square(3))\nprint(square(2 + 1))\nprint(square(2) * 2 + square(3))\nprint(square(square(2)))\nprint(square(2 * square(1) + 2))\n\n\nExercise 9-20\nWhat does this function do? How many parameters does it have? How many statements does the function have? What does the function print? Which value does it return?\ndef power(a, b):\n    print(\"This function computes {}**{}\".format(a, b))\n    return a**b\n\nprint(power(4, 2))\nTry (possibly strange) variations of the code like the ones below better to understand the contribution of each line of code. What is the difference between return and print? What happens when Python gets to a return statement in a function? What happens when the function does not have a return statement?\nVariation 1:\ndef power(a, b):\n    print(\"This computes\", a, \"to the power of\", b)\n    print(a**b)\n\nresult = power(4, 2)\nprint(result)\nVariation 2:\ndef power(a, b):\n    print(\"This computes\", a, \"to the power of\", b)\n    return a**b\n\nresult = power(4, 2)\nprint(result)\nVariation 3:\ndef power(a, b):\n    print(\"This computes\", a, \"to the power of\", b)\n    a**b\n\nprint(power(4, 2))\nVariation 4:\ndef power(a, b):\n    return a**b\n    print(\"This computes\", a, \"to the power of\", b)\n\nprint(power(4, 2))\n\n\nExercise 9-21\nDefine a function called diff, with two parameters, x and y. The function must return the difference between the values of x and y.\nExample:\ndef diff(x, y):\n    ...\n\ndiff(8, 2) # should return 6\ndiff(-1, 2) # should return -3\nSave the value returned from the function in a variable. Then, test if the function works correctly by comparing the result to what you know is the true difference (using ==).\n\n\nExercise 9-22\nDefine a function called all_equal that takes five arguments and returns True if all five arguments have the same value and False otherwise. The function should work with any input, for example:\nall_equal(\"Can\", \"Can\", \"Can\", \"Can\", \"Can\")\nall_equal(0, 0, 0, 0, 0)\nall_equal(0.5, 0.5, 0.5, 0.5, 0.5)\nall_equal(True, True, True, True, True)\nHint: You test equality with a == b. Now, think back to what you learned about logic. Which operator can you use to test if a == b and b == c?\n\n\nExercise 9-23\nDefine a function called is_even, which takes one argument and returns True if (and only if) this is an even number and False otherwise (remember the modulo operator?).\nis_even(8) # should return True\nis_even(3) # should return False\n\n\nExercise 9-24\nDefine a function called is_odd, which takes one argument and returns True if (and only if) the argument is an odd number and False otherwise.\nis_odd(8) # should return False\nis_odd(3) # should return True\nCan you complete this exercise using the is_even you defined in Section 9.0.0.23? How? Why is that a good idea?\n\n\nExercise 9-25\nHere is a function that should return True if given an uppercase (English) vowel and False otherwise:\ndef is_uppercase_vowel(c):\n    c == 'A' or c == 'E' or c == I or c == 'O' or c == 'U'\n    \nchar = 'A' \nif is_uppercase_vowel(char):\n    print(char, \"is an uppercase vowel\")\nelse:\n    print(char, \"is NOT an uppercase vowel\")\nNow you can just type the code exactly as shown and run it. Do you get what you expect? Does the code work? If not, try to figure out why. Try to print the value that the function returns. Do you know if that gives you any hints about the cause of the problem?\n\n\nExercise 9-26\nDefine a function called is_nucleotide_symbol, which takes one argument and returns True if this is either A, C, G, T, a, c, g or t, and False in any other case.\nName your parameter something sensible like symbol.\nis_nucleotide_symbol(\"A\") # should return True\nis_nucleotide_symbol(\"B\") # should return False\nis_nucleotide_symbol(\"Mogens\") # should return False\nis_nucleotide_symbol(\"\") # should return False\n\n\nExercise 9-27\nDefine a function called is_base_pair which takes two parameters, base1, base2, and returns True if base2 is the complementary of base1, and False otherwise.\nis_base_pair(\"A\", \"G\") # should return False\nis_base_pair(\"A\", \"T\") # should return True\nis_base_pair(\"T\", \"A\") # should return True\nis_base_pair(\"Preben\", \"A\") # should return False\n\n\nExercise 9-28\nDid you find the bug in Section 9.0.0.25? You were supposed to find that the function did not have a return value. This makes the function return None by default. Do you think the None value is considered true or false in an if-statement?\n\n\nExercise 9-29\nDefine a function called celcius2fahrenheit that converts from celsius to Fahrenheit. You can do this because you know the linear relationship between the two. On Figure 9.1 you can see that the slope is 9 / 5 and the intercept is 32. The function should have one parameter celsius. Inside the function, you should define the variables slope and intercept and give them the appropriate values. Then you can calculate the conversion to Celcius using these variables and return the result.\n\n\n\n\n\n\nFigure 9.1: Temperature conversion\n\n\n\n\n\nExercise 9-30\nTry to change your conversion function so it takes three arguments, corresponding to celsius, slope and intercept so you can call it like this to convert 27 degrees celsius: conversion(37, 9 / 5, 32). Now you have a function that can do any linear conversion that you can put inside another function like this:\ndef celcius2fahrenheit(celsius):\n    return conversion(celsius, 9 / 5, 32)\n\n\nExercise 9-31\nNow try to extend this to a different problem: It has been found that the height and weight of a person are related by a linear equation with slope = 0.55 and intercept = -25. Define a function called predict_weight which takes just one argument, the height of a person, and returns the estimated weight of the person.\n\n\nExercise 9-32\nBy now you know that some of the words in your code have specific purposes. def defines functions, return returns value from a function, and is a logical operator etc. Here is a list of the ones you will see in this course: and, assert, break, continue def, del, elif, else, False, for, from, if, import, in, is, not, or, pass, return, while, True, None (you can see a full list here)\nThese words are reserved for their special purposes in Python and you will not be allowed to assign values to them. Try this to see for yourself:\nNone = 4\nor this:\nand = 2\n\n\nFAQ - Frequently Asked Questions\nQ: Can function names be anything?\nA: Just about. The rules that apply to variable names also apply to function names. Good function names are lower case with underscores (_) to separate words, like in the examples above.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Organizing code</span>"
    ]
  },
  {
    "objectID": "chapters/python/python_values_are_objects.html",
    "href": "chapters/python/python_values_are_objects.html",
    "title": "10  Values are objects",
    "section": "",
    "text": "Methods\nThis chapter introduces the notion of an object, one of the most central aspects of Python. Once you catch your breath, you will love that all Python values are objects.\nIn Python, a value like an integer or string not only holds data. It is also packaged with a lot of useful functionality relevant to the particular type of value. When a value is packaged with such relevant functionality and meta information, programmers call it an object—and in Python, all values are objects.\nThe associated functionality comes in the form of methods. You can think of methods as functions that are packaged together with the value. For example, string values have a method called capitalize. Try it out:\nTo call the method on the string value, you connect the string value and the method call with a dot. So, to call a method on a value, you do the following:\nYou can see that the method call looks just like a function call, and in many ways, calling a method works much like calling a function. The difference is that when we call a function, we say: “Hey function, capitalize this string!”. When we call a method, we say: “Hey string, capitalize yourself!”\nSo why do we need methods? Why do we need them when we have functions? It is very handy to have some relevant and ready-to-use functionality packaged together with the data it works on. You will start to appreciate that sooner than you think.\nMethods are almost always used with variables. So remember to make any substitutions and reductions required. When we put a method call after a variable like below, the variable is first substituted for its value, and then the method is called on the value. Consider the second line of this example:\nHere, x is first substituted by \"banana\" and then the method is called on that value, like this: \"banana\".capitalize().\nNow write and run these examples:\nYou can see what these methods do. For example: upper returns an uppercased copy of the string.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Values are objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/python_values_are_objects.html#methods",
    "href": "chapters/python/python_values_are_objects.html#methods",
    "title": "10  Values are objects",
    "section": "",
    "text": "x = \"banana\".capitalize()\nprint(x)\n\n\nWrite the value (or a variable name that substitutes for a value).\nThen write a ..\nThen write the name of the method (like capitalize).\nThen, write two parentheses to call the method. If the method takes any arguments other than the value it belongs to, then you write those additional values between the parentheses with commas in between, just as when you call a function.\n\n\n\n\nx = \"banana\"\nprint(x.capitalize())\n\n\nmessage = \"Methods Are Cool\"\nprint(message)\n\nshout = message.upper()\nprint(shout)\n\nwhisper = message.lower()\nprint(whisper)\n\nnew_message = message.replace(\"Cool\", \"Fantastic\")\nprint(new_message)\n\n\nExercise 10-1\nWrite and run the following code. What do you think it does?\nline = '\\n\\tSome text\\n'\nprint(\"&gt;{}&lt;\".format(line))\n\nline = line.strip()\nprint(\"&gt;{}&lt;\".format(line))\nMake sure you do the substitution and reduction steps in your head. Be especially careful about the third line of code. Also, what do you think the special \\t character is?\n\n\nExercise 10-2\nThe string methods you have tried so far have all returned a new string. Try this example:\n'ATGACGCGGA'.startswith('ATG')\nand this\n'ATGACGCGGA'.endswith('ATG')\nWhat do the methods do, and what do they return?",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Values are objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/python_values_are_objects.html#using-the-python-documentation",
    "href": "chapters/python/python_values_are_objects.html#using-the-python-documentation",
    "title": "10  Values are objects",
    "section": "Using the Python documentation",
    "text": "Using the Python documentation\nNow that you are well underway to becoming a programmer, you should know your way around the Python documentation. Especially the part called the Python standard library. There is a lot of details in there that we do not cover in this course. These are mainly tools and techniques for writing more efficient, extensible, robust, and flexible code. The parts we cover in this course are the minimal set that will allow you to write a program that can do anything.\n\nExercise 10-3\nThere is a string method that returns a secret agent:\nprint('7'.zfill(3))\nYou can look it up in the Python documentation.\n\n\nExercise 10-4\nBrowse through all the string methods to get an impression of all the functionality that is packaged with string objects.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Values are objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/python_values_are_objects.html#string-formatting",
    "href": "chapters/python/python_values_are_objects.html#string-formatting",
    "title": "10  Values are objects",
    "section": "String formatting",
    "text": "String formatting\nYou have already tried string formatting in Section 10.0.0.1. String formatting is a simple but powerful technique that lets you generate pretty strings from pre-computed values. You may have noticed that many decimals are shown every time we print a floating-point number. It is not very pretty if you are only interested in two decimals. You use the format method (surprise) to format a string. In its most straightforward use, format replaces occurrences of {} with the arguments that are passed to it - like this:\ntaxon = \"genus:{}, species:{}\".format('Homo', 'sapiens')\n\nExercise 10-5\nWhat happens if you try this?\nquestion = \"Was {} {} Swedish?\".format('Carl', 'Linneaus')\nand this?\nquestion = \"Was {} Swedish?\".format('Carl', 'Linneaus')\nand this?\nquestion = \"Was {} {} Swedish?\".format('Carl Linneaus')\nIn the two last examples, the number of {} did not match the number of arguments to format. What happens when there are too few and when there are too many?\n\n\nExercise 10-6\nConsider this code:\ns = \"{} is larger than {}\".format(4, 3)\nprint(s)\nWhat will happen if you run this code? Write the code and see for yourself once you think you know. If you were wrong, make sure you understand why.\n\n\nExercise 10-7\nConsider this code:\nlanguage = 'Python'\ninvention = 'sliced bread'\ns = '{} is the best thing since {}'.format(language, invention)\nprint(s)\nWhat will happen if you run this code? Write the code and see for yourself once you think you know. If you were wrong, make sure you understand why.\n\n\nExercise 10-8\nConsider this code:\nmy_template = '{} is the best thing since {}'\nlanguage = 'Python'\nprint(my_template.format(language, 'sliced bread'))\nprint(my_template.format(language, 1900 + 89))\nWhat will happen if you run this code? Do the substitution and reduction steps in your head.\n\n\nExercise 10-9\nThink back to Section 5.0.0.3, where you calculated how many cookies you could buy for 30 kr. The bars are 7 kr. So your program looked something like this:\nnr_bars = 30 / 7\nprint('I can buy', nr_bars, 'chocolate bars!')\nand it ran like this: python chocolate.py\nI can buy 4.285714285714286 chocolate bars!\nString formatting lets you rewrite the program like this:\nnr_bars = 30 / 7\nmessage = \"I can buy {} chocolate bars!\".format(nr_bars)\nprint(message)\nTry to replace {} with {:.2f}. format reads the stuff after the colon in each set of curly brackets and uses it as directions for formatting the value it inserts. Try it out and see what happens if you change the number 2 to 3, 4, 5 or 10.\n\n\nExercise 10-10\nSee if you can find the documentation for the format function in the Python documentation. It can do wondrous things, for this course we will only try to control the number of digits and padding with spaces. Look at the examples below. Maybe you can figure out how it works.\npi = 3.14159265359\nprint(\"*{}*\".format(pi))\nprint(\"*{:.3f}*\".format(pi))\nprint(\"*{:.6f}*\".format(pi))\nprint(\"*{:&gt;5.3f}*\".format(pi))\nprint(\"*{:&gt;10.3f}*\".format(pi))\n\n\nExercise 10-11\nThis is bonus info rather than an actual exercise. How do you think Python can figure out that adding strings is supposed to work differently than adding numbers? Remember that '1' + '2' is '12' not 3. The answer is that all values you can add with the + operator have a secret method called __add__ that defines how adding works for that type of value:\ns1 = \"11\"\ns2 = \"22\"\nn1 = 11\nn2 = 22\nprint(s1 + s2)\nprint(s1.__add__(s2))\nprint(n1 + n2)\nprint(n1.__add__(n2))\nThis is one of many examples of how objects allow Python to implement functionality that fits each value type. This was to show how Python does this. Like yellow and black stripes in nature means “don’t touch me!” – double underscores (__) is Python’s way of saying “do not use this!”. You are supposed to use the + operator, not the __add__ method.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Values are objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/python_values_are_objects.html#indexing-and-slicing-strings",
    "href": "chapters/python/python_values_are_objects.html#indexing-and-slicing-strings",
    "title": "10  Values are objects",
    "section": "Indexing and slicing strings",
    "text": "Indexing and slicing strings\nAnother feature of string objects is that they allow you to extract individual parts of the string.\nEach character in a string is identified by its index in the string. To access a character in a list, you write brackets after the string. Between those brackets, you specify the index of the character you want. The first character has an index of 0; the second has an index of 1, and so on.\ncodon = 'ATG'\nprint(\"first base is\", codon[0])\nprint(\"second base is\", codon[1])\nprint(\"third base is\", codon[2])\nYou may wonder why the index of the first character is zero and not one. That is simply the convention in programming and is so for good reason. Over time you will begin to find this useful rather than annoying. You should think of the index as the offset from the start of the string.\nThat also means that the index is not the length of the string but the length minus one:\namino_acids = 'ARNDCQEGHILKMFPSTWYV'\nlast_index = len(amino_acids)-1\nprint(\"Last amino acid is\", amino_acids[last_index])\nIf you want a sub-string from a larger string (we call that a slice), you specify a start index and an end index separated by a colon:\nprint(amino_acids[1:4])\nWhen you run that, you can see that amino_acids[1:4] is substituted for 'RND', so the slicing operation produces a sub-string of amino_acids. You may wonder why the value at index 4 is not in the resulting sub-string. That is another programming convention: intervals are ends exclusive. So when you specify an interval with a start index of 1 and an end index of 4, it represents all the characters starting from 1 and up to, but not including, 4. So, the slice 1:4 corresponds to the characters at indexes 1, 2, and 3. The reason programmers handle intervals in this way is that it makes it easier to write clear and simple code as you will see in the exercises.\n\nExercise 10-12\nWhat does this expression reduce to?\n\"Futterwacken\"[7]\n\n\nExercise 10-13\nWhat is printed here? Do all the substitution and reduction steps and compare to the exercise above.\ns = \"Futterwacken\"\nprint(s[7])\n\n\nExercise 10-14\nWhat is printed here? Do all the substitution and reduction steps – and do it twice. Next week you will be happy you did.\ndna = 'TGAC'\ni = 0\nprint(dna[i])\ni = 1\nprint(dna[i])\ni = 2\nprint(dna[i])\n\n\nExercise 10-15\nWhat do you think happens here? Make up your mind and try out the code below:\ns = \"Futterwacken\"\ns[6] = 'W'\nDid you see that coming? Strings are immutable, which means that you cannot change them once you have made them. If you want \"FutterWacken\" you need to produce a new string with that value. Try to figure out how to do that with the replace method of strings.\n\n\nExercise 10-16\nWhen you do not specify a slice’s start and/or end, Python will assume sensible defaults for the start and end indexes. What do you think they are? Make up your mind and try out the code below:\ns = 'abcdefghijklmnopqrstuvxyz'\nprint(s[:11])\nprint(s[11:])\nprint(s[:])\n\n\nExercise 10-17\nFind the documentation for how the slicing of strings works.\n\n\nExercise 10-18\nWhat do you think happens when you specify an index that does not correspond to a value in the list:\nalphabet = 'abcdefghijklmnopqrstuvxyz'\nprint(alphabet[99])\nRead and make sure you understand the error message. You can try to Google the error message.\n\n\nExercise 10-19\nDo you think you also get an error when you specify a slice where the end is too high? Try it out:\nalphabet = 'abcdefghijklmnopqrstuvxyz'\nprint(alphabet[13:99])\nand this:\nalphabet = 'abcdefghijklmnopqrstuvxyz'\nprint(alphabet[10000:10007])\nI guess that is worth remembering.\n\n\nExercise 10-20\nWhich character in a string named alphabet does this expression reduce to?\nalphabet[len(alphabet)-1]\n\n\nExercise 10-21\nBecause intervals are “ends exclusive” ,we can compute the length of a slice as end - start:\ndna = \"ATGAGGTCAAG\"\nstart = 1\nend = 4\nprint(\"{} has length {}\".format(dna[start:end], end-start))\nFigure out what this code would look like if ends were included in intervals.\n\n\nExercise 10-22\nAnother advantage of “ends exclusive” intervals is that you only need one index to split a string in two:\ns = 'Banana'\nidx = 3\nbeginning = s[:idx]\nend = s[idx:]\nprint(beginning + end)\nFigure out what indexes you would need to use to split a sequence in two if ends were included in intervals.\n\n\nExercise 10-23\nDid you look up the details of how slicing works in Section 10.0.0.17? Then you should be able to explain what happens here:\ns = 'zyxvutsrqponmlkjihgfedcba'\nprint(s[::-1])",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Values are objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/python_values_are_objects.html#general-exercises",
    "href": "chapters/python/python_values_are_objects.html#general-exercises",
    "title": "10  Values are objects",
    "section": "General exercises",
    "text": "General exercises\n\nExercise 10-24\nWill this print Bananas rule!? Do all the substitutions and reductions.\nif 'na' * 2 == \"Banana\"[2:]:\n    print(\"Bananas rule!\")\n\n\nExercise 10-25\nWill this print Bananas rule!? Do all the substitutions and reductions.\nif \"{}s\".format('Banana'[1:].capitalize()) == 'Ananas':\n    print(\"Bananas rule!\")\n\n\nExercise 10-26\nWrite a function called even_string that takes a string argument and returns True if the length of the string is an even number and False otherwise. E.g. even_string('Pear') should return True and even_string('Apple') should return False (remember the modulo operator?).\n\n\nExercise 10-27\nLook at the code below and decide what is printed at the end. Then, write the code and test your prediction. If you are wrong, figure out why by revisiting the chapter about functions.\ndef enigma(x):\n    if x == 4:\n        return x\n\nresult = enigma(5)\nprint(result)\n\n\nExercise 10-28\nInspect the code below and determine why it does not print that you are a super star. Test the function using various inputs and identify the mistake.\ndef even_number(x):\n    if x % 2:\n        return False\n\nif even_number(4):\n    print('You are a super star!')",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Values are objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/lists.html",
    "href": "chapters/python/lists.html",
    "title": "11  Lists of things",
    "section": "",
    "text": "Lists\nThis chapter is about lists and dictionaries that are Python values that can contain other Python values. Lists and dictionaries let you build relationships between values, which is what data structures represent.\nFor many kinds of data, the order of things is important. Just like the order of characters is important for the meaning of the text in a string, we sometimes want to specify the order of other things because the relative order of items in the list has some meaning. It could be a grocery list where you have listed the things to buy in the order you get to them in the supermarket. This is where Python lists are helpful. When you print a list, it nicely shows all the values it contains.\nUnlike strings that can only store the order of characters, lists can contain any kind of values, and you can mix different types of values in any way you like. Here is a list that contains an integer, a boolean, a string, and a list:\nBy now, you have probably guessed you will make a list with two square brackets. Between them, you can put values with commas in between. A list is a container of other values, and the value of the list itself does not depend on the values it contains. This makes sense. Otherwise, an empty list would not have a value:\nYou can add single values to the end of a list using the append method of lists. Try it out:\nIf you have a list you want to add to the end of another list, you use the extend method:\nNotice how append and extend modifies the existing list instead of producing a new list with the added element.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Lists of things</span>"
    ]
  },
  {
    "objectID": "chapters/python/lists.html#lists",
    "href": "chapters/python/lists.html#lists",
    "title": "11  Lists of things",
    "section": "",
    "text": "grocery_list = [\"salad\", \"canned beans\", \"milk\", 'beer', 'candy']\nprint(grocery_list)\n\nmixed_list = [42, True, 'programming', [1, 2, 3] ]\n\nmy_list = []\n\ndesserts = []\nprint(desserts)\ndesserts.append('Crepe suzette')\nprint(desserts)\ndesserts.append('Tiramisu')\nprint(desserts)\ndesserts.append('Creme brulee')\nprint(desserts)\n\ncheeses = ['Gorgonzola', 'Emmentaler', 'Camembert']\ndesserts.extend(cheeses)\nprint(desserts)\n\n\nExercise 11-1\nDo you think this will work?\ncheeses = ['Gorgonzola', \n           'Emmentaler', \n           'Camembert']\nprint(cheeses)\nSurprised? Code inside parentheses, brackets, and braces can span several lines, sometimes making your code easier to read.\n\n\nExercise 11-2\nYou use the’ in’ operator to test if a value is in a list. Try this:\nprint('Tiramisu' in desserts)\nprint('Meatloaf' in desserts)\n\n\nExercise 11-3\nYou can concatenate two lists to produce a new joined list. Make sure you figure out how this works before you try it. Then, experiment with changing the lists. Can you concatenate two empty lists?\nsome_list = [1, 2, 3]\nanother_list = [7, 8, 9]\nmerged_list = some_list + another_list\nprint(merged_list)\nThis is yet another example of how the functionality of Python objects lets them “know” how to behave under different circumstances, such as when adding two objects (see Section 10.0.0.11).\n\n\nExercise 11-4\nWhat do you think is printed here? Make sure you figure out how you think this works before you try it out. What does the append method return?\nmy_list = []\nx = my_list.append(7)\nprint(x)\nprint(my_list)",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Lists of things</span>"
    ]
  },
  {
    "objectID": "chapters/python/lists.html#indexing-and-slicing-lists",
    "href": "chapters/python/lists.html#indexing-and-slicing-lists",
    "title": "11  Lists of things",
    "section": "Indexing and slicing lists",
    "text": "Indexing and slicing lists\nNow you know how to make lists, but to work with the values in lists, you must also know how to access the individual values a list contains. Fortunately, indexing lists work just like indexing strings: Each value in a list is identified by an index exactly like each character in a string:\nnumbers = [7, 4, 6, 2, 8, 1]\nprint(\"first value is\", numbers[0])\nprint(\"second value is\", numbers[1])\nprint(\"third value is\", numbers[2])\nNotice that the function len can also compute the length of a list. So you also get the last value in a list like this:\nnumbers = [7, 4, 6, 2, 8, 1]\nlast_index = len(numbers)-1\nprint(\"Last element is\", numbers[last_index])\nIf you want a sub-list of values from a list (we also call that a slice), you specify a start index and an end index separated by a colon, just like with strings:\nprint(numbers[1:4])\nWhen you run that, you can see that numbers[1:4] is substituted for [4, 6, 2], so the slicing operation produces a new list of the specified values.\n\nExercise 11-5\nWhat do these two expressions reduce to?\n[11, 12, 13, 14, 15, 16, 17][2]\n\n\nExercise 11-6\nWhat is printed here? Do all the substitution and reduction steps and compare them to the exercise above.\nl = [11, 12, 13, 14, 15, 16, 17] \nprint(l[2])\n\n\nExercise 11-7\nWhat is printed here? Do all the substitution and reduction steps — and do them twice. Next week, you will be happy you did.\nnumbers = [1,2,3]\ni = 0\nprint(number[i])\ni = 1\nprint(number[i])\ni = 2\nprint(number[i])\n\n\nExercise 11-8\nWhat do you think happens here? Make up your mind and try out the code below:\nl = [11, 12, 13, 14, 15, 16, 17] \nl[4] = \"Donald\"\nprint(l)\nWere you surprised by what happened? Compare to Section 10.0.0.15. Lists are not immutable like strings, and you can replace values by assigning a new value to an index in the list.\n\n\nExercise 11-9\nWith your knowledge of slicing, what do you think is printed below:\nl = [11, 12, 13, 14, 15, 16, 17]\nprint(l[:3])\nprint(l[3:])\nprint(l[:])\n\n\nExercise 11-10\nWhat do you think happens when you specify an index that does not correspond to a value in the list:\nl = [11, 12, 13, 14, 15, 16, 17] \nprint(l[7])\nRead and understand the error message. Does it ring a bell?\n\n\nExercise 11-11\nDo you also get an error when you specify a slice where the end is too high? Try it out:\nl = [11, 12, 13, 14, 15, 16, 17]\nprint(l[4:99])\nI guess that is also worth remembering.\n\n\nExercise 11-12\nWhich value in a list named l does this expression reduce to?\nl[len(l)-1]\n\n\nExercise 11-13\nIf you do not like Emmentaler, you can delete it. What does the del keyword do?\ncheeses = ['Gorgonzola', 'Emmentaler', 'Camembert']\nprint(cheeses)\ndel cheeses[1]\nprint(cheeses)\n\n\nExercise 11-14\nBecause intervals are “ends exclusive,” we can compute the length of a slice as end - start:\nl = [7, 4, 6, 2, 8, 1]\nstart = 1\nend = 4\nprint(\"{} has length {}\".format(l[start:end], end-start))\nConsider what this code would look like if ends were included in intervals.\n\n\nExercise 11-15\nAnother advantage of “ends exclusive” intervals is that you only need one index to split a list in two:\nnumbers = [1, 2, 3, 4, 5, 6]\ni = 3\nbeginning = numbers[:i]\nend = numbers[i:]\nprint(beginning + end)\nIf ends were included in intervals, this would be more complex.\n\n\nExercise 11-16\nDo all the substitution and reduction steps in your head (or on paper) before you write any of the following code. Think carefully and decide what you think will be printed below. Remember that the value of a list is a container that holds other values in it. Then, write the code and see if you are right. If you were not, figure out what led you to the wrong conclusion.\nx = 'A'\ny = 'B'\nz = 'C'\nlst = [x, y, z]\nprint(lst)\n\nx = 'Preben'\nprint(lst) # what is printed here? \n\nlst[0] = 'Mogens'\nprint(lst) # what is printed here?\n\n\nExercise 11-17\nDo you remember this trick from string slicing?\nl = [1, 2, 3, 4, 5]\nprint(l[::-1])\n\n\nExercise 11-18\nYou can produce a list by splitting a long string into smaller parts. Think: “Hey string, split yourself on this smaller string”. Try these variations to figure out how it works.\n\"Homo sapiens neanderthalensis\".split(\" \")\n\"Homo sapiens neanderthalensis\".split('en')\n'ATGCTCGTAACGACACTGCACTACTACAATAG'.split('')\n\"1, 2, 3, 5, 3, 2, 5, 3\".split(', ')\n\"1,2,3,5,3,2,5,3\".split(',')\n'ATGCTCGTAACGACACTGCACTACTACAATAG'.split()\n\"Homo sapiens neanderthalensis\".split()\nNotice that the method has a default behavior when no argument is passed to it.\n\n\nExercise 11-19\nYou can produce a string by joining the elements of a list (if all the elements are strings, of course). Think: “Hey string, put yourself between all the strings in this list”.\n\"-\".join(['Homo', 'sapiens', 'neanderthalensis'])\n\"...\".join(['Homo', 'sapiens', 'neanderthalensis'])\n\"\".join(['A', 'T', 'G'])\nNotice how you can join something on an empty string. This is a very useful technique to turn a list of characters into a string.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Lists of things</span>"
    ]
  },
  {
    "objectID": "chapters/python/lists.html#general-exercises",
    "href": "chapters/python/lists.html#general-exercises",
    "title": "11  Lists of things",
    "section": "General exercises",
    "text": "General exercises\nWhat does this expression reduce to? 'aaaaa', 'BaBaBa', or 'Banan'. Make up your mind, and then run the expression to check.\n'a'.join('Banana'.split('a')[:3] * 4)[-5:]",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Lists of things</span>"
    ]
  },
  {
    "objectID": "chapters/python/dictionaries.html",
    "href": "chapters/python/dictionaries.html",
    "title": "12  Pairs of things",
    "section": "",
    "text": "Dictionaries\nThis chapter is about dictionaries that, like lists, is another Python value that can contain other Python values. Dictionaries dictionaries let you build relationships between values, which is what data structures represent.\nLists are useful for storing values when the order of the values is important, but they have one drawback: you can only access a value in a list using its index.\nA dictionary called dict in Python, is a much more flexible data type. Like a list, a dictionary is a container for other values, but dictionaries do not store values in sequence. They work more like a database that lets you store individual values. When you store a value, you assign it to a key that you can use to access the stored value. Now, create your first dictionary:\nThis dictionary has three values ('Actor', 'Robert Redford' and 179) and each value is associated with a key. Here 'height' is the key for the value 179. So, when defining a dictionary, you should note the following:\nTo access a value in the dictionary, you put its key in square brackets after the dictionary:\nHere we used strings as keys, but you can also use many types of values as keys (Python will give you an error if you try to use a type that is not allowed):\nA dictionary stores key-value pairs but does not keep track of their order. So, when you print a dictionary, the order of the key-value pairs is arbitrary.\nIf you have a dictionary, you can add key-value pairs in this way:\nNotice that if you assign a value (71) to a key that is already in the dictionary ('age'), then the old value (70) is replaced.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pairs of things</span>"
    ]
  },
  {
    "objectID": "chapters/python/dictionaries.html#dictionaries",
    "href": "chapters/python/dictionaries.html#dictionaries",
    "title": "12  Pairs of things",
    "section": "",
    "text": "person = {'name': 'Robert Redford', 'height': 179, 'job': 'Actor'}\n\n\nYou make a dictionary using braces.\nyou put key-value pairs separated by a colon between your braces.\nCommas separate the key-value pairs.\nTo make an empty dictionary, write the braces with nothing between them: {}.\n\n\n\"{} is a {} cm {}\".format(person['name'], person['height'], person['job'])\n\nmisc_dict = {42: \"Meaning of life\", \"pi\": 3.14159, True: 7}\n\n\nperson['job'] = 'Retired'\nperson['hair'] = 'uniquely combed'\nprint(person)\n\n\nExercise 12-1\nWhat does this expression evaluate to?\n{'name': 'Robert Redford', 'height': 179, 'job': 'Actor'}['name']\n\n\nExercise 12-2\nAssuming the definition of the person dictionary above, what does this expression evaluate? Compare this to the expression in the previous exercise.\nperson['name']\n\n\nExercise 12-3\nThe in operator also works with dictionaries. Look at what these expressions reduce to and then try to figure out what in does when applied to a dictionary:\n'name' in person\n'height' in person\n'job' in person\n84 in person\n'Actor' in person\n'Robert Redford' in person\n\n\nExercise 12-4\nWrite and run this code with different values of key and read any error messages.\nkey = 3\n# key = 'banana'\n# key = 3.14159\n# key = True\n# key = {}\n# key = []\nd = {}\nd[key] = 7\nAre any of the values not allowed as keys?\n\n\nExercise 12-5\nDo you think this will work?\nperson = {'name': 'Robert Redford', \n                'height': 179,\n                'job': 'Actor'}\nprint(person)",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pairs of things</span>"
    ]
  },
  {
    "objectID": "chapters/python/dictionaries.html#general-exercises",
    "href": "chapters/python/dictionaries.html#general-exercises",
    "title": "12  Pairs of things",
    "section": "General exercises",
    "text": "General exercises\n\nStart by making dictionaries for (some of) the Trump family:\ndonald = {'name': 'Donald Trump', 'age': 70, 'job': 'President' }\nmelania = {'name': 'Melania Trump', 'age': 70, 'job': 'First lady' }\ntiffany = {'name': 'Tiffany Trump', 'age': 23, 'job': 'Internet personality' }\nivanka = {'name': 'Ivanka Trump', 'age': 35, 'job': 'Top aide' }\n\nExercise 12-6\nWhat do you think the following code produces? Do all of the substitution and reduction steps in your head, and only then try out the code.\ndonald['child'] = tiffany\nmelania['husband'] = donald\n\nprint(melania)\nprint(melania['husband']['child'])\n\n\nExercise 12-7\nA dictionary can contain any kind of Python values, even lists or dictionaries. Consider the code below, where we add a list of ex-wives to the Trump persona. Can you see why we need to check the 'ex-wives' key before we add it to the list of ex-wives?\ndonald = {'name': 'Donald Trump', 'age': 70, 'job': 'President' }\n\nif 'ex-wives' not in donald:\n    donald['ex-wives'] = []\ndonald['ex-wives'].append('Marla Maples')\ndonald['ex-wives'].append('Ivana Trump')\n\nprint(donald)\n\n\nExercise 12-8\nIn case you wonder what the type of value a list is, or a dictionary, try this:\nprint(\"A list has type:\", type([]))\nprint(\"A dictionary has type:\", type({}))\nNow the types list and dict are your friends too.\n\n\nExercise 12-9\nLists can also contain any type of value. Consider this example. What do you think the following code produces? Do all the substitution and reduction steps in your head, and only then try out the code.\ntrump_family = [donald, melania, ivanka, tiffany]\nprint(trump_family)\nprint(trump_family[1]['job'])\n\n\nExercise 12-10\nWrite and run this code\namino_acids = {}\namino_acids['ATG'] = 'met'\namino_acids['TCT'] = 'ser'\namino_acids['TAC'] = 'tyr'\n\ncodon = 'TCT'\nprint(\"{} encodes {}\".format(codon, amino_acids[codon]))\n\nYou have probably noticed that the interpretation of length is different for each type of value. In a string, it is the number of characters; in a list, it is the number of values in the list; in a dictionary, it is the number of key-value pairs. How do you think Python knows which length interpretation to use when the len function is called? This is where objects shine. len(x) returns the value that x.__len__() returns. So the len function is defined roughly like this:\ndef len(x):\n    return x.__len__()\nSimilarly, the in operator calls a secret __contains__ method.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Pairs of things</span>"
    ]
  },
  {
    "objectID": "chapters/python/gluing_values_in_sequence.html",
    "href": "chapters/python/gluing_values_in_sequence.html",
    "title": "13  Grouping values",
    "section": "",
    "text": "Tuples\nA tuple is a sequence of values, just like a list. However, unlike a list, the elements of a tuple can not be changed. You cannot append to a tuple, either. Once a tuple is made, it is immutable (or unchangeable). To make a tuple, you just use round brackets instead of square brackets:\nIt may seem strange that Python has both tuples and lists. One reason is that tuples are more efficient, whereas lists are more flexible. We will not use tuples often, but you must know what they are.\nYou can do most of the operations on a tuple that you can also do on a list. The following exercises should be easy if you remember how to do the same thing on lists:",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Grouping values</span>"
    ]
  },
  {
    "objectID": "chapters/python/gluing_values_in_sequence.html#tuples",
    "href": "chapters/python/gluing_values_in_sequence.html#tuples",
    "title": "13  Grouping values",
    "section": "",
    "text": "fruits = (\"apple\", \"banana\", \"cherry\")\n\n\n\nExercise 13-1\nFind the number of elements in the fruits tuple using the len function.\n\n\nExercise 13-2\nExtract the second element of the fruits tuple (\"banana\") using indexing.\n\n\nExercise 13-3\nTry to change the second element of the fruits tuple to \"apple\" and see what happens. It should be something like this:\nTraceback (most recent call last):\n  File \"script.py\", line 2, in &lt;module&gt;\n    fruits[3] = \"apple\"\nTypeError: 'tuple' object does not support item assignment\nYou cannot change elements of a tuple because they are immutable (once made, they stay that way).",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Grouping values</span>"
    ]
  },
  {
    "objectID": "chapters/python/gluing_values_in_sequence.html#tuple-assignment",
    "href": "chapters/python/gluing_values_in_sequence.html#tuple-assignment",
    "title": "13  Grouping values",
    "section": "Tuple assignment",
    "text": "Tuple assignment\nPython lets you assign a tuple of values to a tuple of variables like this:\nfather, mother, son = (\"Donald\", \"Ivana\", \"Eric\")\nIt does the same as the following three assignments:\nfather = \"Donald\"\nmother = \"Ivana\"\nson = \"Eric\"\nWhen a tuple is made, the values are “packed” in sequence:\nfamily = (\"Donald\", \"Ivana\", \"Eric\")\nUsing the same analogy, values can be “unpacked” using tuple assignment:\nfather, mother, son = family\nThe only requirement is that the number of variables equals the number of values in the tuple.\nOnce in a while, it is useful to swap the values of two variables. With conventional assignment statements, we have to use a temporary variable. For example, to swap a and b:\ntmp = a\na = b\nb = tmp\n\nExercise 13-4\nTry this and read the error message:\nfamily = (\"Donald\", \"Ivana\", \"Eric\")\nfather, mother = family\n\n\nExercise 13-5\nTry this and read the error message:\nfamily = (\"Donald\", \"Ivana\", \"Eric\")\nfather, mother, son, daughter = family\nCompare to the error message in the previous exercise.\n\n\nExercise 13-6\nSay you want to swap the values of two variables, a and b. To do that, you would need to keep one of the values in an extra variable like this:\ntemp = a\na = b\nb = temp\nUsing what you have learned in this chapter, can you devise a simple and pretty way of swapping a and b in one statement? It may occur to you before you realize how it works, so make sure you can connect your solution to the rules of tuples and tuple assignment.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Grouping values</span>"
    ]
  },
  {
    "objectID": "chapters/python/iteration_over_values.html",
    "href": "chapters/python/iteration_over_values.html",
    "title": "14  Iterating values",
    "section": "",
    "text": "The for-loop\nThis chapter is about how you repeat the same code for many different values – and the many reasons why this is useful.\nPrograms often need to do repetitive things. Consider this example below, where x is assigned a value that is then printed:\nYou can see that we do the same thing four times, with the only difference being that the variable x takes a new value each time. Now, carefully write the alternative version below and compare what is printed to what was printed in the above example.\nIt should be the same. What you just wrote is called a for-loop. It is called a for-loop because it does something for each of many values – in this case, for each value in our list.\nThe statements nested under the for-loop are run as many times as there are values in our list, and every time they are run, x is assigned a new value. The first time the statements are run, x is assigned the first value in the list. The second time they run x, the second value is assigned to the list. This continues until x has been assigned all the values in the list.\nThe semantics of a for-loop is as follows:\nWhat is an iterable, you may ask? It is any Python value that knows how to serve one value at a time until there are none left. Only objects with an __iter__ method can do this. You will get an error if you try to iterate over a value that does not have an __iter__ method. Try the code below and see how Python complains that “‘int’ object is not iterable”:\nTry these variations of the for-loop above and notice how the rules 1-5 apply in each case:\nIn each case, the expression after in reduces to the value [1, 5, 3, 7], which then serves as the iterable.\nIt is not only lists that are iterable. Strings are, too. Their ‘iter’ method of a string tells it that it should serve one character at a time. Try this:\nNeat, right?\nIn programming, you often need to iterate over integer values and sometimes quite a few (like the 250 million bases of the human chromosome one). It would be quite annoying if you had to manually make long lists of integers, so Python provides a built-in function called range that helps you out. It returns a special iterator value that lets you iterate over a specified range of numbers. Try the two examples below and compare what is printed:\nYou can see that using range works just like using a list of numbers, but the cool thing about the range is that it does not return a list. It just serves one number at a time until it is done. This is also why you will not see a list if you try to print what range returns:\nThe range function needs three values to know which values to iterate over: “start”, “end” and “step”. It will assume sensible defaults if you do not give it all three arguments. Try this:\nYou can see that the first and last arguments default to 0 and 1. If you give it two arguments, it will assume that they are “start” and “end”. If you only give it one argument, it will assume that it is the “end”.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Iterating values</span>"
    ]
  },
  {
    "objectID": "chapters/python/iteration_over_values.html#the-for-loop",
    "href": "chapters/python/iteration_over_values.html#the-for-loop",
    "title": "14  Iterating values",
    "section": "",
    "text": "x = 1\nprint(x)\nx = 5\nprint(x)\nx = 3\nprint(x)\nx = 7\nprint(x)\n\nfor x in [1, 5, 3, 7]:\n    print(x)    \n\n\n\n\nFirst, you write for.\nThen, you write the name of the variable that will be assigned a new value for each iteration of the loop (x in the above case).\nThen you write in.\nThe,n you write the name of an iterable or an expression that reduces to one. In the above case, it was the list [1, 5, 3, 7].\nThe statements nested under the for loop are indented with four spaces, just like with if-statements. These statements are executed once for every value in the iterable.\n\n\nfor x in 4:\n    print(x)\n\nfor x in [1, 5, 3, 7]:\n    print(x)    \n\nlist_of_numbers = [1, 5, 3, 7]\nfor x in list_of_numbers:\n    print(x)    \n\nfor x in [1, 5] + [3, 7]:\n    print(x)    \n\n\nfor character in 'banana':\n    print(character)\n\n\ntotal = 0\nfor number in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n    total += number\nprint(total)\n\ntotal = 0\nfor number in range(10):\n    total += number\nprint(total)\n\nnumber_iterator = range(10)\nprint(number_iterator)\n\nfor i in range(0, 10, 1):\n    print(i)\n\nfor i in range(0, 10):\n    print(i)\n\nfor i in range(10):\n    print(i)\n\n\nExercise 14-1\nWhat do you think the third argument to range specifies? Try these variations and see if you can figure it out:\nfor i in range(0, 10, 1):\n    print(i)\n\nfor i in range(0, 10, 2):\n    print(i)\n\nfor i in range(0, 10, 3):\n    print(i)\nCheck the documentation once you have decided.\n\n\nExercise 14-2\nWhat will happen here:\nfor x in []:\n    print(x)  \nand here:\nfor x in range(0):\n    print(x)  \nand here:\nfor x in range(10, 10):\n    print(x)  \n\n\nExercise 14-3\nThe two examples below print the same. Make sure you understand why. Write and experiment with the code on your own.\nlist_of_words = ['one', 'two', 'three']\n\n# example 1\nfor word in list_of_words:\n    print(word)\n\n# example 2\nlist_length = len(list_of_words)\nfor index in range(list_length):\n    print(list_of_words[index])\n\n\nExercise 14-4\nFinish the code below so all the even numbers go into one list, and all the odd numbers go into the other (hint: remember the modulo operator?)\nnumbers = [4, 9, 6, 7, 4, 5, 3, 2, 6]\neven = []\nodd = []\nfor n in numbers:\n    # your code here ...\n    \n    \n\n\nExercise 14-5\nYou can put any statements under the for loop. Here, it includes an if-statement that lets you generate a list of all the a characters in banana (in case you need that).\nresult = []\nfruit = 'banana'\nfor character in fruit:\n    if character == 'a':\n        result.append(character)\nprint(result)\nNow change the code so you instead get the indexes of the ‘a’ characters: [1, 3, 5]. Here are some hints:\n\nYou need a for-loop over a list of numbers.\nrange(len(fruit)) may be relevant numbers :-).\nfruit[1] substitutes for 'a'.\n\n\n\nExercise 14-6\nImagine you want to throw a big party and have rented a place with space for 100 people. Now, you want to start inviting people. What kind of error do you get here, and why?\nfriends = [\"Mogens\", \"Preben\", \"Berit\"]\ninvited = []\nfor index in range(100):\n    invited.append(friends[index])\n\n\nExercise 14-7\nYou can also put a for loop under another for loop, and the rules for each for loop are the same as those explained above. The statements nested under the for loop are indented with four spaces, just like with if statements. These statements are executed once for every value in the iterable. Think carefully about what you think is printed in the example below before you try it out.\nfor i in range(3):\n    for j in range(3):\n        print(i, j)\nMake sure you understand i, j pairs are printed in the order they are.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Iterating values</span>"
    ]
  },
  {
    "objectID": "chapters/python/working_with_data_files.html",
    "href": "chapters/python/working_with_data_files.html",
    "title": "15  Working with files",
    "section": "",
    "text": "Writing files\nThis chapter covers the bare necessities of making your program read data from a file on your computer and how to create a file to write results.\nTo interact with a file on your hard disk, you need to know its name and whether you want to write to it or read from it. Then, you can use the built-in function open to create a file object that lets you read or write to that file. The open function takes two arguments: The first is a string, which gives the file’s name. The second argument is also a string and should be 'w' for “write” if you want to write to the file or 'r' for “read” if you are going to read from the file. To keep things simple, we will assume that the file you want to open is always in the same folder (directory) as the Python script that calls the open function.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Working with files</span>"
    ]
  },
  {
    "objectID": "chapters/python/working_with_data_files.html#writing-files",
    "href": "chapters/python/working_with_data_files.html#writing-files",
    "title": "15  Working with files",
    "section": "",
    "text": "Exercise 15-1\nTry to write the code below and run it:\nf = open('workfile.txt', 'w')\nf.write(\"First line\\n\")\nf.write(\"Second line\\n\")\nf.close()\nNow open the workfile.txt in VScode and see what is in it now. It should contain:\nFirst line\nSecond line\nLet’s break down what happened:\n\nYou used the open built-in function to open a file called “workfile.txt” in writing mode using the 'w' as the second argument.\nYou then wrote the string \"First line\\n\" to the file using the write method of the file object.\nYou wrote another string \"Second line\\n\" to the file using the write method of the file object.\nYou closed the file using the close method of the file object.\n\nNote that if you open a file for writing, a file with that name is created. If a file of that name already exists, it is overwritten.\n\n\nExercise 15-2\nClose workfile.txt in VScode again and change your program above to this (removing the \\n characters):\nf = open('workfile.txt', 'w')\nf.write(\"First line\")\nf.write(\"Second line\")\nf.close()\nWhat do you think the content of workfile.txt is now? Decide before you open workfile.txt in VScode again and have a look. What do you think the \\n character represents?\n\n\nExercise 15-3\nClose workfile.txt in VScode and change your program above to this:\nf = open('workfile.txt', 'w')\nf.write(\"First line\\nSecond line\\n\")\nf.close()\nCan you see how that is equivalent to what you did before? Open workfile.txt in VScode again and have a look.\n\n\nExercise 15-4\nYou can also make print write to a file instead of the terminal. That way, your output will end up in the file instead of the terminal. To make print write to a file, you need to use print’s file keyword argument to tell print which file to write to. The argument must be an object that represents the file you want to write to (file=f below). Try to write the code below and run it:\nf = open('workfile.txt', 'w')\nprint(\"First line\", file=f)\nprint(\"Second line, file=f\")\nf.close()\nCompare the code to that in Section 15.0.0.1. Notice how the strings we print end with a newline character \\n. This is because the default behavior for print is to add a new line to the end of what it prints—just like when you print to the terminal.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Working with files</span>"
    ]
  },
  {
    "objectID": "chapters/python/working_with_data_files.html#reading-files",
    "href": "chapters/python/working_with_data_files.html#reading-files",
    "title": "15  Working with files",
    "section": "Reading files",
    "text": "Reading files\nWhen you want to read a from an existing file, you give the open function the name of that file and specify 'r' for reading as a second argument. If the file you name does not exist, Python will tell you it does not exist (it is nice like that). Before you head into the rest of this section, make sure you redo Section 15.0.0.1 so the content of workfile.txt is:\nFirst line\nSecond line\n\nExercise 15-5\nf = open('workfile.txt', 'r')\nfile_content = f.read()\nprint(file_content)\nf.close()\nLet’s break down what happened:\n\nYou used the open built-in function to open a file called workfile.txt in reading mode using the 'r' as the second argument.\nYou then read the file’s content using the read method, which returns the contents as a string.\nYou printed the string.\nYou closed the file using the close method of the file object.\n\n\n\nExercise 15-6\nTry to read from the file after you close it:\nf = open('workfile', 'r')\nf.close()\nfile_content = f.read()\nDo you get an error? Which one? Do you understand why?\n\n\nExercise 15-7\nYou can use the readline method to read one line at a time. What do you think happens if you run this code:\nf = open('workfile.txt', 'r')\nline = f.readline()\nprint(line)\nline = f.readline()\nprint(line)\nline = f.readline()\nprint(line)\nOnce you decide, try it out. What is printed in the last print statement? The thing is, the file object keeps track of how much of the file it has read. Once it ends, you can read as much as you like- nothing is left. If you want to start reading from the top of the file, you can close it and open it again. Try to insert the following two statements at various places in the code above and see what happens.\nf.close()\nf = open('workfile.txt', 'r')\n\n\nExercise 15-8\nLook at the code below and figure out for yourself what it does:\ninput_file = open('workfile.txt', 'r')\noutput_file = open('results.txt', 'w')\n\nfor line in input_file:\n    line = line.upper()\n    output_file.write(line)\nThen, run it and open results.txt in VScode and see what it produced.\nWere you surprised that the file object can be an iterator in a for-loop? Just like stings can iterate over characters, lists can iterate over values, dictionaries can iterate over keys, and file objects can iterate over the lines in the file.\nTry to modify your code to use the print function instead of the write method (see Section 15.0.0.4).",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Working with files</span>"
    ]
  },
  {
    "objectID": "chapters/python/working_with_data_files.html#general-exercises",
    "href": "chapters/python/working_with_data_files.html#general-exercises",
    "title": "15  Working with files",
    "section": "General exercises",
    "text": "General exercises\n\nExercise 15-9\nWrite a function read_file that takes the name of a file as an argument. The function should read the content and return it. Like:\ncontent_of_file = read_file('some_file.txt')\n\n\nExercise 15-10\nWrite a function that takes the name of two files as arguments. The file should read the content of the first file and write it to the second file.\ncopy_file('some_file.txt', 'other_file.txt')",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Working with files</span>"
    ]
  },
  {
    "objectID": "chapters/python/data_structures.html",
    "href": "chapters/python/data_structures.html",
    "title": "16  Structuring data",
    "section": "",
    "text": "Exercise 16-1\nThis section will further train your ability to create such data structures using iteration. Data structures in Python are fundamental tools that allow you to organize, store, and manipulate data effectively. They provide a way to represent and manage data in a structured and organized manner, making it easier to perform various operations on the data. Python offers a variety of built-in data structures, such as lists and dictionaries, with different properties that are useful for different needs. Sometimes, a single list or dictionary is all you need. Still, sometimes you need to combine many lists and dictionaries to make an elaborate data structure. As your programming skills progress, you will find that mastering the use of different data structures is crucial for becoming a proficient Python programmer.\nImagine you want to count how many times each nucleotide appears in a DNA string like this: 'ATGCCGATTAA'. One way to proceed with an account of this is by using a dictionary where the keys represent the different values we want to count (in this case 'A', 'T', 'C' and 'G'). The value associated with each key is the number of times we have seen that key (nucleotide). So, we want to end up with a dictionary like this one (not necessarily with key-value pairs in this order):\nRemind yourself how you assign a value to an existing key in a dictionary. Here is some code to get you going:",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Structuring data</span>"
    ]
  },
  {
    "objectID": "chapters/python/data_structures.html#general-exercises",
    "href": "chapters/python/data_structures.html#general-exercises",
    "title": "16  Structuring data",
    "section": "General exercises",
    "text": "General exercises\n\nBy now, you have learned a lot, and the general exercises, which serve to keep it all current, get more complicated. But remember: even though the code may mix lists, for-loops, and functions, the rules for lists, for-loops, and functions are not mixed. The separate simple rules for a list, a for-loop, and a function are still the same. If you get confused, it is time to revisit the sections about each separate topic. You may have to do that many times during the course.\n\n\nExercise 16-16\nWrite a function, square_numbers, that takes a list of numbers as argument and returns a new list with the numbers squared.\n# write your function definition here ...\n\nnumbers = [1, 5, 3, 7]\n\n# then you can call it like this:\nsquared = squared_numbers(numbers)\n\n\nExercise 16-17\nWrite a function count_characters, which takes a string argument and returns a dictionary with the counts of each character in the string. When you call the function like this:\ncount_characters('banana')\nit must return (not necessarily with key-value pairs in that order):\n{'n': 2, 'b': 1, 'a': 3}\nThe technique you should use is the one you learned in Section 16.0.0.2. Here, we iterate over a string of characters instead of a list of numbers. Here is a bit of code to help you along.\ndef count_characters(text):\n    counts = {}\n    # fill in the missing code ...\n\n    return counts\n\n\nExercise 16-18\nUse the function you made in the previous exercise to construct the following data structure below from this list: ['banana', 'ananas', 'apple'].\n{ 'banana': {'b': 1, 'a': 3, 'n': 2},\n  'apple': {'a': 1, 'e': 1, 'p': 2, 'l': 1}, \n  'ananas': {'a': 3, 's': 1, 'n': 2} }\nHere is some code to help you along:\nmy_database = {}\nfor word in ['banana', 'ananas', 'apple']:\n    my_database[word] =  # you figure this out...\nOnce you are done, what value do you think my_database['banana'] represents? I.e., what will it be reduced to if used in an expression? And what value does my_database['banana']['a'] represent?\n\n\nExercise 16-19\nRead the code below and make sure you understand each step before you write any of it. If necessary, revisit previous sections and look in the Python documentation. Then, write and run the code—and enjoy that it was exactly what you expected.\ndef get_words(text, search_string):\n    hits = []\n    for word in text.split():\n         if search_string in word:\n            hits.append(word)\n    return hits\n    \ns = 'eenie meenie minie moe'\nnie_words = get_words(s, 'nie')\nm_words = get_words(s, 'm')\n\nprint(' '.join(nie_words))\nprint(' '.join(m_words))\n\n\nExercise 16-20\nThis larger will take you through some of the most common string manipulations. A palindrome is a string that is spelled the same way, backward and forwards.\nWrite a function, is_palindrome, which takes one argument:\n\nA string.\n\nThe function must return:\n\nTrue if the string argument is a palindrome and False otherwise.\n\nExample usage:\nis_palindrome('abcba')\nshould return True and\nis_palindrome('foo')\nshould return False\nOne approach to this is to run through s from the first to the middle character, and for each character, check if the character is equal to the character at the same index from the right rather than the left. Remember that the first character of a string is at index 0 and the last at index -1, the second character is at index 1 and the second last at index -2 and so forth.\nSince you need to run through the string from the first to the middle character, you must first figure out how many characters that corresponds to. Say your palindrome is \"ACTGTCA\", then the number of indexes you need to loop over with a for loop is:\ns = \"ACTGTCA\"\nnr_indexes = len(s)//2 \nFigure out how to make range() return indexes to access the characters in the first half of the sequence. Then make a for loop where you iterate over the indexes you get from range(). Try to make the for-loop print out the first half of the characters, just to make sure you are using the right indexes.\nOnce you get this far, you must compare each character from the first half of the corresponding ones, starting from the other end of the palindrome. Figure out how to change each index used for the first half to the corresponding index for the other half so you can compare the relevant pairs. (You need to compare index 0 with -1, 1 with -2 and so on…)\nNow, try to make the for loop print both the character from the first half and the corresponding character from the other end. If you get the indexes right, you will see that the A prints with the A from the other end, the C with the C, and so on.\nWrite an if-statement in the for-loop that tests whether the two corresponding characters are identical. If the string is a palindrome, then each pair is identical. So, as soon as you see a pair that is not identical, you know it is not a palindrome, and you can let your function return False like this:\nif left_character != right_character:\n    return False\nRemember that the function ends when it encounters a return statement.\nIf all pairs pass the test, the string is a palindrome, and the function should return True when exiting the for loop.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Structuring data</span>"
    ]
  },
  {
    "objectID": "chapters/python/unleash_your_functions.html",
    "href": "chapters/python/unleash_your_functions.html",
    "title": "17  Recursion",
    "section": "",
    "text": "Recursion\nAt the time of “printing,” this chapter was unfinished. So I have added some empty pages so you can merge the chapter into the pdf later without messing up the chapter, exercise and page numbers\nPLACEHOLDER PAGE",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Recursion</span>"
    ]
  },
  {
    "objectID": "chapters/python/unleash_your_functions.html#divide-and-conquer",
    "href": "chapters/python/unleash_your_functions.html#divide-and-conquer",
    "title": "17  Recursion",
    "section": "Divide and conquer",
    "text": "Divide and conquer\nPLACEHOLDER PAGE\n\nExercise 17-1\nPLACEHOLDER PAGE",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Recursion</span>"
    ]
  },
  {
    "objectID": "chapters/python/testing_your_code.html",
    "href": "chapters/python/testing_your_code.html",
    "title": "18  Testing your code",
    "section": "",
    "text": "Why test your code?\nThis chapter is about how you figure out if the code you wrote solves the problem in the way you intended. You will be surprised how often that is not the case – even for seasoned programmers.\nThere are tons of reasons why you should test your code. Here are what I think are the two most important ones:\nTesting of code is a big thing in programming. Professionals consistently test their code. You will do it in time, too, but in this course, you will only do the basic testing yourself. Instead, you will have access to readymade testing suites made especially for each of your programming projects.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testing your code</span>"
    ]
  },
  {
    "objectID": "chapters/python/testing_your_code.html#why-test-your-code",
    "href": "chapters/python/testing_your_code.html#why-test-your-code",
    "title": "18  Testing your code",
    "section": "",
    "text": "Makes you think: Testing forces you to slow down and think about exactly what the code is supposed to do. By deciding what tests to do before you start coding, you try to anticipate errors and cases that need to be covered by how you want to solve the problem. The notion of falsification is important in science and in coding, too. You should try to prove that your idea is wrong and consider it valid only if this process fails. So, testing motivates you to think about ways to break their code, thereby helping you solve your programming problem in a general and robust way so that it does exactly what you expect it to do.\nGives you peace of mind: Testing increases your confidence that a function you have written works as it is supposed to so that you can now stop thinking about how it is implemented and focus on using it as a component in solving a larger, more complicated problem. Having set up a series of tests also allows you to change and improve your function’s implementation without worrying that it stops working the way it is supposed to. As long as it passes all the tests, it should be ok.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testing your code</span>"
    ]
  },
  {
    "objectID": "chapters/python/testing_your_code.html#basic-testing",
    "href": "chapters/python/testing_your_code.html#basic-testing",
    "title": "18  Testing your code",
    "section": "Basic testing",
    "text": "Basic testing\nSay that you are asked to make a function that takes a string argument and returns True if that string is a palindrome and False otherwise (hypothetical example). Then you start thinking about which strings should make the function return True and which return False. Once you have defined your is_palindrome function, you can set up some fairly obvious tests like this:\nprint(is_palindome('123321') == True)\nprint(is_palindome('ATGGTA') == True)\nprint(is_palindome('ATGATG') == False)\nprint(is_palindome('XY') == False)\nBut if you keep thinking, maybe you will come up with more tests to cover all the different types of cases you may encounter:\nprint(is_palindome('12321') == True) # uneven length\nprint(is_palindome('121') == True) # uneven length\nprint(is_palindome('AA') == True)\nprint(is_palindome('A') == True) # single char",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testing your code</span>"
    ]
  },
  {
    "objectID": "chapters/python/testing_your_code.html#the-project-testing-utility",
    "href": "chapters/python/testing_your_code.html#the-project-testing-utility",
    "title": "18  Testing your code",
    "section": "The project testing utility",
    "text": "The project testing utility\nTo keep you focused on the programming part, each of the programming projects you will do in this course comes with a ready-made suite of tests of the functions you are asked to implement. So, for each function, you can run tests to ensure it implements the behavior it is supposed to.\nEach project comes with two files that you can download from the course page. They have their names for a good reason, so do not change them. In the first project about translating DNA, they are called translationproject.py and test_translationproject.py.\nTo be able to test your functions, you must write your code in the file called translationproject.py. To run your code, you type this in the Terminal as usual:\n\n\nTerminal\n\npython translationproject.py\n\nTo test the functions as you complete each one, you can run the test script test_translationproject.py like this:\n\n\nTerminal\n\npython test_translationproject.py\n\nThe code in test_translationproject.py reads your code in translationproject.py and performs a series of tests of each function. When you run the test script, four things may happen depending on the state of your code:\nCase 1: If you have not yet implemented all the functions, the test script will remind you (once for each test) that you did not implement the functions with the names required.\n\n\nTerminal\n\n*********************************************************\nATTENTION! The following functions are not defined:\n\n    translate_codon\n    split_codons\n    translate_orf\n\nThese functions are either not correctly named (spelled)\nor not defined at all. They will be marked as FAILED.\nCheck your spelling if this is not what you intend.\n*********************************************************\n\nRan 16 tests in 0.000s\n\nOK (skipped=14)\n\nIf you have implemented a function but misspelled its name, you will also get this type of reminder. The reminders are meant to ensure that you do not hand in the assignment with missing or misspelled function definitions.\nCase 2: If a test for one of the functions you have written fails, the testing is aborted, and the script prints some information to help you understand what the problem could be. Say you wrote the function translate_codon wrongly so that it always returns M for some reason:\ndef translate_codon(codon):\n    return 'M' # crazy\nthen you would get this message:\n\n\nTerminal\n\nFAILED TEST CASE: test_translate_codon_2\n\nMESSAGE:\n    The call:\n\n        translate_codon('TAA')\n\n    returned:\n\n        'M'\n\n    However, it should return:\n\n        '*'\n\n======================================================================\nRan 4 tests in 0.001s\n\nFAILED (failures=1)\n\nIt is now left to you to figure out why your function returns the wrong value when called with these arguments.\nCase 3: If you defined all functions correctly and they all work the way they are supposed to, then the test script just prints:\nRan 14 tests in 0.140s\n\nOK",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testing your code</span>"
    ]
  },
  {
    "objectID": "chapters/python/your_own_types_of_objects.html",
    "href": "chapters/python/your_own_types_of_objects.html",
    "title": "19  Your Own Objects",
    "section": "",
    "text": "Introduction: Objects and Types\nDo you know what type you are? Python objects do, and this chapter is about how they know and how you can create your own types.\nBy now, you know that Python values are objects and that there are different types of objects. Strings have the type str, integers have the type int, and lists have the type list, to mention a few of the ones we know. Remember that you can use the built-in type function to find out what type an object is. Try this:\nAs you already know, objects are “data with associated functionality.” Some of this functionality is available as methods: for example, the upper and split methods of string objects or the append and extend methods of lists.\nObjects also behave sensibly in different contexts: When you sum two integers (4 + 6), you get their sum 10, but if you sum two strings ('ban' + 'ana'), you get a concatenation of the two strings 'banana'. The two different types of objects each know how to act with the + operator. Another example is iteration: If you iterate over a string, you get one character at a time in order, but if you iterate over a dictionary, you get the keys in the dictionary. This super convenient context-aware behavior is, in fact, also defined as methods, but more about that below.\nEach object carries its own data: different numbers, different strings, etc., but their functionality is shared by all objects of the same type. So all string objects refer to the same definition of the upper and split methods in the string type as graphically illustrated in figure Figure 19.1.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Your Own Objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/your_own_types_of_objects.html#introduction-objects-and-types",
    "href": "chapters/python/your_own_types_of_objects.html#introduction-objects-and-types",
    "title": "19  Your Own Objects",
    "section": "",
    "text": "print(type('banana'))  # &lt;class 'str'&gt;\nprint(type(4))         # &lt;class 'int'&gt;\nprint(type([]))        # &lt;class 'list'&gt;\n\n\n\n\n\n\n\n\n\nFigure 19.1: Objects and type. Each object carries its own data, but refers to the type for a definition of its methods.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Your Own Objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/your_own_types_of_objects.html#why-create-your-own-classes",
    "href": "chapters/python/your_own_types_of_objects.html#why-create-your-own-classes",
    "title": "19  Your Own Objects",
    "section": "Why Create Your Own Classes?",
    "text": "Why Create Your Own Classes?\nIn bioinformatics and computational biology, we often work with complex data structures that represent biological entities. While Python’s built-in types are powerful, they may not perfectly capture the essence of domain-specific concepts. For instance, a DNA sequence is more than just a string—it has specific properties and behaviors unique to biological sequences. A phylogenetic tree is more than nested lists—it represents evolutionary relationships with specific traversal and analysis methods.\nCreating custom classes allows us to: 1. Encapsulate related data and functionality in a single, coherent unit 2. Model real-world entities more naturally in our code 3. Reuse code through well-defined interfaces 4. Maintain code more easily through clear organization 5. Extend functionality through inheritance and composition",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Your Own Objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/your_own_types_of_objects.html#basic-class-definition",
    "href": "chapters/python/your_own_types_of_objects.html#basic-class-definition",
    "title": "19  Your Own Objects",
    "section": "Basic Class Definition",
    "text": "Basic Class Definition\nImagine creating your own types of objects carrying a particular kind of data and functionality. “Data” can be anything. In bioinformatics, it could be an open reading frame, a patient profile, or a phylogenetic tree. Fortunately, Python lets us do this using something called classes. Classes define new kinds of objects and the functionality they provide. So, by defining a new class, you define a new type of object, and the methods you define in that class are available to objects of that particular type.\nLet’s define a new class, Point, that represents a point in a two-dimensional coordinate system. As shown in Figure 19.2, each such point has an x-value and a y-value, so we need our new point object to hold two numbers representing those values.\n\n\n\n\n\n\nFigure 19.2: Points\n\n\n\nHere is how we define the Point class:\nclass Point:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\nLet us break it down: We use the keyword class followed by the name of the class we define. By convention, class names in Python use CamelCase (also called PascalCase), where each word starts with a capital letter. Nested under that statement, we define methods that will be available to all instances of this class.\n\nThe Constructor: __init__\nThe __init__ method is special—it’s called a constructor. This method is automatically called when we create a new instance of the class. The parameters it accepts (besides self) determine what arguments we need to provide when creating an object:\n# Creating instances of the Point class\np1 = Point(3, 4)\np2 = Point(-1, 2)\np3 = Point(0, 0)\n\n# Accessing attributes\nprint(f\"Point 1: ({p1.x}, {p1.y})\")  # Point 1: (3, 4)\nprint(f\"Point 2: ({p2.x}, {p2.y})\")  # Point 2: (-1, 2)\nThe mysterious self parameter deserves special attention. It represents the instance being created or operated on. When Python calls __init__, it automatically passes the newly created object as the first argument. Inside __init__, we use self to store data in the object by creating attributes: self.x = x creates an attribute named x and stores the value of the parameter x in it.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Your Own Objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/your_own_types_of_objects.html#instance-methods",
    "href": "chapters/python/your_own_types_of_objects.html#instance-methods",
    "title": "19  Your Own Objects",
    "section": "Instance Methods",
    "text": "Instance Methods\nMethods are functions defined inside a class. They automatically receive the instance as their first parameter (conventionally named self). Let’s add some methods to our Point class:\nclass Point:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def distance_from_origin(self):\n        \"\"\"Calculate the distance from this point to the origin.\"\"\"\n        return (self.x ** 2 + self.y ** 2) ** 0.5\n\n    def distance_to(self, other):\n        \"\"\"Calculate the distance from this point to another point.\"\"\"\n        dx = self.x - other.x\n        dy = self.y - other.y\n        return (dx ** 2 + dy ** 2) ** 0.5\n\n    def move(self, dx, dy):\n        \"\"\"Move the point by dx and dy.\"\"\"\n        self.x += dx\n        self.y += dy\n\n    def __str__(self):\n        \"\"\"Return a string representation of the point.\"\"\"\n        return f\"Point({self.x}, {self.y})\"\nNow we can use these methods:\np1 = Point(3, 4)\np2 = Point(0, 0)\n\nprint(p1.distance_from_origin())  # 5.0\nprint(p1.distance_to(p2))         # 5.0\n\np1.move(1, 1)\nprint(p1)  # Point(4, 5)",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Your Own Objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/your_own_types_of_objects.html#special-methods-the-magic-behind-pythons-elegance",
    "href": "chapters/python/your_own_types_of_objects.html#special-methods-the-magic-behind-pythons-elegance",
    "title": "19  Your Own Objects",
    "section": "Special Methods: The Magic Behind Python’s Elegance",
    "text": "Special Methods: The Magic Behind Python’s Elegance\nPython classes can define special methods (also called magic methods or dunder methods) that enable objects to work with built-in Python operations. These methods have names surrounded by double underscores.\n\nString Representation Methods\nThe __str__ method we saw above defines how an object is converted to a string for display. There’s also __repr__, which provides a more detailed representation:\nclass Point:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def __str__(self):\n        \"\"\"Human-readable string representation.\"\"\"\n        return f\"({self.x}, {self.y})\"\n\n    def __repr__(self):\n        \"\"\"Developer-friendly representation.\"\"\"\n        return f\"Point(x={self.x}, y={self.y})\"\n\np = Point(3, 4)\nprint(str(p))   # (3, 4)\nprint(repr(p))  # Point(x=3, y=4)\n\n\nArithmetic Operations\nWe can define how objects behave with mathematical operators:\nclass Point:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def __add__(self, other):\n        \"\"\"Define addition for points.\"\"\"\n        return Point(self.x + other.x, self.y + other.y)\n\n    def __sub__(self, other):\n        \"\"\"Define subtraction for points.\"\"\"\n        return Point(self.x - other.x, self.y - other.y)\n\n    def __mul__(self, scalar):\n        \"\"\"Define scalar multiplication.\"\"\"\n        return Point(self.x * scalar, self.y * scalar)\n\n    def __eq__(self, other):\n        \"\"\"Define equality comparison.\"\"\"\n        return self.x == other.x and self.y == other.y\n\n# Using the operators\np1 = Point(3, 4)\np2 = Point(1, 2)\n\np3 = p1 + p2  # Point(4, 6)\np4 = p1 - p2  # Point(2, 2)\np5 = p1 * 2   # Point(6, 8)\n\nprint(p1 == Point(3, 4))  # True\n\n\nContainer-like Behavior\nClasses can behave like containers (lists, dictionaries) by implementing appropriate methods:\nclass DNASequence:\n    def __init__(self, sequence):\n        self.sequence = sequence.upper()\n\n    def __len__(self):\n        \"\"\"Return the length of the sequence.\"\"\"\n        return len(self.sequence)\n\n    def __getitem__(self, index):\n        \"\"\"Allow indexing and slicing.\"\"\"\n        return self.sequence[index]\n\n    def __contains__(self, subsequence):\n        \"\"\"Allow 'in' operator.\"\"\"\n        return subsequence in self.sequence\n\n    def __iter__(self):\n        \"\"\"Make the object iterable.\"\"\"\n        return iter(self.sequence)\n\n# Using the DNA sequence\ndna = DNASequence(\"ATGCGATCG\")\n\nprint(len(dna))           # 9\nprint(dna[0])             # A\nprint(dna[2:5])           # GCG\nprint(\"GAT\" in dna)       # True\n\nfor base in dna:\n    print(base, end=\" \")  # A T G C G A T C G",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Your Own Objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/your_own_types_of_objects.html#class-variables-vs-instance-variables",
    "href": "chapters/python/your_own_types_of_objects.html#class-variables-vs-instance-variables",
    "title": "19  Your Own Objects",
    "section": "Class Variables vs Instance Variables",
    "text": "Class Variables vs Instance Variables\nSo far, we’ve seen instance variables (attributes specific to each object). Classes can also have class variables shared by all instances:\nclass Atom:\n    # Class variable - shared by all instances\n    element_symbols = {\n        1: \"H\", 2: \"He\", 3: \"Li\", 4: \"Be\", 5: \"B\",\n        6: \"C\", 7: \"N\", 8: \"O\", 9: \"F\", 10: \"Ne\"\n    }\n\n    def __init__(self, atomic_number, x, y, z):\n        # Instance variables - unique to each instance\n        self.atomic_number = atomic_number\n        self.x = x\n        self.y = y\n        self.z = z\n\n    def get_element(self):\n        \"\"\"Return the element symbol for this atom.\"\"\"\n        return self.element_symbols.get(self.atomic_number, \"Unknown\")\n\n    @classmethod\n    def add_element(cls, atomic_number, symbol):\n        \"\"\"Add a new element to the periodic table.\"\"\"\n        cls.element_symbols[atomic_number] = symbol\n\n# All atoms share the same element_symbols dictionary\natom1 = Atom(6, 0, 0, 0)\natom2 = Atom(8, 1, 0, 0)\n\nprint(atom1.get_element())  # C\nprint(atom2.get_element())  # O\n\n# Adding a new element affects all instances\nAtom.add_element(11, \"Na\")\natom3 = Atom(11, 2, 0, 0)\nprint(atom3.get_element())  # Na",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Your Own Objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/your_own_types_of_objects.html#inheritance-building-on-existing-classes",
    "href": "chapters/python/your_own_types_of_objects.html#inheritance-building-on-existing-classes",
    "title": "19  Your Own Objects",
    "section": "Inheritance: Building on Existing Classes",
    "text": "Inheritance: Building on Existing Classes\nOne of the most powerful features of object-oriented programming is inheritance, which allows us to create new classes based on existing ones. The new class (child or subclass) inherits attributes and methods from the parent class (superclass) and can add or modify functionality.\nclass Sequence:\n    \"\"\"Base class for biological sequences.\"\"\"\n\n    def __init__(self, sequence, name=\"\"):\n        self.sequence = sequence.upper()\n        self.name = name\n\n    def __len__(self):\n        return len(self.sequence)\n\n    def __str__(self):\n        return f\"{self.name}: {self.sequence}\" if self.name else self.sequence\n\n    def count(self, subsequence):\n        \"\"\"Count occurrences of a subsequence.\"\"\"\n        return self.sequence.count(subsequence)\n\nclass DNASequence(Sequence):\n    \"\"\"DNA sequence with additional DNA-specific methods.\"\"\"\n\n    def __init__(self, sequence, name=\"\"):\n        # Validate DNA sequence\n        valid_bases = set(\"ATGCN\")\n        if not all(base in valid_bases for base in sequence.upper()):\n            raise ValueError(\"Invalid DNA sequence\")\n        super().__init__(sequence, name)\n\n    def transcribe(self):\n        \"\"\"Convert DNA to RNA.\"\"\"\n        return RNASequence(self.sequence.replace('T', 'U'),\n                          f\"{self.name}_RNA\" if self.name else \"\")\n\n    def gc_content(self):\n        \"\"\"Calculate GC content percentage.\"\"\"\n        gc_count = self.sequence.count('G') + self.sequence.count('C')\n        return (gc_count / len(self.sequence)) * 100 if len(self.sequence) &gt; 0 else 0\n\n    def reverse_complement(self):\n        \"\"\"Return the reverse complement of the DNA sequence.\"\"\"\n        complement = {'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G', 'N': 'N'}\n        rev_comp = ''.join(complement[base] for base in self.sequence[::-1])\n        return DNASequence(rev_comp, f\"{self.name}_RC\" if self.name else \"\")\n\nclass RNASequence(Sequence):\n    \"\"\"RNA sequence with RNA-specific methods.\"\"\"\n\n    def __init__(self, sequence, name=\"\"):\n        # Validate RNA sequence\n        valid_bases = set(\"AUGCN\")\n        if not all(base in valid_bases for base in sequence.upper()):\n            raise ValueError(\"Invalid RNA sequence\")\n        super().__init__(sequence, name)\n\n    def translate(self):\n        \"\"\"Translate RNA to protein.\"\"\"\n        codon_table = {\n            'UUU': 'F', 'UUC': 'F', 'UUA': 'L', 'UUG': 'L',\n            'UCU': 'S', 'UCC': 'S', 'UCA': 'S', 'UCG': 'S',\n            'UAU': 'Y', 'UAC': 'Y', 'UAA': '*', 'UAG': '*',\n            'UGU': 'C', 'UGC': 'C', 'UGA': '*', 'UGG': 'W',\n            # ... (abbreviated for space)\n        }\n\n        protein = []\n        for i in range(0, len(self.sequence) - 2, 3):\n            codon = self.sequence[i:i+3]\n            amino_acid = codon_table.get(codon, 'X')\n            if amino_acid == '*':\n                break\n            protein.append(amino_acid)\n\n        return ProteinSequence(''.join(protein),\n                               f\"{self.name}_protein\" if self.name else \"\")\n\nclass ProteinSequence(Sequence):\n    \"\"\"Protein sequence with protein-specific methods.\"\"\"\n\n    def molecular_weight(self):\n        \"\"\"Calculate approximate molecular weight.\"\"\"\n        weights = {\n            'A': 89, 'R': 174, 'N': 132, 'D': 133, 'C': 121,\n            'E': 147, 'Q': 146, 'G': 75, 'H': 155, 'I': 131,\n            'L': 131, 'K': 146, 'M': 149, 'F': 165, 'P': 115,\n            'S': 105, 'T': 119, 'W': 204, 'Y': 181, 'V': 117\n        }\n        return sum(weights.get(aa, 0) for aa in self.sequence)\n\n# Using inheritance\ndna = DNASequence(\"ATGGCCATTGTAATGGGCCGCTGAAAGGGTGCCCGATAG\", \"gene1\")\nprint(f\"DNA length: {len(dna)}\")\nprint(f\"GC content: {dna.gc_content():.1f}%\")\n\nrna = dna.transcribe()\nprint(f\"RNA: {rna.sequence[:20]}...\")\n\nprotein = rna.translate()\nprint(f\"Protein: {protein.sequence}\")",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Your Own Objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/your_own_types_of_objects.html#composition-objects-containing-objects",
    "href": "chapters/python/your_own_types_of_objects.html#composition-objects-containing-objects",
    "title": "19  Your Own Objects",
    "section": "Composition: Objects Containing Objects",
    "text": "Composition: Objects Containing Objects\nSometimes, rather than inheritance, it’s better to use composition—having objects contain other objects:\nclass Gene:\n    \"\"\"A gene with regulatory and coding regions.\"\"\"\n\n    def __init__(self, name, chromosome, start, end):\n        self.name = name\n        self.chromosome = chromosome\n        self.start = start\n        self.end = end\n        self.exons = []\n        self.promoter = None\n\n    def add_exon(self, start, end, sequence):\n        \"\"\"Add an exon to the gene.\"\"\"\n        exon = Exon(start, end, sequence)\n        self.exons.append(exon)\n        self.exons.sort(key=lambda e: e.start)\n\n    def set_promoter(self, sequence, position):\n        \"\"\"Set the gene's promoter.\"\"\"\n        self.promoter = Promoter(sequence, position)\n\n    def get_coding_sequence(self):\n        \"\"\"Concatenate all exon sequences.\"\"\"\n        return ''.join(exon.sequence for exon in self.exons)\n\n    def get_length(self):\n        \"\"\"Calculate total gene length.\"\"\"\n        return self.end - self.start + 1\n\n    def __str__(self):\n        return f\"Gene: {self.name} ({self.chromosome}:{self.start}-{self.end})\"\n\nclass Exon:\n    \"\"\"An exon within a gene.\"\"\"\n\n    def __init__(self, start, end, sequence):\n        self.start = start\n        self.end = end\n        self.sequence = sequence\n\n    def __len__(self):\n        return self.end - self.start + 1\n\nclass Promoter:\n    \"\"\"A promoter region.\"\"\"\n\n    def __init__(self, sequence, position):\n        self.sequence = sequence\n        self.position = position\n\n    def find_tata_box(self):\n        \"\"\"Find TATA box in promoter.\"\"\"\n        tata_variants = ['TATAAA', 'TATAWAW', 'TATAWAR']\n        for variant in tata_variants:\n            if variant in self.sequence:\n                return self.sequence.index(variant)\n        return -1\n\n# Using composition\nbrca1 = Gene(\"BRCA1\", \"chr17\", 43044295, 43125483)\nbrca1.set_promoter(\"AGCTATAAAAGCGCGC\", 43044200)\nbrca1.add_exon(43044295, 43044400, \"ATGGATTTATCTGCTCTTCGC\")\nbrca1.add_exon(43045800, 43045950, \"GTGAAGCAGCATCTGGGTGT\")\n\nprint(brca1)\nprint(f\"Gene length: {brca1.get_length()} bp\")\nprint(f\"Number of exons: {len(brca1.exons)}\")",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Your Own Objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/your_own_types_of_objects.html#properties-controlled-access-to-attributes",
    "href": "chapters/python/your_own_types_of_objects.html#properties-controlled-access-to-attributes",
    "title": "19  Your Own Objects",
    "section": "Properties: Controlled Access to Attributes",
    "text": "Properties: Controlled Access to Attributes\nPython provides properties as a way to control access to attributes, allowing validation and computed attributes:\nclass Organism:\n    \"\"\"Represents a biological organism.\"\"\"\n\n    def __init__(self, scientific_name, common_name=\"\"):\n        self.scientific_name = scientific_name\n        self.common_name = common_name\n        self._genome_size = 0  # Private attribute (convention)\n\n    @property\n    def genome_size(self):\n        \"\"\"Get genome size in base pairs.\"\"\"\n        return self._genome_size\n\n    @genome_size.setter\n    def genome_size(self, value):\n        \"\"\"Set genome size with validation.\"\"\"\n        if value &lt; 0:\n            raise ValueError(\"Genome size cannot be negative\")\n        self._genome_size = value\n\n    @property\n    def genome_size_mb(self):\n        \"\"\"Get genome size in megabases (computed property).\"\"\"\n        return self._genome_size / 1_000_000\n\n    @property\n    def genus(self):\n        \"\"\"Extract genus from scientific name.\"\"\"\n        return self.scientific_name.split()[0] if self.scientific_name else \"\"\n\n    @property\n    def species(self):\n        \"\"\"Extract species from scientific name.\"\"\"\n        parts = self.scientific_name.split()\n        return parts[1] if len(parts) &gt; 1 else \"\"\n\n# Using properties\nhuman = Organism(\"Homo sapiens\", \"Human\")\nhuman.genome_size = 3_200_000_000\n\nprint(f\"Organism: {human.scientific_name}\")\nprint(f\"Genus: {human.genus}\")\nprint(f\"Species: {human.species}\")\nprint(f\"Genome size: {human.genome_size_mb:.1f} Mb\")\n\n# This would raise an error:\n# human.genome_size = -100  # ValueError: Genome size cannot be negative",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Your Own Objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/your_own_types_of_objects.html#static-methods-and-class-methods",
    "href": "chapters/python/your_own_types_of_objects.html#static-methods-and-class-methods",
    "title": "19  Your Own Objects",
    "section": "Static Methods and Class Methods",
    "text": "Static Methods and Class Methods\nBesides instance methods, classes can have static methods (not bound to instances) and class methods (bound to the class):\nclass SequenceAnalyzer:\n    \"\"\"Utility class for sequence analysis.\"\"\"\n\n    nucleotide_weights = {\n        'A': 331.2, 'T': 322.2, 'U': 308.2,\n        'G': 347.2, 'C': 307.2\n    }\n\n    @staticmethod\n    def is_palindrome(sequence):\n        \"\"\"Check if a sequence is a palindrome (static method).\"\"\"\n        sequence = sequence.upper()\n        return sequence == sequence[::-1]\n\n    @staticmethod\n    def hamming_distance(seq1, seq2):\n        \"\"\"Calculate Hamming distance between two sequences.\"\"\"\n        if len(seq1) != len(seq2):\n            raise ValueError(\"Sequences must have equal length\")\n        return sum(c1 != c2 for c1, c2 in zip(seq1, seq2))\n\n    @classmethod\n    def molecular_weight_dna(cls, sequence):\n        \"\"\"Calculate molecular weight of DNA (class method).\"\"\"\n        weight = sum(cls.nucleotide_weights.get(base, 0)\n                    for base in sequence.upper())\n        # Subtract water molecules for phosphodiester bonds\n        return weight - 18.0 * (len(sequence) - 1)\n\n    @classmethod\n    def gc_skew(cls, sequence, window_size=100):\n        \"\"\"Calculate GC skew in sliding windows.\"\"\"\n        sequence = sequence.upper()\n        skews = []\n\n        for i in range(0, len(sequence) - window_size + 1):\n            window = sequence[i:i + window_size]\n            g_count = window.count('G')\n            c_count = window.count('C')\n\n            if g_count + c_count &gt; 0:\n                skew = (g_count - c_count) / (g_count + c_count)\n            else:\n                skew = 0\n\n            skews.append(skew)\n\n        return skews\n\n# Using static and class methods\nprint(SequenceAnalyzer.is_palindrome(\"ATCGCGAT\"))  # False\nprint(SequenceAnalyzer.is_palindrome(\"ATCGCTA\"))   # False\nprint(SequenceAnalyzer.hamming_distance(\"ATCG\", \"ATTG\"))  # 1\n\ndna_weight = SequenceAnalyzer.molecular_weight_dna(\"ATCG\")\nprint(f\"Molecular weight: {dna_weight:.1f} Da\")",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Your Own Objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/your_own_types_of_objects.html#advanced-topics-multiple-inheritance-and-mixins",
    "href": "chapters/python/your_own_types_of_objects.html#advanced-topics-multiple-inheritance-and-mixins",
    "title": "19  Your Own Objects",
    "section": "Advanced Topics: Multiple Inheritance and Mixins",
    "text": "Advanced Topics: Multiple Inheritance and Mixins\nPython supports multiple inheritance, where a class can inherit from multiple parent classes. This can be used to create mixins—classes that provide specific functionality to be mixed into other classes:\nclass Serializable:\n    \"\"\"Mixin for JSON serialization.\"\"\"\n\n    def to_json(self):\n        \"\"\"Convert object to JSON.\"\"\"\n        import json\n        return json.dumps(self.__dict__)\n\n    @classmethod\n    def from_json(cls, json_str):\n        \"\"\"Create object from JSON.\"\"\"\n        import json\n        data = json.loads(json_str)\n        return cls(**data)\n\nclass Comparable:\n    \"\"\"Mixin for comparison operations.\"\"\"\n\n    def __lt__(self, other):\n        return self.compare_key() &lt; other.compare_key()\n\n    def __le__(self, other):\n        return self.compare_key() &lt;= other.compare_key()\n\n    def __gt__(self, other):\n        return self.compare_key() &gt; other.compare_key()\n\n    def __ge__(self, other):\n        return self.compare_key() &gt;= other.compare_key()\n\n    def compare_key(self):\n        \"\"\"Override this method to define comparison behavior.\"\"\"\n        raise NotImplementedError\n\nclass Mutation(Serializable, Comparable):\n    \"\"\"A genetic mutation with serialization and comparison.\"\"\"\n\n    def __init__(self, chromosome, position, ref, alt, gene=\"\"):\n        self.chromosome = chromosome\n        self.position = position\n        self.ref = ref\n        self.alt = alt\n        self.gene = gene\n\n    def compare_key(self):\n        \"\"\"Define comparison based on genomic position.\"\"\"\n        # Convert chromosome to number for comparison\n        chr_num = self.chromosome.replace(\"chr\", \"\")\n        chr_num = 23 if chr_num == \"X\" else 24 if chr_num == \"Y\" else int(chr_num)\n        return (chr_num, self.position)\n\n    def __str__(self):\n        return f\"{self.chromosome}:{self.position} {self.ref}&gt;{self.alt}\"\n\n# Using multiple inheritance\nmut1 = Mutation(\"chr7\", 140453136, \"A\", \"T\", \"BRAF\")\nmut2 = Mutation(\"chr3\", 178936091, \"G\", \"A\", \"PIK3CA\")\n\n# Comparison (from Comparable)\nprint(mut1 &lt; mut2)  # False (chr7 comes after chr3)\n\n# Serialization (from Serializable)\njson_str = mut1.to_json()\nprint(json_str)\n\n# Reconstruction from JSON\nmut3 = Mutation.from_json(json_str)\nprint(mut3)",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Your Own Objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/your_own_types_of_objects.html#best-practices-in-class-design",
    "href": "chapters/python/your_own_types_of_objects.html#best-practices-in-class-design",
    "title": "19  Your Own Objects",
    "section": "Best Practices in Class Design",
    "text": "Best Practices in Class Design\nWhen designing classes, follow these principles for maintainable and robust code:\n\n1. Single Responsibility Principle\nEach class should have one primary purpose. Don’t create “god classes” that do everything:\n# Good: Separate concerns\nclass SequenceReader:\n    \"\"\"Handles reading sequences from files.\"\"\"\n    def read_fasta(self, filename):\n        pass\n\nclass SequenceAligner:\n    \"\"\"Handles sequence alignment.\"\"\"\n    def align(self, seq1, seq2):\n        pass\n\n# Bad: Too many responsibilities\nclass SequenceDoEverything:\n    def read_fasta(self, filename):\n        pass\n    def align(self, seq1, seq2):\n        pass\n    def translate(self):\n        pass\n    def find_orfs(self):\n        pass\n\n\n2. Encapsulation\nHide internal implementation details and provide clean interfaces:\nclass Phylogeny:\n    \"\"\"A phylogenetic tree.\"\"\"\n\n    def __init__(self):\n        self._nodes = []  # Private (by convention)\n        self._root = None\n\n    def add_node(self, node):\n        \"\"\"Public interface for adding nodes.\"\"\"\n        self._validate_node(node)\n        self._nodes.append(node)\n\n    def _validate_node(self, node):\n        \"\"\"Private helper method.\"\"\"\n        if not isinstance(node, TreeNode):\n            raise TypeError(\"Must be a TreeNode instance\")\n\n\n3. Documentation\nAlways document your classes and methods:\nclass AlignmentScore:\n    \"\"\"\n    Represents an alignment score with associated metadata.\n\n    This class encapsulates alignment scoring information including\n    the raw score, normalized score, and statistical significance.\n\n    Attributes:\n        raw_score (float): The raw alignment score\n        length (int): Length of the alignment\n        method (str): Scoring method used\n\n    Example:\n        &gt;&gt;&gt; score = AlignmentScore(150.5, 250, \"BLOSUM62\")\n        &gt;&gt;&gt; print(score.normalized_score())\n        0.602\n    \"\"\"\n\n    def __init__(self, raw_score, length, method=\"BLOSUM62\"):\n        \"\"\"\n        Initialize an AlignmentScore.\n\n        Args:\n            raw_score: The raw alignment score\n            length: Length of the alignment\n            method: Scoring matrix used (default: BLOSUM62)\n        \"\"\"\n        self.raw_score = raw_score\n        self.length = length\n        self.method = method\n\n    def normalized_score(self):\n        \"\"\"\n        Calculate normalized score (score per position).\n\n        Returns:\n            float: The normalized score\n        \"\"\"\n        return self.raw_score / self.length if self.length &gt; 0 else 0",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Your Own Objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/your_own_types_of_objects.html#real-world-example-a-complete-bioinformatics-class",
    "href": "chapters/python/your_own_types_of_objects.html#real-world-example-a-complete-bioinformatics-class",
    "title": "19  Your Own Objects",
    "section": "Real-World Example: A Complete Bioinformatics Class",
    "text": "Real-World Example: A Complete Bioinformatics Class\nLet’s create a comprehensive class for handling biological sequences with practical features:\nclass BioSequence:\n    \"\"\"\n    A comprehensive biological sequence class.\n\n    Handles DNA, RNA, and protein sequences with validation,\n    analysis methods, and file I/O capabilities.\n    \"\"\"\n\n    SEQUENCE_TYPES = {'DNA', 'RNA', 'PROTEIN'}\n\n    DNA_BASES = set('ATGCN')\n    RNA_BASES = set('AUGCN')\n    AMINO_ACIDS = set('ACDEFGHIKLMNPQRSTVWY*X')\n\n    def __init__(self, sequence, seq_type, seq_id=\"\", description=\"\"):\n        \"\"\"Initialize a biological sequence.\"\"\"\n        self.seq_type = seq_type.upper()\n        if self.seq_type not in self.SEQUENCE_TYPES:\n            raise ValueError(f\"Invalid sequence type: {seq_type}\")\n\n        self.sequence = sequence.upper()\n        self._validate_sequence()\n\n        self.seq_id = seq_id\n        self.description = description\n        self.annotations = {}\n\n    def _validate_sequence(self):\n        \"\"\"Validate sequence based on type.\"\"\"\n        if self.seq_type == 'DNA':\n            valid = self.DNA_BASES\n        elif self.seq_type == 'RNA':\n            valid = self.RNA_BASES\n        else:  # PROTEIN\n            valid = self.AMINO_ACIDS\n\n        invalid_chars = set(self.sequence) - valid\n        if invalid_chars:\n            raise ValueError(f\"Invalid characters for {self.seq_type}: {invalid_chars}\")\n\n    def __len__(self):\n        \"\"\"Return sequence length.\"\"\"\n        return len(self.sequence)\n\n    def __str__(self):\n        \"\"\"String representation.\"\"\"\n        if self.seq_id:\n            return f\"&gt;{self.seq_id} {self.description}\\n{self.sequence[:60]}...\"\n        return self.sequence[:60] + \"...\" if len(self.sequence) &gt; 60 else self.sequence\n\n    def __repr__(self):\n        \"\"\"Developer representation.\"\"\"\n        return f\"BioSequence(type={self.seq_type}, length={len(self)}, id={self.seq_id})\"\n\n    def __getitem__(self, index):\n        \"\"\"Enable slicing and indexing.\"\"\"\n        if isinstance(index, slice):\n            return BioSequence(\n                self.sequence[index],\n                self.seq_type,\n                f\"{self.seq_id}_slice\",\n                f\"Slice of {self.description}\"\n            )\n        return self.sequence[index]\n\n    def __eq__(self, other):\n        \"\"\"Check sequence equality.\"\"\"\n        if not isinstance(other, BioSequence):\n            return False\n        return self.sequence == other.sequence and self.seq_type == other.seq_type\n\n    def composition(self):\n        \"\"\"Calculate composition of sequence elements.\"\"\"\n        from collections import Counter\n        return dict(Counter(self.sequence))\n\n    def gc_content(self):\n        \"\"\"Calculate GC content for nucleotide sequences.\"\"\"\n        if self.seq_type not in ['DNA', 'RNA']:\n            raise ValueError(\"GC content only applicable to nucleotide sequences\")\n\n        gc_count = self.sequence.count('G') + self.sequence.count('C')\n        return (gc_count / len(self)) * 100 if len(self) &gt; 0 else 0\n\n    def find_motif(self, motif):\n        \"\"\"Find all occurrences of a motif.\"\"\"\n        positions = []\n        motif = motif.upper()\n        start = 0\n\n        while True:\n            pos = self.sequence.find(motif, start)\n            if pos == -1:\n                break\n            positions.append(pos)\n            start = pos + 1\n\n        return positions\n\n    def to_fasta(self):\n        \"\"\"Convert to FASTA format.\"\"\"\n        header = f\"&gt;{self.seq_id}\"\n        if self.description:\n            header += f\" {self.description}\"\n\n        # Format sequence in 60-character lines\n        lines = [header]\n        for i in range(0, len(self.sequence), 60):\n            lines.append(self.sequence[i:i+60])\n\n        return '\\n'.join(lines)\n\n    @classmethod\n    def from_fasta(cls, fasta_string):\n        \"\"\"Create BioSequence from FASTA string.\"\"\"\n        lines = fasta_string.strip().split('\\n')\n        if not lines or not lines[0].startswith('&gt;'):\n            raise ValueError(\"Invalid FASTA format\")\n\n        header = lines[0][1:]  # Remove '&gt;'\n        parts = header.split(None, 1)\n        seq_id = parts[0] if parts else \"\"\n        description = parts[1] if len(parts) &gt; 1 else \"\"\n\n        sequence = ''.join(lines[1:])\n\n        # Guess sequence type\n        seq_upper = sequence.upper()\n        if all(c in 'ATGCN' for c in seq_upper):\n            seq_type = 'DNA'\n        elif all(c in 'AUGCN' for c in seq_upper):\n            seq_type = 'RNA'\n        else:\n            seq_type = 'PROTEIN'\n\n        return cls(sequence, seq_type, seq_id, description)\n\n    def annotate(self, feature, start, end, notes=\"\"):\n        \"\"\"Add annotation to sequence.\"\"\"\n        if feature not in self.annotations:\n            self.annotations[feature] = []\n\n        self.annotations[feature].append({\n            'start': start,\n            'end': end,\n            'notes': notes\n        })\n\n    def get_features(self, feature_type=None):\n        \"\"\"Get annotated features.\"\"\"\n        if feature_type:\n            return self.annotations.get(feature_type, [])\n        return self.annotations\n\n# Using the comprehensive BioSequence class\nseq = BioSequence(\n    \"ATGGCCATTGTAATGGGCCGCTGAAAGGGTGCCCGATAG\",\n    \"DNA\",\n    \"BRCA1_exon1\",\n    \"Breast cancer gene 1, exon 1\"\n)\n\nprint(seq)\nprint(f\"Length: {len(seq)}\")\nprint(f\"GC content: {seq.gc_content():.1f}%\")\nprint(f\"Composition: {seq.composition()}\")\n\n# Add annotations\nseq.annotate(\"start_codon\", 0, 3, \"ATG start codon\")\nseq.annotate(\"stop_codon\", 37, 40, \"TAG stop codon\")\n\n# Find motifs\ncg_positions = seq.find_motif(\"CG\")\nprint(f\"CG dinucleotide positions: {cg_positions}\")\n\n# Convert to FASTA\nprint(\"\\nFASTA format:\")\nprint(seq.to_fasta())\n\n# Slicing\nsubseq = seq[10:20]\nprint(f\"\\nSubsequence: {subseq.sequence}\")",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Your Own Objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/your_own_types_of_objects.html#conclusion",
    "href": "chapters/python/your_own_types_of_objects.html#conclusion",
    "title": "19  Your Own Objects",
    "section": "Conclusion",
    "text": "Conclusion\nPython classes provide a powerful mechanism for creating custom data types that model real-world entities and concepts. In bioinformatics, classes allow us to create abstractions that closely match biological entities—sequences, genes, proteins, mutations, and phylogenetic trees—making our code more intuitive and maintainable.\nKey takeaways from this comprehensive exploration of Python classes:\n\nClasses define new types of objects with their own data and methods\nThe __init__ constructor initializes new instances with specific data\nInstance methods operate on individual objects using self\nSpecial methods enable natural Python syntax (operators, iteration, etc.)\nInheritance allows building specialized classes from general ones\nComposition enables complex objects built from simpler ones\nProperties provide controlled access to attributes\nClass and static methods offer functionality not tied to instances\nGood design principles lead to maintainable, reusable code\n\nBy mastering classes, you gain the ability to write more organized, reusable, and expressive code. Whether you’re analyzing genomic sequences, modeling protein structures, or building phylogenetic trees, classes provide the foundation for robust bioinformatics software.\nThe journey from simple classes to complex inheritance hierarchies mirrors the journey from basic programming to software engineering. As you develop more sophisticated bioinformatics applications, you’ll find that well-designed classes make your code not just functional, but elegant and maintainable. The investment in learning object-oriented programming pays dividends in code quality, reusability, and your ability to tackle complex biological problems with computational solutions.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Your Own Objects</span>"
    ]
  },
  {
    "objectID": "chapters/python/using_code_from_other_files.html",
    "href": "chapters/python/using_code_from_other_files.html",
    "title": "20  Code in other files",
    "section": "",
    "text": "A module is a file with code\nThis chapter is about …",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Code in other files</span>"
    ]
  },
  {
    "objectID": "chapters/python/on_the_shoulders_of_giants.html",
    "href": "chapters/python/on_the_shoulders_of_giants.html",
    "title": "21  Giant’s shoulders",
    "section": "",
    "text": "Using code in other files\nOn the shoulders of giants\nPython modules",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Giant's shoulders</span>"
    ]
  },
  {
    "objectID": "chapters/python/on_the_shoulders_of_giants.html#standard-library-modules",
    "href": "chapters/python/on_the_shoulders_of_giants.html#standard-library-modules",
    "title": "21  Giant’s shoulders",
    "section": "Standard library modules",
    "text": "Standard library modules",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Giant's shoulders</span>"
    ]
  },
  {
    "objectID": "chapters/python/on_the_shoulders_of_giants.html#biopython",
    "href": "chapters/python/on_the_shoulders_of_giants.html#biopython",
    "title": "21  Giant’s shoulders",
    "section": "BioPython",
    "text": "BioPython\n\nA birds eye view of Biopython to give you an impression of what you can do if you know just a little bit about modules classes",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Giant's shoulders</span>"
    ]
  },
  {
    "objectID": "chapters/python/on_the_shoulders_of_giants.html#a-seq-class",
    "href": "chapters/python/on_the_shoulders_of_giants.html#a-seq-class",
    "title": "21  Giant’s shoulders",
    "section": "A Seq class",
    "text": "A Seq class",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Giant's shoulders</span>"
    ]
  },
  {
    "objectID": "chapters/python/on_the_shoulders_of_giants.html#reading-and-writing-sequence-formats",
    "href": "chapters/python/on_the_shoulders_of_giants.html#reading-and-writing-sequence-formats",
    "title": "21  Giant’s shoulders",
    "section": "Reading and writing sequence formats",
    "text": "Reading and writing sequence formats",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Giant's shoulders</span>"
    ]
  },
  {
    "objectID": "chapters/python/appendix_bsf.html",
    "href": "chapters/python/appendix_bsf.html",
    "title": "22  Appendix: PyMol for BSF",
    "section": "",
    "text": "Install Pixi",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Appendix: PyMol for BSF</span>"
    ]
  },
  {
    "objectID": "chapters/python/appendix_bsf.html#install-pixi",
    "href": "chapters/python/appendix_bsf.html#install-pixi",
    "title": "22  Appendix: PyMol for BSF",
    "section": "",
    "text": "Mac Windows\n\n\n\nClick the clipboard icon at the right end of the box below to copy the command to your clipboard.\nOpen your Terminal application.\nPaste the command into the Terminal window and press Enter. You will be prompted several times for either your user password or permissions of the installed apps.\n\ncurl -fsSL https://pixi.sh/install.sh | sh\n\n\n\nClick the clipboard icon at the right end of the box below to copy the command to your clipboard.\nOpen your PowerShell application.\nPaste the command into the PowerShell window and press Enter. You will be prompted several times to allow the app to make changes to your computer.\n\npowershell -ExecutionPolicy ByPass -c \"irm -useb https://pixi.sh/install.ps1 | iex\"\nIf that fails, you can download this interactive installer and run it.",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Appendix: PyMol for BSF</span>"
    ]
  },
  {
    "objectID": "chapters/python/appendix_bsf.html#install-pymol",
    "href": "chapters/python/appendix_bsf.html#install-pymol",
    "title": "22  Appendix: PyMol for BSF",
    "section": "Install PyMol",
    "text": "Install PyMol\n\n Mac Windows\n\n\n\nCreate a folder on your computer dedicated to the BSF course. Let’s say you call it ‘BSF’ and put it in your ‘Documents’ folder.\nOpen your Terminal application, and navigate to that folder using cd (something like cd Documents and then cd BSF).\nOnce you are in the ‘BSF’ folder, paste the command into the Terminal window and press Enter. You will be prompted several times for either your user password or permissions of the installed apps.\n\npixi init --platform osx-64 -c conda-forge -c anaconda -c schrodinger && pixi config set --local run-post-link-scripts insecure && pixi add pyqt \"pymol-bundle=3.1\" \"vtk-m=1.8\"\n\n\n\nCreate a folder on your computer dedicated to the BSF course. Let’s say you call it ‘BSF’ and put it in your ‘Documents’ folder.\nOpen your PowerShell application, and navigate to that folder using cd (something like cd Documents and then cd BSF).\nOnce you are in the ‘BSF’ folder, paste the command into the PowerShell window and press Enter. You will be prompted several times to allow the app to make changes to your computer.\n\npixi init -c conda-forge -c anaconda -c schrodinger ; pixi config set --local run-post-link-scripts insecure ; pixi add pyqt \"pymol-bundle=3.1\" \"vtk-m=1.8\"",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Appendix: PyMol for BSF</span>"
    ]
  },
  {
    "objectID": "chapters/python/appendix_bsf.html#running-pymol",
    "href": "chapters/python/appendix_bsf.html#running-pymol",
    "title": "22  Appendix: PyMol for BSF",
    "section": "Running PyMol",
    "text": "Running PyMol\nIn your BSF folder, run:\npixi run pymol",
    "crumbs": [
      "Learning Python",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Appendix: PyMol for BSF</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/preface.html",
    "href": "chapters/bioinformatics/preface.html",
    "title": "23  Preface",
    "section": "",
    "text": "What is bioinformatics\nBioinformatics is an interdisciplinary field that combines biology, computer science, mathematics, and statistics to analyze and interpret biological data, particularly large-scale molecular and genetic information. It involves the application of computational methods and techniques to understand biological processes, enhance our knowledge of genetics, and provide insights into various biological phenomena. With advancements in biotechnology, particularly in genomics, transcriptomics, proteomics, and metabolomics, enormous amounts of biological data are generated. These datasets contain information about genes, proteins, molecules, and their interactions. Typical work for a bioinformatician involves data mining and analysis and applying statistical and machine learning techniques to identify patterns, correlations, and significant features in biological datasets.\nBioinformatics involves the development of databases and software tools to efficiently store, organize, and manage these vast datasets. These resources enable researchers to access and manipulate the data for analysis. One of the core aspects of bioinformatics is the analysis of DNA, RNA, and protein sequences. This includes tasks such as sequence alignment, where sequences are compared to identify similarities and differences. Sequence alignment is crucial for understanding evolutionary relationships, identifying functional elements, and detecting genetic variations.\nStructural bioinformatics predicts and analyzes the three-dimensional structures of proteins, RNA, and other molecules. Understanding the structure of biomolecules is essential for comprehending their functions, interactions, and mechanisms. To this end, bioinformaticians also build tools used to predict the functions of genes, proteins, and other biomolecules. This involves comparing sequences to known functional elements or domains and inferring their roles based on similarities.\nBioinformaticians also develop and use algorithms to cluster sequences and construct trees revealing the history and relationships among different species or genes. A related area is the comparison of genomes across different species, revealing insights into genomic evolution, gene conservation, and functional divergence.\nIn medical research, bioinformatics contributes to drug development by predicting potential drug targets, simulating molecular interactions, and identifying candidate compounds for further experimental testing. Bioinformaticians contribute analyses of individuals’ genetic and molecular data and thus support personalized medicine, where medical treatments and interventions can be tailored to a person’s unique genetic makeup.\nIn essence, bioinformatics provides the tools and methodologies to extract meaningful insights from biological data, advancing our understanding of life sciences, genetics, and various other areas of biology. It is a rapidly evolving field that continues to play a pivotal role in modern biological research and applications.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/preface.html#why-should-i-care",
    "href": "chapters/bioinformatics/preface.html#why-should-i-care",
    "title": "23  Preface",
    "section": "Why should I care?",
    "text": "Why should I care?\nYou may wonder how you find yourself with lecture notes on bioinformatics. The reason is that programming and digital literacy are increasingly important for molecular biologists because of the growing reliance on data-driven approaches, high-throughput technologies, and computational analyses in modern biological research. Programming skills and digital literacy empower molecular biologists to efficiently handle data and perform complex analyses. As biological research becomes more intertwined with computational approaches, these skills have become integral for conducting impactful and innovative research in molecular biology. Here are several reasons why these skills are essential for molecular biologists:\nData Handling and Analysis: Molecular biology research generates vast amounts of data from techniques like DNA sequencing, gene expression profiling, and protein structure determination. Programming skills enable scientists to process, analyze, and extract meaningful insights from these large datasets using computational tools and algorithms.\nEfficiency and Automation: Repetitive tasks like data preprocessing can be automated using programming scripts. This increases efficiency and reduces the chances of human error, allowing researchers to focus more on the scientific interpretation of results.\nCustomized Data Analysis: Pre-existing software tools may not always meet specific research needs. With programming skills, molecular biologists can develop custom scripts and algorithms tailored to their experiments, ensuring optimal analysis and interpretation of results.\nStatistical Analysis: Many biological experiments require statistical analysis to draw meaningful conclusions. Programming allows researchers to implement statistical methods, conduct hypothesis testing, and visualize results effectively.\nCollaboration and Communication: Digital literacy allows researchers to effectively collaborate by sharing data, code, and results online. This promotes transparency, reproducibility, and knowledge sharing within the scientific community.\nKeeping Up with Advancements: Many new technologies and techniques in molecular biology are heavily reliant on computational analyses. Researchers with programming skills can easily adapt to new methodologies and stay current with the rapidly evolving field.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/preface.html#reading-this-book",
    "href": "chapters/bioinformatics/preface.html#reading-this-book",
    "title": "23  Preface",
    "section": "Reading this book",
    "text": "Reading this book\nIn this course, we do not cover all aspects of bioinformatics but focus on aspects that relate to the analysis of genes and genomes. These topics will become increasingly dominant as personalized medicine becomes the norm and introduces many essential bioinformatics concepts, models, and algorithms. Many of you follow the course Biological Structure and Function, teaching structural biology in bioinformatics.\nIn each of the topics we cover, we will spend the lectures building an understanding of the algorithms and inner workings of the relevant bioinformatics tools. We will use the exercises to test out some of the tools in small self-contained bioinformatics projects.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/association.html",
    "href": "chapters/bioinformatics/association.html",
    "title": "24  Genetic Association",
    "section": "",
    "text": "Introduction: The Promise of Genetic Association\nThe completion of the Human Genome Project in 2003 marked a pivotal moment in biomedical research, providing the reference sequence that would enable systematic investigation of how genetic variation influences human health and disease. This monumental achievement, combined with dramatic reductions in sequencing costs and advances in high-throughput genotyping technologies, has transformed our ability to identify genetic factors underlying complex diseases. Genome-wide association studies (GWAS) have emerged as the primary tool for discovering these genetic influences, fundamentally changing our understanding of disease architecture and opening new avenues for precision medicine.\nThe central premise of association studies is elegantly simple: by comparing genetic variants between individuals with a disease (cases) and those without (controls), we can identify genomic regions that contribute to disease susceptibility. However, this conceptual simplicity belies the enormous statistical, computational, and biological complexities involved in conducting and interpreting these studies. Modern GWAS routinely test millions of genetic variants across thousands or even hundreds of thousands of individuals, requiring sophisticated statistical methods to distinguish true associations from the noise of multiple testing while accounting for population structure, environmental factors, and the complex genetic architecture of human traits.\nThe impact of GWAS on biomedical research has been profound. Since the first successful GWAS identified a variant in the complement factor H gene associated with age-related macular degeneration in 2005, thousands of studies have identified tens of thousands of genetic associations for hundreds of complex traits and diseases. These discoveries have revealed unexpected biological pathways, identified new drug targets, enabled better disease risk prediction, and provided insights into human evolution and population history. Yet, despite these successes, significant challenges remain in translating GWAS findings into clinical applications and understanding the biological mechanisms through which genetic variants influence disease.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genetic Association</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/association.html#statistical-foundations-of-association-testing",
    "href": "chapters/bioinformatics/association.html#statistical-foundations-of-association-testing",
    "title": "24  Genetic Association",
    "section": "Statistical Foundations of Association Testing",
    "text": "Statistical Foundations of Association Testing\n\nThe Basic Association Test\nAt its core, genetic association testing examines whether the frequency of a genetic variant differs between individuals with and without a particular trait or disease. For a single nucleotide polymorphism (SNP) with two alleles (A and a), we can construct a 2×2 contingency table comparing allele frequencies between cases and controls. The null hypothesis states that the variant is not associated with the disease, meaning allele frequencies should be similar in both groups.\nThe most fundamental test for association is the chi-square test of independence. For a SNP with minor allele frequency (MAF) p in controls and q in cases, with n₀ controls and n₁ cases, the test statistic is:\n\\[\\chi^2 = \\frac{2n_0n_1(p-q)^2}{(n_0+n_1)p(1-p)}\\]\nUnder the null hypothesis of no association, this statistic follows a chi-square distribution with one degree of freedom. The beauty of this test lies in its simplicity and its direct connection to the effect size: larger differences in allele frequency between cases and controls yield larger test statistics and more significant p-values.\nHowever, real genetic data presents numerous complications. Genotype data comes in three forms for a biallelic SNP (AA, Aa, aa), requiring decisions about the genetic model. The additive model, which assumes each copy of the risk allele contributes equally to disease risk, has become standard due to its robustness and statistical power across a range of true genetic architectures. Under this model, we can perform linear regression for quantitative traits or logistic regression for binary outcomes:\n\\[\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1 \\cdot G + \\beta_2 \\cdot X\\]\nwhere Y is the disease status, G is the genotype coded as 0, 1, or 2 copies of the minor allele, and X represents covariates.\n\n\nPower and Sample Size Considerations\nStatistical power in GWAS depends on multiple factors: sample size, effect size, allele frequency, disease prevalence, and the significance threshold. The relationship between these factors is captured by the non-centrality parameter (NCP) of the chi-square distribution:\n\\[\\text{NCP} = n \\cdot 2p(1-p) \\cdot \\left(\\frac{r^2}{1-K^2 \\cdot r^2}\\right)\\]\nwhere n is the total sample size, p is the MAF, r is the genotype relative risk, and K is the disease prevalence. This formula reveals several crucial insights. Power increases with sample size and is maximized for common variants (MAF ≈ 0.5). For rare variants, much larger sample sizes are needed to achieve comparable power. The effect size enters quadratically, meaning that detecting variants with small effects requires dramatically larger samples.\nThe power calculation must also account for the multiple testing burden. With one million independent tests and a genome-wide significance threshold of 5×10⁻⁸, detecting a variant with MAF = 0.2 and odds ratio = 1.2 requires approximately 10,000 cases and 10,000 controls for 80% power. This explains why successful GWAS have required ever-larger sample sizes as the field has progressed from detecting large-effect variants to uncovering the many small-effect variants that contribute to complex trait architecture.\n\n\nMultiple Testing Correction\nThe massive multiple testing problem in GWAS—testing millions of SNPs simultaneously—necessitates stringent correction procedures to control false positive rates. The Bonferroni correction, which divides the nominal significance level by the number of tests, provides a conservative bound but assumes all tests are independent. In reality, SNPs in linkage disequilibrium (LD) are correlated, making Bonferroni overly conservative.\nThe field has converged on a genome-wide significance threshold of p &lt; 5×10⁻⁸, which roughly corresponds to Bonferroni correction for one million independent tests—the approximate number of independent common variant haplotypes in the European population. This threshold maintains the family-wise error rate at 5% across all tests. However, this one-size-fits-all threshold has limitations: different populations have different LD structures and thus different numbers of independent tests, and the threshold doesn’t account for prior biological knowledge that might make some associations more plausible than others.\nAlternative approaches to multiple testing include the false discovery rate (FDR), which controls the expected proportion of false positives among all discoveries. For a set of p-values p₁, …, pₘ ordered from smallest to largest, the Benjamini-Hochberg procedure declares significant all tests with:\n\\[p_i \\leq \\frac{i}{m} \\cdot \\alpha\\]\nwhere α is the desired FDR level. While FDR provides greater power than family-wise error rate control, it has been less widely adopted in GWAS, partly due to the community’s emphasis on minimizing false positives.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genetic Association</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/association.html#population-structure-and-stratification",
    "href": "chapters/bioinformatics/association.html#population-structure-and-stratification",
    "title": "24  Genetic Association",
    "section": "Population Structure and Stratification",
    "text": "Population Structure and Stratification\n\nThe Problem of Population Stratification\nPopulation stratification represents one of the most serious confounding factors in genetic association studies. When cases and controls differ in their ancestral backgrounds, any genetic variant whose frequency varies across populations will show apparent association with the disease, even if it has no causal relationship. This phenomenon can generate thousands of false positive associations and has historically plagued genetic association studies.\nConsider a simple example: suppose we conduct a GWAS of chopstick usage in a sample combining individuals of European and East Asian ancestry. Any variant that differs in frequency between these populations—which includes millions of SNPs—will appear associated with chopstick use, not because of any biological relationship but simply because chopstick use correlates with ancestry. While this example is obvious, subtler forms of stratification can arise from cryptic relatedness, regional population differences, or socioeconomic factors that correlate with both ancestry and disease risk.\nThe genomic control method provides a simple diagnostic and correction for stratification. Under the null hypothesis of no association and no stratification, the median chi-square statistic should equal 0.456. The genomic inflation factor λ is defined as:\n\\[\\lambda = \\frac{\\text{median}(\\chi^2_{\\text{observed}})}{0.456}\\]\nValues of λ &gt; 1 indicate inflation due to stratification or other confounders. The method corrects all test statistics by dividing by λ, though this assumes uniform inflation across the genome, which may not hold for complex stratification patterns.\n\n\nPrincipal Component Analysis\nPrincipal component analysis (PCA) has become the standard method for detecting and correcting population stratification. PCA identifies the major axes of genetic variation in the sample by eigendecomposition of the genotype correlation matrix. The top principal components (PCs) typically capture population structure, with each PC representing a continuous axis of ancestry.\nThe mathematical foundation of PCA begins with the centered genotype matrix X, where each element xᵢⱼ represents the minor allele count for individual i at SNP j, centered by subtracting the mean. The genetic relationship matrix (GRM) is:\n\\[\\text{GRM} = \\frac{1}{m}XX^T\\]\nwhere m is the number of SNPs. The eigenvectors of the GRM are the principal components, and the eigenvalues indicate the amount of variance explained by each component. Typically, the top 10-20 PCs are included as covariates in association testing:\n\\[Y = \\beta_0 + \\beta_g \\cdot G + \\sum_{k=1}^{K} \\beta_k \\cdot PC_k + \\epsilon\\]\nThis adjustment removes confounding due to population structure while preserving true associations.\n\n\nMixed Models and Relatedness\nLinear mixed models (LMMs) provide a unified framework for handling both population structure and cryptic relatedness. The model includes both fixed effects (the SNP being tested and covariates) and random effects that model the polygenic background:\n\\[Y = X\\beta + g + \\epsilon\\]\nwhere g ~ N(0, σ²ₐK) with K being the kinship matrix estimated from genome-wide SNPs. The kinship matrix captures all pairwise relationships in the sample, from close relatives to distant population-level relationships.\nThe key insight is that the random effect g captures the polygenic contribution to the trait from all SNPs not being directly tested. This accounts for confounding due to relatedness at all levels, from siblings to subtle population structure. The computational challenge lies in fitting this model millions of times (once for each SNP), which has motivated efficient algorithms like BOLT-LMM and fastGWA that achieve nearly linear time complexity through various approximations.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genetic Association</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/association.html#linkage-disequilibrium-and-fine-mapping",
    "href": "chapters/bioinformatics/association.html#linkage-disequilibrium-and-fine-mapping",
    "title": "24  Genetic Association",
    "section": "Linkage Disequilibrium and Fine-Mapping",
    "text": "Linkage Disequilibrium and Fine-Mapping\n\nUnderstanding Linkage Disequilibrium\nLinkage disequilibrium (LD), the non-random association of alleles at different loci, is both a blessing and a curse for GWAS. It enables the detection of causal variants through their correlation with genotyped SNPs, allowing GWAS to survey the genome with a finite number of markers. However, LD also complicates the interpretation of associations, as significant SNPs may merely tag the true causal variants rather than being causal themselves.\nThe classical measure of LD is r², the squared correlation coefficient between two SNPs:\n\\[r^2 = \\frac{(p_{AB} - p_A p_B)^2}{p_A(1-p_A)p_B(1-p_B)}\\]\nwhere pₐ and p_B are the allele frequencies and p_AB is the haplotype frequency. The value r² represents the proportion of variance at one SNP explained by the other. For association mapping, if a causal variant with effect size β is in LD with a genotyped SNP with r², the expected effect size at the genotyped SNP is r²β, and the power to detect association is reduced by a factor of r².\nLD patterns vary dramatically across the genome and between populations. In European populations, LD typically extends over tens to hundreds of kilobases, structured in discrete blocks separated by recombination hotspots. African populations show shorter-range LD due to their longer evolutionary history and larger historical effective population size. These population differences have important implications: while European populations require fewer SNPs for genome-wide coverage, African populations provide better resolution for fine-mapping causal variants.\n\n\nStatistical Fine-Mapping\nFine-mapping aims to identify the causal variant(s) within a region of association. The challenge is that many variants in LD with the causal variant will show significant association, creating a “forest” of significant p-values around each true signal. Statistical fine-mapping uses the pattern of associations and LD structure to prioritize likely causal variants.\nBayesian fine-mapping methods calculate the posterior probability of causality for each variant. Assuming a single causal variant in the region, the posterior probability for variant j is:\n\\[\\text{PP}_j = \\frac{\\text{BF}_j \\cdot \\pi_j}{\\sum_k \\text{BF}_k \\cdot \\pi_k}\\]\nwhere BF_j is the Bayes factor comparing the model with variant j as causal versus the null model, and π_j is the prior probability. The Bayes factor can be approximated from summary statistics:\n\\[\\text{BF}_j \\approx \\sqrt{\\frac{V}{V + \\omega}} \\cdot \\exp\\left(\\frac{\\omega \\cdot z_j^2}{2(V + \\omega)}\\right)\\]\nwhere z_j is the z-score, V is the prior variance of effect sizes, and ω is typically set to 1/5000 for common variants.\n\n\nConditional Analysis and Multiple Signals\nMany loci harbor multiple independent association signals, requiring conditional analysis to dissect their contributions. The stepwise conditional approach iteratively identifies independent signals by conditioning on previously identified variants:\n\nIdentify the most significant SNP in the region\nCondition on this SNP and retest all others: Y = β₀ + β₁G₁ + β₂G₂ + ε\nIf any SNP remains significant, add it to the model\nRepeat until no significant associations remain\n\nThis approach can distinguish multiple causal variants from the shadow of LD but requires individual-level genotype data. Approximate conditional analysis using summary statistics has been developed:\n\\[\\beta_{\\text{cond}} = \\beta_{\\text{marginal}} - R \\cdot \\beta_{\\text{condition}}\\]\nwhere R is the LD matrix between the tested SNP and conditioning SNPs. This enables conditional analysis in meta-analyses where individual data cannot be shared.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genetic Association</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/association.html#meta-analysis-and-large-scale-consortia",
    "href": "chapters/bioinformatics/association.html#meta-analysis-and-large-scale-consortia",
    "title": "24  Genetic Association",
    "section": "Meta-Analysis and Large-Scale Consortia",
    "text": "Meta-Analysis and Large-Scale Consortia\n\nThe Power of Meta-Analysis\nMeta-analysis has become essential in GWAS, enabling the combination of results from multiple studies to achieve sample sizes impossible for any single study. By aggregating evidence across studies, meta-analysis increases power to detect associations, improves effect size estimates, and allows examination of heterogeneity across populations and study designs.\nThe fixed-effects meta-analysis model assumes all studies estimate the same underlying effect:\n\\[\\hat{\\beta}_{\\text{meta}} = \\frac{\\sum_i w_i \\hat{\\beta}_i}{\\sum_i w_i}\\]\nwhere w_i = 1/SE²_i is the inverse-variance weight for study i. The standard error of the combined estimate is:\n\\[\\text{SE}_{\\text{meta}} = \\frac{1}{\\sqrt{\\sum_i w_i}}\\]\nThis weighting scheme is optimal when all studies measure the same effect, giving more weight to precise estimates from larger studies.\n\n\nHeterogeneity Assessment\nHeterogeneity between studies can arise from differences in population ancestry, phenotype definition, environmental factors, or gene-environment interactions. Cochran’s Q statistic tests for heterogeneity:\n\\[Q = \\sum_i w_i(\\hat{\\beta}_i - \\hat{\\beta}_{\\text{meta}})^2\\]\nwhich follows a chi-square distribution with k-1 degrees of freedom under the null hypothesis of no heterogeneity. The I² statistic quantifies the proportion of variance due to heterogeneity:\n\\[I^2 = \\frac{Q - (k-1)}{Q} \\times 100\\%\\]\nValues of I² &gt; 50% suggest substantial heterogeneity that should be investigated.\nRandom-effects meta-analysis accommodates heterogeneity by assuming study-specific effects are drawn from a distribution:\n\\[\\hat{\\beta}_{\\text{RE}} = \\frac{\\sum_i w_i^* \\hat{\\beta}_i}{\\sum_i w_i^*}\\]\nwhere \\(w_i^* = 1/(\\text{SE}_i^2 + \\tau^2)\\) and τ² is the between-study variance estimated from the data. This approach yields wider confidence intervals when heterogeneity is present, appropriately reflecting uncertainty about the true effect.\n\n\nTrans-Ancestry Meta-Analysis\nCombining GWAS across diverse ancestries presents both opportunities and challenges. The opportunities include improved fine-mapping resolution due to different LD patterns, increased sample size and power, and the ability to assess the transferability of genetic effects across populations. However, differences in allele frequencies, LD structure, and potentially in genetic architecture complicate the analysis.\nTrans-ancestry meta-analysis methods must account for these differences. The MANTRA approach models relatedness between populations using a Bayesian partition model, allowing effects to be more similar between related populations. MR-MEGA explicitly models heterogeneity due to ancestry by including axes of genetic variation as covariates in the meta-regression:\n\\[\\beta_i = \\beta_0 + \\sum_j \\gamma_j \\cdot PC_{ij} + \\epsilon_i\\]\nwhere PC_ij represents the jth principal component of population structure for study i. This identifies variants with consistent effects across ancestries versus those with population-specific effects.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genetic Association</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/association.html#functional-interpretation-and-biological-insights",
    "href": "chapters/bioinformatics/association.html#functional-interpretation-and-biological-insights",
    "title": "24  Genetic Association",
    "section": "Functional Interpretation and Biological Insights",
    "text": "Functional Interpretation and Biological Insights\n\nFrom Association to Function\nThe vast majority of GWAS associations fall in non-coding regions of the genome, presenting a major challenge for functional interpretation. Unlike coding variants where the consequence on protein structure can be predicted, non-coding variants likely affect gene regulation through mechanisms that remain poorly understood. This “missing function” problem has motivated extensive efforts to annotate the non-coding genome and predict variant effects.\nExpression quantitative trait loci (eQTL) analysis links genetic variants to gene expression levels, providing a molecular phenotype intermediate between genotype and disease. The basic eQTL model tests association between each SNP and the expression of nearby genes:\n\\[E_g = \\alpha + \\beta \\cdot G_{\\text{SNP}} + \\epsilon\\]\nwhere E_g is the expression level of gene g. Variants associated with both disease and gene expression (colocalization) suggest a mechanism where the variant influences disease risk through effects on gene expression.\nThe challenge lies in determining whether the same variant drives both the GWAS and eQTL signals. Colocalization analysis calculates the posterior probability that the two traits share a causal variant:\n\\[\\text{PP}_{H4} = \\frac{P(D|H_4)}{P(D|H_0) + P(D|H_1) + P(D|H_2) + P(D|H_3) + P(D|H_4)}\\]\nwhere H₄ represents the hypothesis of a shared causal variant, and the other hypotheses represent no association (H₀), association with one trait only (H₁, H₂), or different causal variants (H₃).\n\n\nEnrichment Analysis and Pathway Discovery\nPathway and gene set enrichment analyses aggregate evidence across multiple variants to identify biological processes underlying disease. These methods test whether genes in specific pathways show more association signal than expected by chance. The gene set enrichment score can be calculated as:\n\\[\\text{ES} = \\max_{1 \\leq j \\leq p} \\left| \\sum_{i=1}^j \\frac{r_i \\cdot I(g_i \\in S)}{N_R} - \\frac{I(g_i \\notin S)}{N - N_S} \\right|\\]\nwhere genes are ranked by association strength r_i, S is the gene set, N_S is its size, and N_R is the sum of ranks in S.\nMAGMA (Multi-marker Analysis of GenoMic Annotation) provides a flexible framework for gene and gene-set analysis. It first aggregates SNP-level associations to gene-level statistics accounting for LD:\n\\[T_g = Z_g^T V_g^{-1} Z_g\\]\nwhere Z_g contains the z-scores for SNPs in gene g and V_g is their correlation matrix. Gene-level statistics are then tested for enrichment in gene sets while accounting for gene-gene correlations.\n\n\nPolygenic Risk Scores\nPolygenic risk scores (PRS) aggregate the effects of many genetic variants into a single measure of genetic liability. The simplest PRS sums risk alleles weighted by their effect sizes:\n\\[\\text{PRS}_i = \\sum_{j=1}^m \\hat{\\beta}_j \\cdot G_{ij}\\]\nwhere G_ij is the genotype of individual i at SNP j. The challenge lies in selecting which SNPs to include and estimating their weights.\nTraditional approaches use GWAS summary statistics, including either genome-wide significant SNPs (most specific but missing heritability) or all SNPs below a p-value threshold (more inclusive but including noise). LD clumping removes redundant SNPs in high LD to avoid double-counting signal. However, these simple approaches capture only a fraction of trait heritability.\nModern PRS methods use Bayesian approaches to model the entire genome simultaneously. LDpred models the effect sizes as drawn from a mixture distribution:\n\\[\\beta_j \\sim \\begin{cases} N(0, \\frac{h^2}{mp}) & \\text{with probability } p \\\\ 0 & \\text{with probability } 1-p \\end{cases}\\]\nwhere h² is the heritability, m is the number of SNPs, and p is the proportion of causal SNPs. The posterior effect sizes integrate over uncertainty in which SNPs are causal:\n\\[\\hat{\\beta}_j^{\\text{posterior}} = \\frac{p \\cdot \\hat{\\beta}_j^{\\text{inf}}}{1 + p(\\chi_j^2 - 1)}\\]\nwhere \\(\\hat{\\beta}_j^{\\text{inf}}\\) is the infinitesimal effect accounting for LD.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genetic Association</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/association.html#complex-trait-architecture",
    "href": "chapters/bioinformatics/association.html#complex-trait-architecture",
    "title": "24  Genetic Association",
    "section": "Complex Trait Architecture",
    "text": "Complex Trait Architecture\n\nThe Omnigenic Model\nRecent insights from GWAS have led to new models of complex trait architecture. The omnigenic model proposes that gene regulatory networks are so interconnected that all genes expressed in disease-relevant cells can affect the functions of core disease genes. This model distinguishes between core genes with direct effects on disease and peripheral genes with indirect effects mediated through the regulatory network.\nThe model explains several puzzling observations from GWAS: why trait heritability is spread across the entire genome rather than concentrated near relevant genes, why most traits are highly polygenic with thousands of associated variants, and why associated variants are enriched in regulatory elements active in disease-relevant cell types but not necessarily near obvious candidate genes.\nMathematically, if trait values depend on the expression of core genes C, and peripheral gene expression P affects core genes through the regulatory network with effect matrix A:\n\\[Y = \\beta_C^T C + \\epsilon = \\beta_C^T(AC \\cdot P + C_0) + \\epsilon\\]\nwhere C₀ represents core gene expression independent of peripheral genes. This shows how variants affecting peripheral genes anywhere in the genome can influence the trait through network propagation.\n\n\nMissing Heritability\nThe “missing heritability” problem refers to the gap between heritability estimated from family studies and the proportion of variance explained by GWAS-identified variants. For many traits, GWAS variants explain only 10-30% of the heritability estimated from twin studies. Several factors contribute to this gap:\nIncomplete coverage of rare variants is a major factor. The contribution of variants to heritability depends on their frequency and effect size:\n\\[h^2 = \\sum_j 2p_j(1-p_j)\\beta_j^2\\]\nRare variants (MAF &lt; 0.01) are poorly captured by current GWAS arrays and require very large sample sizes for detection. If genetic architecture follows the neutral model, the contribution to heritability at each frequency should be constant, suggesting substantial heritability from rare variants.\nSNP heritability estimation using all common SNPs simultaneously provides an upper bound on what GWAS can detect. The GREML approach estimates heritability from the genetic relationship matrix:\n\\[Y = g + \\epsilon, \\quad g \\sim N(0, \\sigma_g^2 K)\\]\nwhere K is constructed from all SNPs. SNP heritability typically captures 50-80% of family-based heritability, suggesting that much missing heritability is due to incomplete LD between causal variants and genotyped SNPs.\n\n\nGene-Environment Interactions\nGene-environment interactions (GxE) occur when genetic effects on a trait depend on environmental exposures. These interactions can explain additional trait variance and missing heritability, reveal biological mechanisms, and identify individuals who might benefit most from interventions. The basic GxE model includes an interaction term:\n\\[Y = \\beta_0 + \\beta_G \\cdot G + \\beta_E \\cdot E + \\beta_{GxE} \\cdot G \\times E + \\epsilon\\]\nThe power to detect interactions is substantially lower than for main effects, requiring sample sizes roughly four times larger for comparable power. This has limited GxE discoveries despite their likely importance.\nLarge-scale biobanks with extensive phenotyping enable systematic GxE screening. The UK Biobank’s collection of genetic data with thousands of environmental variables allows testing millions of GxE pairs. However, the multiple testing burden is even more severe than standard GWAS, requiring careful consideration of which interactions to test based on biological plausibility.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genetic Association</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/association.html#clinical-translation-and-precision-medicine",
    "href": "chapters/bioinformatics/association.html#clinical-translation-and-precision-medicine",
    "title": "24  Genetic Association",
    "section": "Clinical Translation and Precision Medicine",
    "text": "Clinical Translation and Precision Medicine\n\nRisk Prediction and Stratification\nThe ultimate goal of many GWAS is to improve disease prediction and enable preventive interventions. Polygenic risk scores are beginning to show clinical utility for some conditions. For coronary artery disease, individuals in the top 5% of PRS distribution have three-fold increased risk, comparable to monogenic mutations. This level of risk may warrant enhanced screening or preventive treatments.\nThe predictive performance of a PRS is typically evaluated using the area under the receiver operating characteristic curve (AUC) for binary traits or the proportion of variance explained (R²) for quantitative traits. The theoretical maximum R² for a PRS equals the SNP heritability, which for most complex diseases is 20-50%. Current PRS achieve 5-15% R² for well-studied traits with large GWAS, suggesting room for improvement.\nIntegration of PRS with clinical risk factors often provides better prediction than either alone. For example, combining a PRS with traditional cardiovascular risk factors (age, sex, cholesterol, blood pressure, smoking) improves the AUC from 0.75 to 0.80. The net reclassification improvement (NRI) quantifies how many individuals are correctly reclassified into different risk categories:\n\\[\\text{NRI} = P(\\text{up}|\\text{event}) - P(\\text{up}|\\text{non-event}) + P(\\text{down}|\\text{non-event}) - P(\\text{down}|\\text{event})\\]\n\n\nTherapeutic Target Discovery\nGWAS have identified numerous new drug targets, with genetic evidence substantially increasing the probability of drug development success. Variants that mimic the effects of pharmacological intervention provide human validation for targets. For example, PCSK9 loss-of-function variants reduce LDL cholesterol and cardiovascular disease risk, validating PCSK9 as a therapeutic target and leading to approved drugs.\nMendelian randomization (MR) uses genetic variants as instrumental variables to assess causal relationships between risk factors and disease. If genetic variants (G) affect disease (Y) only through a risk factor (X), the causal effect can be estimated:\n\\[\\hat{\\beta}_{XY} = \\frac{\\hat{\\beta}_{GY}}{\\hat{\\beta}_{GX}}\\]\nwhere β_GY is the variant-disease association and β_GX is the variant-risk factor association. This approach has confirmed causal roles for many risk factors and identified new intervention targets.\n\n\nPharmacogenomics\nPharmacogenomics uses genetic information to guide drug selection and dosing. While most pharmacogenomic variants were discovered through candidate gene studies rather than GWAS, the approach illustrates the clinical implementation of genetic testing. Classic examples include HLA-B*5701 screening before abacavir treatment to prevent hypersensitivity reactions and TPMT genotyping before thiopurine therapy to prevent toxicity.\nGWAS are beginning to identify pharmacogenomic variants, particularly for drug response rather than toxicity. The challenge is that pharmacogenomic GWAS require large cohorts of patients treated with specific drugs and with standardized outcome measures. Electronic health records linked to biobanks may enable such studies, though confounding by indication and treatment selection bias complicate the analysis.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genetic Association</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/association.html#future-directions-and-emerging-methods",
    "href": "chapters/bioinformatics/association.html#future-directions-and-emerging-methods",
    "title": "24  Genetic Association",
    "section": "Future Directions and Emerging Methods",
    "text": "Future Directions and Emerging Methods\n\nWhole-Genome Sequencing Studies\nAs sequencing costs continue to decline, whole-genome sequencing (WGS) association studies are becoming feasible. WGS captures all genetic variation, including rare variants, structural variants, and variants in regions poorly covered by genotyping arrays. The UK Biobank’s plan to sequence 500,000 genomes will enable comprehensive investigation of the full frequency spectrum of genetic variation.\nThe statistical challenges of WGS association studies are substantial. With 100 million variants to test, the multiple testing burden is severe. Rare variant association tests aggregate signals across multiple variants, such as burden tests that sum rare allele counts:\n\\[\\text{Burden} = \\sum_{j \\in \\text{gene}} w_j \\cdot I(\\text{MAF}_j &lt; 0.01) \\cdot G_{ij}\\]\nwhere w_j weights variants by predicted deleteriousness. SKAT (Sequence Kernel Association Test) allows for different effect directions:\n\\[Q = (Y - \\hat{Y})^T K (Y - \\hat{Y})\\]\nwhere K is a kernel matrix capturing genetic similarity at rare variants.\n\n\nMachine Learning Integration\nMachine learning methods are increasingly being applied to GWAS data to capture non-additive effects and improve prediction. Deep learning models can learn complex patterns in genetic data:\n\\[h^{(l+1)} = \\sigma(W^{(l)} h^{(l)} + b^{(l)})\\]\nwhere h^(l) represents the hidden layer activations. These models can capture epistatic interactions and non-linear effects that traditional methods miss.\nHowever, the interpretability of machine learning models remains a challenge. Methods like SHAP (SHapley Additive exPlanations) values can identify which variants contribute most to predictions, but understanding the biological basis of learned patterns is difficult. The integration of biological knowledge into neural network architectures, such as using gene regulatory networks to define connections, may improve both performance and interpretability.\n\n\nSingle-Cell Genomics Integration\nSingle-cell technologies provide unprecedented resolution of cellular heterogeneity and gene regulation. Integration with GWAS can identify the specific cell types and states where disease-associated variants act. The challenge is linking population-level GWAS signals to cell-level molecular phenotypes.\nMethods like LDSC-SEG (LD Score regression applied to specifically expressed genes) test whether GWAS signals are enriched in genes specifically expressed in certain cell types:\n\\[\\chi^2_j = N \\cdot h^2 \\sum_C \\tau_C \\cdot l_{j,C} + N \\cdot a + 1\\]\nwhere l_{j,C} is the LD score for SNP j restricted to genes in cell type category C, and τ_C represents the per-SNP heritability enrichment.\n\n\nGlobal Diversity and Health Equity\nThe lack of diversity in GWAS participants—over 80% are of European ancestry—limits the global applicability of findings and exacerbates health disparities. Polygenic risk scores show reduced predictive accuracy when applied across populations:\n\\[R^2_{\\text{target}} = R^2_{\\text{discovery}} \\cdot \\rho_g^2 \\cdot \\frac{h^2_{\\text{target}}}{h^2_{\\text{discovery}}}\\]\nwhere ρ_g is the genetic correlation between populations. This reduction arises from differences in LD patterns, allele frequencies, and potentially in causal variants or effect sizes.\nLarge-scale efforts to increase diversity include the All of Us Research Program, H3Africa, and GenomeAsia 100K. These studies will improve fine-mapping through trans-ancestry meta-analysis, enable discovery of population-specific variants, and ensure that precision medicine benefits all populations. Methods development focuses on improving trans-ancestry PRS through approaches like PRS-CSx that model population-specific LD patterns while borrowing strength across populations.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genetic Association</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/association.html#conclusion-achievements-and-challenges",
    "href": "chapters/bioinformatics/association.html#conclusion-achievements-and-challenges",
    "title": "24  Genetic Association",
    "section": "Conclusion: Achievements and Challenges",
    "text": "Conclusion: Achievements and Challenges\nGenome-wide association studies have transformed our understanding of the genetic basis of complex traits and diseases. From a handful of robustly associated variants in 2005, we now have catalogs of tens of thousands of associations providing insights into disease biology, evolution, and potential therapeutic targets. The field has developed sophisticated statistical methods to handle massive datasets, control for confounding, and extract biological meaning from genetic associations.\nThe achievements of GWAS are remarkable. We have learned that most complex traits are highly polygenic, with hundreds to thousands of variants contributing small effects. We have discovered that disease-associated variants are enriched in regulatory elements active in disease-relevant cell types, pointing to gene regulation as a key mechanism. We have identified new drug targets and begun to implement polygenic risk prediction in clinical settings. Perhaps most importantly, GWAS have revealed how much we still don’t understand about the relationship between genotype and phenotype.\nSignificant challenges remain. The functional interpretation of non-coding variants remains difficult, limiting our ability to translate associations into biological mechanisms. The lack of diversity in GWAS cohorts threatens to exacerbate health disparities as genetic findings are translated to clinical applications. The missing heritability problem suggests that our models of genetic architecture are incomplete. The integration of GWAS findings with other omics data and biological knowledge remains a major computational and conceptual challenge.\nLooking forward, the field is poised for continued transformation. Whole-genome sequencing will provide complete catalogs of genetic variation. Single-cell technologies will reveal the cellular contexts where variants act. Machine learning may capture complex genetic effects that current methods miss. Most importantly, increasing diversity and sample sizes will ensure that the benefits of genomic medicine are available to all.\nThe journey from the first GWAS to precision medicine has been remarkable but incomplete. As we enter the next phase of genetic discovery, the challenge shifts from finding associations to understanding their biological basis and translating findings to improve human health. This will require continued innovation in statistical methods, deeper integration of biological knowledge, and a commitment to ensuring that genomic medicine reduces rather than exacerbates health disparities. The promise of GWAS—to understand how genetic variation influences human health and to use this knowledge to prevent and treat disease—remains as compelling as ever.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Genetic Association</span>"
    ]
  },
  {
    "objectID": "chapters/web/gwas_databases/index.html",
    "href": "chapters/web/gwas_databases/index.html",
    "title": "25  GWAS candidates",
    "section": "",
    "text": "Browsing SNPs from a GWAS\nThe purpose of this exercise is to expose you to the different kinds of information that are stored in databases relevant to bioinformatics. It takes experience and skill to navigate the user interface of these databases. Here, you will see a few important ones, but there are many others. I have put a list at the end of this exercise with some additional relevant ones on Brightspace. Browse them at your heart’s content.\nGenome-wide association studies (GWAS) are a powerful tool in genetics and genomics research to identify genetic variants associated with diseases or traits. When a significant SNP is identified in a GWAS, it is a genomic signpost tagging an associated genomic region. Still, it only partially reveals the genes causing the disease. The genomic region identified likely contains several genes or genomic features. Investigating candidate disease genes near a GWAS SNP involves: Exploring the genomic region around the SNP. Studying nearby genes and regulatory elements. Evaluating their potential roles in the disease or trait of interest. A standard GWAS only includes a select subset of the SNPs in the genome. So, the most significant included SNP variant is rarely responsible for the disease. Still, it is so close to the causal one that individuals with the causal SNP variant carry it. The closer two SNPs are along the genome, the more likely they appear together like this. The further away from each other, the more likely it is that genetic recombination has removed this correlation.\nType 1 diabetes, often referred to as juvenile diabetes or insulin-dependent diabetes, is a chronic autoimmune condition that affects how the body regulates blood sugar (glucose). Unlike type 2 diabetes, which is commonly associated with lifestyle factors such as obesity and physical inactivity, type 1 diabetes is not preventable and typically develops early in life, often during childhood or adolescence. In type 1 diabetes, the immune system, usually responsible for defending the body against harmful invaders like bacteria and viruses, becomes misguided. It mistakenly identifies the insulin-producing beta cells within the pancreas as foreign threats. This misrecognition triggers an autoimmune response, leading immune cells to launch an attack on these vital beta cells. The pancreas, an organ located behind the stomach, is a key player in regulating blood sugar levels. It consists of clusters of cells called the Islets of Langerhans, which house the insulin-producing beta cells. When the immune system attacks these beta cells, it results in their destruction or severe impairment. This process is thought to involve various immune cells, such as T-cells and antibodies, which play pivotal roles in autoimmune reactions. Due to this autoimmune attack, the pancreas gradually loses its ability to produce insulin, a hormone with crucial responsibilities in maintaining proper blood sugar balance. Insulin acts as a molecular key that unlocks the doors of cells throughout the body, allowing glucose to enter and be used as an energy source. Think of insulin as a bridge that facilitates the movement of glucose from the bloodstream into cells. Research into type 1 diabetes is ongoing, and scientists are exploring potential cures and better management strategies. However, until a cure is found, individuals with type 1 diabetes continue to lead fulfilling lives by effectively managing their condition through insulin therapy, a balanced diet, regular exercise, and closely monitoring their blood sugar levels.\nStart by examining a Manhattan plot or list of significant SNPs obtained from a GWAS study. We are going to select one of these SNPs and do further analysis. While doing the analysis, note the SNP identifier (rsID) and its significance level (p-value). Go to locuszoom.org and press the blue button shown on Figure 25.1:\nNow, you should be able to search for public GWAS studies. Copy/paste “Rare Genetic Variants of Large Effect Influence Risk of Type 1 Diabetes” into the search bar. Only one result should show up. Press it. Now, you are ready to examine the Manhattan plot. You can see a lot of significant SNPs in the Manhattan plot, and you are welcome to examine them, but the one we are interested in for this exercise is the one located close to RSBN1. Click it, and it will zoom in. The SNP has an rsID of rs6679677, and its position is 113.761.186 on chromosome 1. It’s a C to A mutation. Save this information because we will use it for the rest of the exercise.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>GWAS candidates</span>"
    ]
  },
  {
    "objectID": "chapters/web/gwas_databases/index.html#browsing-snps-from-a-gwas",
    "href": "chapters/web/gwas_databases/index.html#browsing-snps-from-a-gwas",
    "title": "25  GWAS candidates",
    "section": "",
    "text": "Figure 25.1: Screenshot of the LocusZoom website",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>GWAS candidates</span>"
    ]
  },
  {
    "objectID": "chapters/web/gwas_databases/index.html#exploring-a-genomic-region",
    "href": "chapters/web/gwas_databases/index.html#exploring-a-genomic-region",
    "title": "25  GWAS candidates",
    "section": "Exploring a genomic region",
    "text": "Exploring a genomic region\nIn this section, we are going to explore our SNP and its surrounding genomic regions to identify our candidate gene/genes for diabetes type 1. Go to the UCSC Genome Browser. In the “Human Assembly” drop-down, make sure it says “GRh38/hg38”. That way, you access the genome assembly also used in the GWAS study. If you choose another one, the SNP and gene coordinates will be different. Now paste your SNP identifier rs6679677 into the search field where it says “Position/Search Term” and click GO. You will get a list of search results. Clicking the link in the top one (if it says rs6679677) should take you to a page centered on this SNP looking something like the one shown in Figure 25.2:\n\n\n\n\n\n\nFigure 25.2: Screenshot of the UCSC genome browser\n\n\n\nNow, you see the SNP in a window. You can play around with the zoom function until you start seeing a couple of other SNP IDs. Now we want to look at surrounding genes, so scroll down to the “Genes and Gene predictions” Section. All the options you see here are called “tracks”, and they all contain different information about the genome. If you want to look at the annotated gene, select the “GENCODE V48” select “pack” and press refresh on the right side of the section. Play around with the different settings (pack, dense, squish, all). They all give you a different amount of information. Select the one that you are most comfortable with when exploring. You might have to change it for some types of questions. “Pack” is a good place to start.\nIf there is too much going on for your liking, you can hide certain tracks at the bottom of the page. Here is a link that will lead you to the browser, with all but the gene and SNP tracks hidden: [UCSC Genome Browser with hidden tracks] (https://genome-euro.ucsc.edu/cgi-bin/hgTracks?db=hg38&lastVirtModeType=default&lastVirtModeExtraState=&virtModeType=default&virtMode=0&nonVirtPosition=&hgsid=407916288_fkUQBcFRTUFVP1XZP2VxzusTgHLA)\n\nExercise 25-1\nPlay around with the zoom function. To the left of where you inserted the SNP ID is the size of the window you are looking at. The more you zoom out, the more genes you will be able to see.\n\n\nExercise 25-2\nIdentify nearby genes, including their names, locations, and orientations. Remember that linkage means that the causal gene may not be the one closest to the GWAS SNP. Are there any genes located close to the SNP of interest? If yes. Note them down for further analysis.\n\n\nExercise 25-3\nWhat is the genomic context (e.g., intronic, intergenic) of the SNP?\n\n\nExercise 25-4\nYou see the same gene multiple times in the gene track. Why do you think that is?\n\n\nExercise 25-5\nTry to select other tracks such as promoters (EPDnew Promoters) and regulatory elements (ENCODE Regulation). See if there are known regulatory elements, such as enhancers or promoters, in the vicinity of the SNP.\n\n\nExercise 25-6\nAre there any annotated regulatory elements near the SNP that might influence gene expression? Try to press the regulatory track. What kind of information does this track represent? (Hint: Read the description of the track).",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>GWAS candidates</span>"
    ]
  },
  {
    "objectID": "chapters/web/gwas_databases/index.html#gathering-information-about-genes",
    "href": "chapters/web/gwas_databases/index.html#gathering-information-about-genes",
    "title": "25  GWAS candidates",
    "section": "Gathering information about genes",
    "text": "Gathering information about genes\nNow, we are ready to look at the genes we are interested in relation to our SNP. For this, we will use 3 well-known databases. The goal is to identify which genes are the potential cause of type 1 diabetes. Take your genes to each of the websites and use the information they give you to see which genes can be related to diabetes.\n\nNCBI Gene\nENSEMBL\nGenecards\n\nIt’s important to understand what kind of information these sites provide and their intentions. So, let us do some light exploration.\n\nExercise 25-7\nGoogle “NCBI”. What is it, and who administers it? Do the same with ENSEMBL. What’s its function, and who administers it?\n\n\nExercise 25-8\nOn the NCBI Gene website, the drop-down menu shows “gene”, but other databases on NCBI exist. Get yourself an overview. Do you recognize other databases? Some you don’t know?\n\n\nExercise 25-9\nWhen a gene is found in the “About this gene” section at ENSEMBL, try to press “phenotypes” if there are any. Do you see diabetes for any of the genes?\n\n\nExercise 25-10\nThe Genecards resource is different from the two other databases. Can you list two major differences?\n\n\nExercise 25-11\nBy now, you should have a feeling for the databases and their goals. NCBI and ENSEMBL store the same sequence information, so it is mostly about which site presents data in the most useful way for the task at hand. Which one is your favorite one and why? Which of the genes are related to Type 1 Diabetes? If you can’t find a related gene, go back to “Accessing the UCSC Genome Browser and make initial exploration” and look at your tracks again. You may have missed it.\n\n\nExercise 25-12\nWith the gene you now have chosen, answer the following questions by using the 3 above databases.\n\nHow long is the entire of the gene (in bp or kb)?\nHow many transcripts of the gene are in NCBIs “Transcript table”?\nDoes Ensembl and NCBI show the same number of transcripts?\nIn the graphical representation of a gene, how are exons and introns depicted at ENSEMBL and NCBI?\nAt ENSEMBL, try to click on the UniprotKN identifier. This takes you to a page in the “UniProt” database. Can you retrive the amino acid sequence? - Do that.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>GWAS candidates</span>"
    ]
  },
  {
    "objectID": "chapters/web/gwas_databases/index.html#exploring-tissue-expression",
    "href": "chapters/web/gwas_databases/index.html#exploring-tissue-expression",
    "title": "25  GWAS candidates",
    "section": "Exploring tissue expression",
    "text": "Exploring tissue expression\nIn this step, we are interested in investigating our gene’s expression. Or multiple genes if we have yet to choose a specific gene. Expression analysis will help us understand the genes better. Go to the GTEx Portal (https://www.gtexportal.org/home/) and search for the gene/genes associated with your SNP to see their expression profiles across tissues. Explore tissue-specific gene expression data for the identified genes. In which tissues is the gene(s) expressed, and is there any tissue where its expression is notably higher? You may know “EBV-transformed lymphocytes” as “lymphocytes” cells. They are important to the immune system.\n\nExercise 25-13\nIn which tissues is the gene(s) typically expressed, and are there variations in expression levels across different tissues?\n\n\nExercise 25-14\nDo the expression profiles make sense with the information you got from the other databases? Do you expect to see certain tissues with a high expression? Other with a low? Is it relevant to the disease we are investigating?\n\n\nExercise 25-15\nWhat can you infer about the potential functional relevance of the gene(s) in the context of the disease or trait?",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>GWAS candidates</span>"
    ]
  },
  {
    "objectID": "chapters/web/gwas_databases/index.html#hypotheses-and-discussion",
    "href": "chapters/web/gwas_databases/index.html#hypotheses-and-discussion",
    "title": "25  GWAS candidates",
    "section": "Hypotheses and Discussion",
    "text": "Hypotheses and Discussion\nBased on the information gathered from UCSC, various gene databases, and GTEx, propose hypotheses about the potential role of the identified gene(s) in the disease or trait associated with the SNP. Think about the importance of tissue-specific gene expression in understanding disease mechanisms. Are there any other genes or regulatory elements near the SNP that could also be relevant to the disease or trait?\n\nExercise 25-16\nBased on the genomic and expression data, what are your hypotheses regarding the role of the identified gene(s) in the disease or trait?\n\n\nExercise 25-17\nHow might tissue-specific gene expression patterns provide insights into the disease mechanism?\n\n\nExercise 25-18\nAre there potential challenges or limitations in establishing causality between the gene(s) and the disease?\n\n\nExercise 25-19\nSummarize your findings and discuss the significance of investigating candidate disease genes near a GWAS SNP. Reflect on the challenges and limitations of this approach in pinpointing causal genes for complex diseases.\n\n\nExercise 25-20\nWhat did you learn from this exercise about the importance of exploring the genomic context and tissue-specific gene expression in GWAS follow-up?\n\n\nExercise 25-21\nHow might these findings inform future research or therapeutic approaches for the disease or trait?",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>GWAS candidates</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/databases.html",
    "href": "chapters/bioinformatics/databases.html",
    "title": "26  Databases",
    "section": "",
    "text": "GenBank\nBioinformatics databases are crucial resources that provide access to a vast array of biological data, ranging from genetic sequences and protein structures to functional annotations and disease-related information. These databases play a pivotal role in various research fields, aiding scientists in data analysis, hypothesis testing, and discovery. Here’s an overview of some of the most important bioinformatics databases and how to use them:\nGenBank is a comprehensive database maintained by the National Center for Biotechnology Information (NCBI) that contains DNA sequences submitted by researchers worldwide. It includes sequences from various organisms, along with associated metadata and annotations. Users can search for specific sequences, access genome assemblies, and retrieve information for various genomic regions.\nKey Features of GenBank:",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/databases.html#genbank",
    "href": "chapters/bioinformatics/databases.html#genbank",
    "title": "26  Databases",
    "section": "",
    "text": "Sequence Collection: GenBank contains a vast collection of DNA, RNA, and protein sequences from diverse organisms, including viruses, bacteria, fungi, plants, animals, and more.\nAnnotations: Sequences in GenBank are accompanied by annotations that provide valuable information about the sequence, such as gene names, protein products, functional regions, and other relevant data.\nGenomic Assemblies: GenBank hosts whole genome assemblies for many organisms, enabling researchers to access and analyze complete genomes.\nVersioning: Each sequence in GenBank is assigned a version number to track changes and updates. This allows researchers to access historical versions of sequences.\nCross-References: GenBank cross-references other databases and resources, facilitating integration and collaboration among different data sources.\nBioProject and BioSample: GenBank integrates with BioProject and BioSample databases, providing additional context for submitted sequences, such as information about the source organism and experimental details.\nSearch and Retrieval: The GenBank website offers a user-friendly interface for searching and retrieving sequences based on keywords, accession numbers, organisms, and more.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/databases.html#uniprot",
    "href": "chapters/bioinformatics/databases.html#uniprot",
    "title": "26  Databases",
    "section": "UniProt",
    "text": "UniProt\nUniProt is a database that provides detailed information about protein sequences and their functional annotations. It integrates data from various resources, offering a centralized repository for protein-related information. Researchers can search for proteins by name, sequence, or keywords and access data such as protein function, structure, domains, and post-translational modifications.\nUniProt (Universal Protein Resource) is a comprehensive and widely used bioinformatics database that provides a centralized repository of protein sequences and functional information. Maintained collaboratively by the European Bioinformatics Institute (EBI), the Swiss Institute of Bioinformatics (SIB), and the Protein Information Resource (PIR), UniProt serves as a valuable resource for researchers, educators, and students in the fields of biology, genetics, and molecular biology.\nUniProt is divided into three main components:\n\nUniProtKB: This is the primary component, providing a comprehensive collection of protein sequences and annotations. UniProtKB is further subdivided into three subdatabases:\nUniProtKB/Swiss-Prot: Curated protein entries with high-quality annotations. UniProtKB/TrEMBL: Automatically annotated protein entries that have not yet been curated. UniRef: This component clusters closely related protein sequences into UniRef clusters, reducing redundancy and simplifying sequence analysis.\nUniParc: UniParc contains a unique and non-redundant collection of all publicly available protein sequences, regardless of their level of annotation.\n\nKey Features of UniProt:\n\nProtein Sequences: UniProt contains a vast collection of protein sequences from a wide range of organisms, including bacteria, archaea, fungi, plants, animals, and viruses.\nFunctional Annotations: Each protein entry in UniProt is accompanied by extensive functional annotations, providing information about the protein’s biological role, structure, domains, post-translational modifications, interactions, subcellular localization, and more.\nCross-References: UniProt integrates information from other databases and resources, offering cross-references to relevant data sources, such as gene databases, literature citations, and 3D protein structures.\nIsoforms and Variants: UniProt includes information about protein isoforms, splice variants, and natural variants that affect protein function.\nTaxonomic Information: Protein entries are categorized based on taxonomy, enabling users to explore protein diversity across different organisms.\nSequence Alignments: UniProt provides sequence alignments for families of homologous proteins, aiding in understanding protein evolution and relationships.\nDisease Annotations: UniProt includes annotations linking proteins to various diseases, such as genetic disorders and cancers, based on experimental evidence.\nLiterature Citations: Protein entries often include references to scientific literature, allowing users to access the original research articles related to specific proteins.\nKeywords and Annotations: UniProt uses controlled vocabularies to assign keywords and annotations to proteins, facilitating standardized and structured data representation.\n\nUsage of UniProt:\n\nSearch: Enter protein names, identifiers, gene names, keywords, or organisms in the search bar.\nBrowse: Explore protein entries by taxonomy, organism, keywords, and functional annotations.\nRetrieve Information: Access protein sequences, functional annotations, cross-references, and literature citations.\nAnalyze: Utilize the provided data to understand protein function, structure, interactions, and disease associations.\nIntegration: Integrate UniProt data with other bioinformatics tools and resources for comprehensive analyses.\nUniProt is an invaluable resource for researchers studying proteins, their functions, and their roles in various biological processes. Its user-friendly interface, extensive annotations, and cross-referencing capabilities make it a cornerstone tool in the field of molecular biology.\n\nPubMed: PubMed is a database maintained by the National Library of Medicine, offering access to an extensive collection of biomedical literature. Researchers can search for articles related to specific topics, diseases, genes, or proteins. PubMed provides abstracts, full-text articles, and links to external resources, aiding literature review and research planning.\nEnsembl: Ensembl is a genome browser and annotation database that provides genomic sequences, gene annotations, and functional information for a wide range of species. Users can explore gene structures, regulatory elements, genetic variations, and comparative genomics data. Ensembl’s browser interface allows for interactive exploration of genomic regions.\nNCBI Gene: This database offers comprehensive information about genes, including functional annotations, expression data, gene-disease associations, and orthologous relationships. Researchers can search for specific genes, access genetic and protein sequences, and retrieve details about gene function and expression patterns.\nThe NCBI Gene database, maintained by the National Center for Biotechnology Information (NCBI), is a comprehensive resource that provides information about genes from a wide range of organisms. It serves as a centralized repository for gene-related data, annotations, and functional information. The database plays a crucial role in molecular biology research, aiding scientists in understanding gene functions, interactions, and regulatory mechanisms.\nKey Features of the NCBI Gene Database:\nGene Information: The NCBI Gene database contains detailed information about individual genes, including gene names, symbols, aliases, genomic locations, chromosomal coordinates, and orientations.\nGene Function: Each gene entry is accompanied by functional annotations that describe the gene’s biological roles, molecular functions, and involvement in cellular processes.\nTranscripts and Isoforms: The database provides information about different transcript variants and isoforms associated with a gene. This includes details about alternative splicing, coding regions, and untranslated regions.\nOrthologs and Paralogs: NCBI Gene includes information about orthologous and paralogous genes across different species, aiding in understanding gene evolution and relationships.\nHomology and Alignment: Gene entries often include sequence alignments, similarity scores, and evolutionary relationships with related genes.\nProtein Products: NCBI Gene provides information about the protein products encoded by genes, including functional domains, post-translational modifications, and protein interactions.\nGene Ontology (GO) Annotations: Many gene entries are associated with Gene Ontology terms that categorize gene functions, cellular components, and biological processes.\nExpression Data: The database offers data on gene expression patterns in various tissues, developmental stages, and conditions, aiding in understanding gene regulation.\nGenomic Context: Gene entries include information about neighboring genes, regulatory elements, and genetic variations in the genomic vicinity.\nDisease Associations: Gene entries may include annotations linking genes to specific diseases, based on experimental evidence.\nUsage of the NCBI Gene Database:\nTo use the NCBI Gene database effectively:\nSearch: Enter gene names, symbols, IDs, or keywords in the search bar. Browse: Explore gene entries by species, functional annotations, and genomic locations. Retrieve Information: Access gene details, functional annotations, orthologs, paralogs, and protein products. Analyze: Utilize the provided data to understand gene functions, expression patterns, and disease associations. Cross-Reference: NCBI Gene integrates with other NCBI databases, providing links to nucleotide sequences, protein sequences, PubMed articles, and more. The NCBI Gene database serves as a critical resource for researchers studying individual genes and their roles in various biological processes. Its comprehensive annotations, integration with other NCBI resources, and user-friendly interface make it an essential tool for molecular biologists, geneticists, and researchers across diverse fields.\ndbSNP: The Single Nucleotide Polymorphism Database (dbSNP) is a repository of genetic variations, including SNPs and other types of variations. Researchers can search for specific variations, retrieve their annotations, and explore data related to population frequencies, diseases, and functional effects.\nThe dbSNP (Single Nucleotide Polymorphism Database) is a comprehensive bioinformatics resource maintained by the National Center for Biotechnology Information (NCBI). It serves as a repository for genetic variations, including single nucleotide polymorphisms (SNPs), insertions, deletions, and other types of genetic variants. dbSNP plays a crucial role in cataloging and providing access to genetic variations across various species, aiding researchers in understanding the genetic basis of traits, diseases, and population diversity.\nKey Features of the dbSNP Database:\nVariation Catalog: dbSNP contains a wide variety of genetic variations, including SNPs, short insertions and deletions (indels), microsatellites, and larger structural variants.\nVariant Annotations: Each genetic variant is accompanied by annotations, such as genomic coordinates, alleles, frequencies in different populations, and clinical significance (if applicable).\nPopulation Data: dbSNP provides information about the frequency of genetic variants in different human populations, enabling researchers to study population genetics and diversity.\nFunctional Annotations: Many genetic variants are annotated with potential functional consequences, such as effects on protein coding, splicing, or regulatory elements.\nCross-References: dbSNP cross-references other databases, allowing users to access additional information about genes, proteins, and diseases associated with specific genetic variants.\nGenomic Context: The database includes information about the genomic context of variations, such as neighboring genes, regulatory elements, and conserved regions.\nClinVar Integration: dbSNP integrates with the ClinVar database, providing information about the clinical significance of genetic variants and their associations with diseases.\nGenome Build Compatibility: dbSNP supports different genome builds, enabling users to map variations to specific versions of the genome.\nUsage of the dbSNP Database:\nTo use the dbSNP database effectively:\nSearch: Enter SNP IDs, genomic coordinates, gene names, or keywords in the search bar. Browse: Explore variants by chromosome, population, frequency, or functional annotations. Retrieve Information: Access variant details, genomic coordinates, allele frequencies, and clinical significance information. Analyze: Utilize variant annotations for studying disease associations, population genetics, and functional effects. Integration: Combine dbSNP data with other resources to enhance genetic and genomic analyses. dbSNP is a crucial resource for researchers investigating the genetic variations that contribute to phenotypic differences, diseases, and population diversity. Its extensive collection of annotated variants, integration with other databases, and user-friendly interface make it an indispensable tool for geneticists, epidemiologists, and researchers in related fields.\nOMIM: Online Mendelian Inheritance in Man (OMIM) is a database that catalogs genetic mutations and their relationships to inherited diseases. Researchers can search for genes, phenotypes, and diseases to access information about genetic disorders, associated mutations, and relevant literature.\nThe Online Mendelian Inheritance in Man (OMIM) database is a comprehensive and authoritative resource that catalogues information about genetic disorders, traits, and other phenotypic traits in humans. Maintained by the McKusick-Nathans Institute of Genetic Medicine at Johns Hopkins University, OMIM plays a pivotal role in bridging the gap between genetic research and clinical medicine, providing valuable insights into the genetic basis of various human conditions.\nKey Features of the OMIM Database:\nGenetic Disorders: OMIM contains detailed information about a wide range of genetic disorders, both rare and common, providing insights into the genetic mutations, inheritance patterns, clinical features, and molecular mechanisms underlying these conditions.\nPhenotypic Traits: In addition to genetic disorders, OMIM also catalogues information about various phenotypic traits, such as physical characteristics, susceptibility to diseases, and responses to treatments.\nGene-Centric Information: OMIM provides gene-centric entries that include information about specific genes associated with genetic disorders. These entries detail gene functions, mutations, protein products, and any known associations with diseases or traits.\nClinical Descriptions: Each entry includes detailed clinical descriptions of the disorders or traits, including symptoms, diagnostic criteria, and information about disease progression.\nGenetic Inheritance Patterns: OMIM categorizes disorders based on their inheritance patterns, such as autosomal dominant, autosomal recessive, X-linked, and mitochondrial.\nGene and Locus References: OMIM cross-references genes and loci to other databases and resources, facilitating integration with genomic and functional data.\nLinks to Literature: OMIM entries include references to scientific literature, providing access to primary research articles and reviews related to specific disorders and genes.\nMolecular Mechanisms: Many OMIM entries describe the molecular mechanisms underlying genetic disorders, providing insights into how mutations affect biological processes.\nUsage of the OMIM Database:\nTo use the OMIM database effectively:\nSearch: Enter disease names, gene names, phenotypic keywords, or gene identifiers in the search bar. Browse: Explore entries by disease categories, gene names, inheritance patterns, and other criteria. Retrieve Information: Access detailed information about genetic disorders, phenotypic traits, genes, clinical descriptions, and molecular mechanisms. Analyze: Utilize the provided data to understand the genetic basis of diseases, inheritance patterns, and potential therapeutic targets. Integration: Combine OMIM data with other genetic and genomic resources to enhance research and clinical applications. OMIM is widely used by geneticists, clinicians, researchers, and healthcare professionals to gain insights into the genetic underpinnings of diseases, guide clinical diagnoses, and inform therapeutic strategies. Its rich content, authoritative sources, and user-friendly interface make it an essential tool for anyone involved in genetics and genomics research.\nSTRING: The STRING database provides information about protein-protein interactions and functional associations. Users can search for a protein of interest to explore its interactions, view network maps, and analyze the potential functions of proteins within a network context.\nTo use these databases effectively:\nKeyword Search: Start by entering relevant keywords, gene names, protein identifiers, or disease names in the search bar. Filters: Many databases offer filters to refine search results based on criteria such as species, data type, and publication date. Advanced Search: Use advanced search options to specify specific fields or search criteria to narrow down results. Data Retrieval: Once you find relevant information, you can often download sequences, annotations, and other data for further analysis. Citations: When using data from these databases in your research, make sure to provide proper citations to acknowledge the original sources. These databases are continuously updated and expanded, so it’s essential to explore their documentation and tutorials to stay informed about the latest features and search strategies.\nThe STRING database (Search Tool for the Retrieval of Interacting Genes/Proteins) is a valuable bioinformatics resource that provides information about protein-protein interactions (PPIs) and functional associations. STRING helps researchers explore the relationships between proteins, gain insights into biological processes, and understand the complex networks that underlie various cellular functions. Maintained by a collaborative team of researchers, STRING is widely used in various fields, including molecular biology, systems biology, and drug discovery.\nKey Features of the STRING Database:\nProtein Interaction Data: STRING aggregates and integrates experimental and computational data on protein interactions from diverse sources, including high-throughput experiments, curated databases, and literature mining.\nFunctional Associations: Beyond direct physical interactions, STRING provides information about functional associations between proteins, such as co-expression, shared domains, and similar annotations.\nConfidence Scores: STRING assigns confidence scores to interactions and associations based on the evidence supporting them. These scores help users assess the reliability of the reported interactions.\nNetwork Visualization: STRING generates interactive graphical representations of protein networks, allowing users to visualize and explore protein interactions and functional relationships in a visual format.\nSpecies Coverage: STRING supports a wide range of organisms, from model organisms to less-studied species, enabling researchers to investigate protein interactions in different biological contexts.\nEnrichment Analysis: STRING offers enrichment analysis tools that help users identify overrepresented functional categories, pathways, and Gene Ontology terms within a set of proteins of interest.\nPrediction Methods: STRING includes computational prediction methods that estimate potential interactions based on sequence similarities, domain architectures, and other features.\nCross-References: STRING provides cross-references to external resources, allowing users to access additional information about interacting proteins, pathways, and diseases.\nUsage of the STRING Database:\nTo use the STRING database effectively:\nSearch: Enter protein names, identifiers, or keywords to find interactions and associations. Network Visualization: Explore and visualize protein interaction networks, adjusting confidence thresholds and interaction types. Retrieve Information: Access interaction scores, functional annotations, pathway information, and external references. Analyze: Utilize the provided data to understand protein functions, regulatory mechanisms, and disease associations. Enrichment Analysis: Perform enrichment analysis to identify functional themes within a set of proteins. Integration: Combine STRING data with other omics data to gain a holistic understanding of cellular processes. STRING is a versatile tool for researchers investigating protein interactions, cellular pathways, and systems-level biology. Its user-friendly interface, comprehensive data sources, and advanced analysis features make it an essential resource for studying the complex molecular networks that govern biological functions.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/databases.html#sequence-databases",
    "href": "chapters/bioinformatics/databases.html#sequence-databases",
    "title": "26  Databases",
    "section": "26.1 Sequence databases",
    "text": "26.1 Sequence databases",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/databases.html#genome-browsers",
    "href": "chapters/bioinformatics/databases.html#genome-browsers",
    "title": "26  Databases",
    "section": "26.2 Genome browsers",
    "text": "26.2 Genome browsers\nThe UCSC Genome Browser is a widely used and comprehensive online tool for visualizing and exploring genomic information from a wide range of organisms. Developed and maintained by the University of California, Santa Cruz (UCSC), this browser offers a user-friendly interface to navigate and analyze genomes, gene structures, regulatory elements, genetic variations, and other genomic features. Here’s an overview of the UCSC Genome Browser and its key features:\nFeatures of the UCSC Genome Browser:\nGenome Selection: The UCSC Genome Browser supports a vast collection of genomes from various species, including humans, model organisms, and microbes. Users can select the genome of interest from a dropdown menu.\nGenome Views: The browser provides different views of the genome, including the standard linear view, a circular view (for some genomes), and a conservation view that highlights evolutionarily conserved regions.\nNavigation and Zoom: Users can easily navigate through genomic regions using navigation arrows and zoom controls. This allows detailed examination of specific regions.\nAnnotation Tracks: One of the most powerful features of the UCSC Genome Browser is the ability to overlay various types of annotation tracks onto the genome view. These tracks include gene annotations, regulatory elements, genetic variations, and more. Users can customize which tracks are displayed and adjust their visibility and order.\nGene Annotations: The browser displays gene structures, including exons, introns, coding sequences, and untranslated regions. It provides information about gene names, functions, and alternative splicing patterns.\nRegulatory Elements: Regulatory elements such as promoters, enhancers, and transcription factor binding sites can be visualized, aiding in understanding gene regulation.\nGenetic Variations: Genetic variations, including single nucleotide polymorphisms (SNPs), insertions, and deletions, can be displayed. Users can examine variations’ positions, alleles, frequencies, and potential functional consequences.\nCustom Tracks: Users can upload their own data, such as experimental results or custom annotations, to visualize alongside the provided tracks.\nComparative Genomics: The browser offers tools to compare genomic sequences and annotations across multiple species. This aids in identifying conserved regions and evolutionary changes.\nSearch and Highlight: Users can search for specific genes, regions, sequences, or annotations and highlight them in the genome view for detailed examination.\nLinks to External Resources: The UCSC Genome Browser provides links to external databases and resources, enabling seamless access to additional information.\nUsage:\nTo use the UCSC Genome Browser:\nAccess the Browser: Go to the UCSC Genome Browser website (genome.ucsc.edu).\nChoose a Genome: Select the genome of interest from the dropdown menu.\nNavigate and Explore: Navigate to specific genomic regions using the search bar, navigation arrows, and zoom controls. Customize the displayed tracks to focus on relevant features.\nAnalyze and Download: Analyze gene structures, regulatory elements, and genetic variations in the context of the genome. Download data and images for use in research or presentations.\nThe UCSC Genome Browser serves as an invaluable resource for researchers, educators, and students seeking to understand and analyze genomic information across various organisms. Its intuitive interface, extensive collection of annotation tracks, and powerful visualization capabilities make it a cornerstone tool in the field of genomics.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/database_searching.html",
    "href": "chapters/bioinformatics/database_searching.html",
    "title": "27  Database Searching",
    "section": "",
    "text": "Introduction: The Need for Database Searching\nHere’s a staggering fact to put things in perspective: as of 2024, GenBank contains over 250 million sequences comprising more than 3 trillion nucleotides, and the database doubles approximately every 18 months. That’s exponential growth in the truest sense—if you wait a year and a half, there’s twice as much data to sift through. This vast, ever-expanding repository of genetic information is simultaneously one of the most valuable resources in modern biology and one of the most daunting computational challenges. The fundamental problem is straightforward: you’ve got a newly determined sequence—maybe a gene you just cloned, a protein you purified, or a mysterious open reading frame from a genome project—and you want to know: what is this thing related to? What does it do? Where did it come from? To answer these questions, you need to search through billions of sequences to find the ones that matter. But here’s the catch: you can’t just do brute-force comparisons because even with modern computers, comparing your query to every sequence in the database using optimal alignment algorithms would take forever. You need algorithms that are both fast enough to be practical and sensitive enough to detect biologically meaningful relationships even when sequence similarity is weak. This is the core challenge of sequence database searching, and solving it has been one of the great success stories of bioinformatics. The ability to rapidly and accurately search sequence databases has become as fundamental to modern biology as the microscope was to earlier generations—it’s how we identify the function of newly sequenced genes, find homologs across species, detect contamination in sequencing projects, and discover novel members of protein families. Every time you sequence something, the first question is always: what does BLAST say? That ubiquity speaks to how thoroughly database searching has permeated biological research. The challenge goes deeper than just computational speed, though. Biological sequences evolve through mutations, insertions, and deletions, so related sequences don’t look identical—they might share only limited similarity, and that similarity might be partially obscured by random noise. You have to distinguish genuine evolutionary relationships from chance matches that happen to look similar by accident. This requires sophisticated statistical frameworks that can tell you not just “these sequences align with this score,” but “this alignment is statistically significant and unlikely to occur by random chance.” BLAST elegantly combines both requirements: heuristic algorithms for speed and rigorous statistics for confidence.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Database Searching</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/database_searching.html#the-computational-challenge",
    "href": "chapters/bioinformatics/database_searching.html#the-computational-challenge",
    "title": "27  Database Searching",
    "section": "The Computational Challenge",
    "text": "The Computational Challenge\n\nScale and Complexity\nThe fundamental challenge of database searching can be stated simply: given a query sequence Q of length m, find all sequences in a database D containing n sequences with total length L that are similar to Q. A naive approach using dynamic programming would require O(mL) time, which becomes prohibitively expensive for large databases. For a typical protein query of 300 amino acids searched against a database containing 100 million proteins with average length 400, this would require approximately 10^13 operations—far too slow for routine use.\nThe problem is further complicated by the need to detect different types of similarity. Global similarity tools seek sequences that align well over their entire length, appropriate for finding orthologs or recently diverged sequences. However, many biologically important relationships involve only local regions of similarity—shared domains, motifs, or functional sites. Local alignment methods must efficiently identify these regions without prior knowledge of their location or extent.\nThe statistical challenge is equally daunting. In a large database, we expect to find many alignments with reasonable scores simply by chance. A scoring system must distinguish these random alignments from those reflecting true biological relationships. This requires not just a scoring scheme but also a statistical framework for assessing the significance of observed scores. The larger the database, the more likely we are to observe high-scoring alignments by chance, necessitating adjustment of significance thresholds based on database size.\n\n\nEvolution and Sequence Similarity\nBiological sequences evolve through a process of mutation and selection, creating a complex landscape of sequence relationships. Homologous sequences—those descended from a common ancestor—may have diverged to the point where their similarity is barely detectable above background noise. The “twilight zone” of sequence similarity, typically below 20-25% identity for proteins, represents a region where homology cannot be reliably inferred from sequence similarity alone.\nDifferent types of mutations occur at different rates and have different impacts on function. Substitutions between chemically similar amino acids (conservative substitutions) are more likely to preserve function and thus occur more frequently in evolution. Insertions and deletions, while less common, can dramatically alter local sequence similarity. These evolutionary processes must be modeled in the scoring systems used for database searching.\nThe rate of evolution varies dramatically across sequences and even within different regions of the same sequence. Functionally important regions evolve slowly due to purifying selection, while less constrained regions may diverge rapidly. This heterogeneity means that related sequences may show a patchwork of strongly conserved and highly divergent regions, requiring algorithms capable of identifying local similarities even when embedded in otherwise dissimilar sequences.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Database Searching</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/database_searching.html#blast-the-basic-local-alignment-search-tool",
    "href": "chapters/bioinformatics/database_searching.html#blast-the-basic-local-alignment-search-tool",
    "title": "27  Database Searching",
    "section": "BLAST: The Basic Local Alignment Search Tool",
    "text": "BLAST: The Basic Local Alignment Search Tool\n\nHistorical Development and Impact\nBLAST, developed by Altschul, Gish, Miller, Myers, and Lipman in 1990, revolutionized sequence database searching by providing a method that was both fast and statistically rigorous. The original paper has become one of the most cited scientific papers of all time, reflecting BLAST’s fundamental importance to biological research. The algorithm’s success stems from its elegant solution to the speed-sensitivity trade-off: by using a heuristic approach that sacrifices guaranteed optimal solutions for dramatic speed improvements, BLAST made large-scale database searching practical while maintaining sufficient sensitivity for most biological applications.\nThe key insight behind BLAST was that homologous sequences, even those that have diverged substantially, typically contain short regions of high similarity. Rather than comparing entire sequences, BLAST first identifies these short, high-scoring segments (seeds) and then extends them to find longer alignments. This seed-and-extend strategy reduces the search space dramatically while maintaining high sensitivity for biologically relevant matches.\n\n\nThe Original BLAST Algorithm\nThe original BLAST algorithm operates in three main phases: seeding, extension, and evaluation. Each phase involves critical parameters and design decisions that affect both speed and sensitivity.\n\n\n\n\n\n\n\n\nFigure 27.1: BLAST algorithm workflow showing the three main phases: seeding (word matching), extension (ungapped then gapped), and evaluation (statistical significance). The seed-and-extend strategy dramatically reduces search space while maintaining sensitivity.\n\n\n\n\n\nSeeding Phase: BLAST begins by decomposing the query sequence into overlapping words of length \\(W\\) (typically \\(W=3\\) for proteins, \\(W=11\\) for nucleotides). For each word, BLAST generates a neighborhood of similar words that would score at least T when aligned with the query word using a substitution matrix. For example, with the protein query word “WFY” and using the BLOSUM62 matrix, the neighborhood might include “WFY” itself (score 20), “WYY” (score 17), and “WFF” (score 18), assuming these exceed the threshold T.\nThe algorithm then scans the database for exact matches to any word in these neighborhoods. This is accomplished using a finite state machine or hash table that allows rapid identification of word matches. The word size W and threshold T are critical parameters: larger W and higher T increase speed but reduce sensitivity, as more distant homologs may not contain any words scoring above the threshold.\nExtension Phase: When a word match is found, BLAST attempts to extend the alignment in both directions without gaps. The extension proceeds as long as the cumulative score increases or remains within a certain dropout limit of the maximum score achieved so far. The dropout parameter X determines how far the score can drop before extension is terminated. This ungapped extension phase identifies High-scoring Segment Pairs (HSPs)—ungapped alignments between the query and database sequence.\nEvaluation Phase: HSPs exceeding a score threshold S are reported to the user. The significance of each HSP is evaluated using the Karlin-Altschul statistics, which provide a theoretical framework for calculating E-values (expected values) that indicate how many alignments of similar or better score would be expected by chance in a database of the given size.\n\n\nScoring Local Alignments\nThe scoring of alignments between query and database sequences is fundamental to BLAST’s ability to identify biologically meaningful matches. The scoring system must balance multiple considerations: rewarding matches between similar residues, penalizing mismatches based on their likelihood, and accounting for insertions and deletions. The choice of scoring parameters profoundly affects both the sensitivity and specificity of database searches.\nSubstitution Scoring: At the heart of alignment scoring are substitution matrices that assign numerical values to matches and mismatches. For protein sequences, these matrices reflect the likelihood of amino acid substitutions observed in evolutionarily related sequences. A match between identical residues receives a positive score, while substitutions receive scores that reflect their frequency in homologous sequences—conservative substitutions (e.g., leucine to isoleucine) score higher than radical changes (e.g., tryptophan to glycine).\nThe raw score S of an ungapped alignment is calculated as:\n\\[S = \\sum_{i=1}^{l} s(q_i, d_i)\\]\nwhere l is the alignment length, q_i is the amino acid at position i in the query, d_i is the amino acid at position i in the database sequence, and s(q_i, d_i) is the substitution score from the scoring matrix.\nNucleotide Scoring: For DNA sequences, scoring is typically simpler, with matches receiving positive scores (usually +1 or +2) and mismatches receiving negative scores (usually -3). Some specialized applications use more complex schemes that distinguish transitions (purine to purine or pyrimidine to pyrimidine) from transversions, as transitions occur more frequently in evolution.\nGap Scoring: Modern BLAST implementations use affine gap penalties, where opening a gap costs more than extending it:\n\\[\\text{Gap penalty} = G_o + G_e \\times (L - 1)\\]\nwhere G_o is the gap opening penalty, G_e is the gap extension penalty, and L is the gap length. This scoring scheme reflects the biological reality that a single insertion/deletion event creating a multi-residue gap is more likely than multiple independent single-residue events.\nLog-Odds Scoring: The scores in substitution matrices are log-odds ratios that compare the probability of observing an alignment under two models—homology versus random chance:\n\\[s(i,j) = \\frac{1}{\\lambda} \\ln\\left(\\frac{q_{ij}}{p_i p_j}\\right)\\]\nwhere q_{ij} is the target frequency (probability of amino acids i and j being aligned in homologous sequences), p_i and p_j are the background frequencies of amino acids i and j, and λ is a scaling factor. Positive scores indicate that the pairing is more likely in related sequences than by chance, while negative scores indicate the opposite.\nScore Normalization: Raw scores depend on the length of the alignment and the scoring system used, making direct comparisons difficult. BLAST addresses this by converting raw scores to bit scores, which are normalized and independent of the scoring system:\n\\[S' = \\frac{\\lambda S - \\ln K}{\\ln 2}\\]\nThis normalization allows scores from different searches, different databases, and different scoring matrices to be compared directly. A bit score of 50 means the alignment is 2^50 times more likely under the homology model than the random model.\n\n\nMathematical Foundation: Karlin-Altschul Statistics\nThe statistical theory underlying BLAST, developed by Karlin and Altschul, provides a rigorous framework for assessing the significance of local alignments. This theory shows that the distribution of optimal local alignment scores between random sequences follows an extreme value distribution (EVD), specifically the Gumbel distribution.\nFor ungapped alignments, the number of alignments with score at least S expected by chance in a database search is given by:\n\\[E = Kmn e^{-\\lambda S}\\]\nwhere: - m is the query length - n is the database length - K and λ are statistical parameters that depend on the scoring system and amino acid composition - S is the alignment score\nThe parameter λ is particularly important as it provides the scale for the scoring system. It can be computed from the substitution matrix and amino acid frequencies:\n\\[\\sum_{i,j} p_i p_j e^{\\lambda s_{ij}} = 1\\]\nwhere p_i and p_j are the background frequencies of amino acids i and j, and s_ij is the score for aligning them.\nThe E-value has an intuitive interpretation: it represents the number of alignments with score at least S that would be expected to occur by chance in a database of the size being searched. An E-value of 1 means one match of this quality would be expected by chance alone. E-values much less than 1 (typically &lt; 0.01 or 0.001) suggest that the match is unlikely to have occurred by chance and may represent a true biological relationship.\nThe bit score provides a normalized score that is independent of database size:\n\\[S' = \\frac{\\lambda S - \\ln K}{\\ln 2}\\]\nThis normalization allows scores from different searches to be compared directly, regardless of the scoring system or database size used.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Database Searching</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/database_searching.html#blast-parameters-and-their-effects",
    "href": "chapters/bioinformatics/database_searching.html#blast-parameters-and-their-effects",
    "title": "27  Database Searching",
    "section": "BLAST Parameters and Their Effects",
    "text": "BLAST Parameters and Their Effects\n\nWord Size (W)\nThe word size parameter W has a profound impact on both the speed and sensitivity of BLAST searches. For protein searches (BLASTP), the default word size is 3, meaning the algorithm looks for exact matches of 3-amino acid segments. For nucleotide searches (BLASTN), the default is typically 11, though this can vary with the task.\nIncreasing word size accelerates the search by reducing the number of seeds that need to be extended. With W=3 for proteins, any position in the query can potentially match (20^3 = 8,000 possible words), but with W=4, the number increases to 160,000, making exact matches less likely. However, larger word sizes reduce sensitivity because distantly related sequences may not share any words of that length with sufficient similarity.\nThe choice of word size represents a fundamental trade-off. For closely related sequences (&gt;50% identity), larger word sizes (W=4 or 5 for proteins) can be used without significant loss of sensitivity. For detecting distant homologs, smaller word sizes are essential. Some BLAST variants use multiple word sizes simultaneously or use reduced alphabet representations to maintain sensitivity while increasing effective word size.\n\n\nThreshold Score (T)\nThe neighborhood word score threshold T determines which similar words are included in the search. Lower T values increase sensitivity by allowing more divergent words to serve as seeds, but this comes at a computational cost as more potential matches must be evaluated. The default T value for BLASTP is typically 11-13 when using BLOSUM62 scoring.\nThe interaction between W and T is critical for BLAST’s performance. The two-hit algorithm, introduced in BLAST2, modified this relationship by requiring two non-overlapping word pairs on the same diagonal within a distance A before triggering extension. This allowed the use of lower T values (increasing sensitivity) without proportionally increasing computation time, as single random hits are ignored.\n\n\nSubstitution Matrices\nThe choice of substitution matrix fundamentally affects BLAST’s ability to detect homologs at different evolutionary distances. BLAST commonly uses the BLOSUM (BLOcks SUbstitution Matrix) or PAM (Point Accepted Mutation) series of matrices, each optimized for different levels of sequence divergence.\nBLOSUM62, the default for most BLAST searches, was derived from sequences with approximately 62% identity and performs well for a wide range of evolutionary distances. For closely related sequences, BLOSUM80 or BLOSUM90 may be more appropriate, as they penalize mismatches more heavily. For distant relationships, BLOSUM45 or BLOSUM50 allows more mismatches and may detect remote homologs missed by BLOSUM62.\nThe substitution matrix affects not only which alignments are found but also their statistical significance. The Karlin-Altschul parameters K and λ must be recalculated for each matrix, and some matrices may not satisfy the theoretical requirements for the statistics to be valid (e.g., all scores must have an expected value less than zero for random alignments).\n\n\nGap Penalties\nWhile the original BLAST only provided ungapped alignments, modern versions incorporate gaps with associated opening and extension penalties. The gap opening penalty (typically 10-12 for proteins) is charged for introducing a gap, while the gap extension penalty (typically 1-2) is charged for each additional position in the gap.\nGap penalties must be balanced with the substitution matrix. If gap penalties are too high relative to mismatch scores, the algorithm will force misalignments rather than introduce necessary gaps. If too low, excessive gaps may be introduced, leading to spurious alignments. The optimal gap penalties depend on the evolutionary distance between sequences and the specific biological context.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Database Searching</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/database_searching.html#e-values-and-statistical-significance",
    "href": "chapters/bioinformatics/database_searching.html#e-values-and-statistical-significance",
    "title": "27  Database Searching",
    "section": "E-values and Statistical Significance",
    "text": "E-values and Statistical Significance\n\nUnderstanding E-values\nThe E-value is arguably the most important statistic provided by BLAST, yet it is often misunderstood. The E-value is not a probability but rather an expected count—it tells us how many alignments with a score at least as good as the observed score we would expect to find by chance in a database of the size being searched.\nThe relationship between E-value and database size is linear: searching a database twice as large will double the E-value for any given alignment. This makes intuitive sense—we expect to find twice as many chance matches in a database twice as large. However, this also means that the same alignment can have very different E-values depending on the database searched.\nFor multiple HSPs from the same database sequence, BLAST calculates a combined E-value that considers the probability of observing multiple high-scoring alignments by chance. This is particularly important for multi-domain proteins or sequences with repeated elements.\n\n\nQuery Length Effects\nQuery length affects E-values in a more complex manner than database size. Longer queries have more opportunities to achieve high-scoring alignments by chance, increasing the expected number of random matches. However, the relationship is not strictly linear because the probability of achieving very high scores increases more slowly than query length.\nThe effective length of the query and database, rather than their actual lengths, are used in E-value calculations. Edge effects mean that alignments cannot start at the very end of sequences, so BLAST adjusts the lengths accordingly:\n\\[m' = m - l \\cdot H / \\lambda\\] \\[n' = n - N \\cdot l \\cdot H / \\lambda\\]\nwhere l is the typical alignment length, H is the relative entropy of the scoring system, and N is the number of sequences in the database.\n\n\nSignificance Thresholds\nDetermining appropriate significance thresholds requires consideration of both the biological question and the multiple testing problem. For a single pairwise comparison, an E-value of 0.01 indicates a 1% chance of seeing such a score by chance. However, when searching large databases, multiple testing corrections become important.\nThe Bonferroni correction would suggest dividing the significance threshold by the number of sequences in the database, but this is overly conservative because BLAST E-values already account for database size. Instead, researchers typically use empirical thresholds based on experience: E &lt; 10^-5 for strong matches, E &lt; 10^-3 for probable homologs, and E &lt; 1 for possible relationships worth investigating further.\n\n\nComposition-Based Statistics\nSequence composition can significantly affect the statistics of local alignments. Sequences with biased amino acid composition (e.g., transmembrane proteins rich in hydrophobic residues) can produce artificially high scores when aligned with similarly biased sequences, even in the absence of homology.\nComposition-based statistics adjust for these biases by recalculating the statistical parameters for each sequence pair based on their specific amino acid compositions. This adjustment can dramatically affect E-values, sometimes changing them by several orders of magnitude. While more accurate, composition-based statistics are computationally intensive and may reduce sensitivity for some true homologs.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Database Searching</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/database_searching.html#blast-variants-and-improvements",
    "href": "chapters/bioinformatics/database_searching.html#blast-variants-and-improvements",
    "title": "27  Database Searching",
    "section": "BLAST Variants and Improvements",
    "text": "BLAST Variants and Improvements\n\nBLAST2 and the Two-Hit Algorithm\nBLAST2, introduced in 1997, implemented several crucial improvements that increased both speed and sensitivity. The centerpiece was the two-hit algorithm, which requires two non-overlapping word pairs on the same diagonal within a distance A (typically 40 amino acids) before triggering the computationally expensive extension phase.\nThis seemingly simple modification had profound effects. By requiring two hits, the algorithm could use a lower word score threshold T without increasing computation time, as single random hits are ignored. This increased sensitivity particularly for distant homologs that might not contain any single word with a very high score but do contain multiple moderately scoring words in proximity.\nThe two-hit algorithm also enabled efficient gapped extensions. When two hits trigger an extension, BLAST2 performs a gapped alignment using dynamic programming, but only in the region around the hits. This limited dynamic programming is much faster than full sequence alignment while capturing most biologically relevant alignments.\n\n\nGapped BLAST\nThe introduction of gapped alignments addressed a major limitation of the original BLAST. Many homologous sequences, particularly those separated by long evolutionary distances, contain insertions and deletions that prevent long ungapped alignments. Gapped BLAST uses a seed-and-extend strategy similar to ungapped BLAST but performs dynamic programming in the extension phase.\nThe gapped extension algorithm uses a banded dynamic programming approach, limiting the search to a region around the diagonal defined by the initial seeds. The bandwidth is adjusted dynamically based on the score decay, allowing the algorithm to follow the optimal path while avoiding unnecessary computation. This approach typically achieves speeds within a factor of 3-4 of ungapped BLAST while providing much greater sensitivity.\n\n\nPSI-BLAST: Position-Specific Iterated BLAST\nPSI-BLAST represents one of the most important advances in sequence database searching, enabling detection of remote homologs that share less than 15% sequence identity. The key innovation is the construction of a position-specific scoring matrix (PSSM) that captures patterns of conservation specific to a protein family.\nThe PSI-BLAST algorithm operates iteratively:\n\nInitial Search: Perform a standard BLAST search with the query sequence\nProfile Construction: Align significant hits and construct a PSSM where each position has specific scores for each amino acid based on their frequency in the alignment\nProfile Search: Search the database using the PSSM instead of the original sequence\nIteration: Add newly discovered sequences to the alignment and rebuild the profile\nConvergence: Repeat until no new sequences are found\n\nThe PSSM captures evolutionary information more effectively than generic substitution matrices. Positions that are highly conserved have large positive scores for the conserved residue and negative scores for others, while variable positions have more uniform scores. This position-specific information dramatically increases sensitivity for detecting distant homologs.\nHowever, PSI-BLAST requires careful use to avoid profile corruption. Including false positives in the profile can lead to profile drift, where the search progressively moves away from the original query family. Setting appropriate inclusion thresholds (typically E &lt; 0.001-0.005) and manually inspecting results between iterations helps prevent this problem.\n\n\nPHI-BLAST: Pattern-Hit Initiated BLAST\nPHI-BLAST combines pattern matching with local alignment to find sequences containing a specific motif or pattern. The user provides both a query sequence and a pattern (using PROSITE syntax or regular expressions), and PHI-BLAST only reports alignments that include the pattern.\nThis approach is particularly useful when searching for specific functional motifs or domains. By requiring the presence of a pattern, PHI-BLAST can eliminate many false positives and focus on functionally relevant matches. The pattern serves as an anchor for the alignment, ensuring that functionally important regions are properly aligned even if surrounding regions have diverged substantially.\n\n\nDELTA-BLAST: Domain Enhanced Lookup Time Accelerated BLAST\nDELTA-BLAST improves upon PSI-BLAST by using a database of pre-constructed PSSMs from conserved domains. Instead of building a profile from scratch, DELTA-BLAST first searches a database of domain profiles (like CDD - Conserved Domain Database) to find relevant PSSMs, then uses these for searching.\nThis approach provides several advantages: - Faster convergence than PSI-BLAST as it starts with information-rich profiles - Reduced risk of profile corruption from false positives - Better detection of remote homologs that share only domain-level similarity - More consistent results across different searches",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Database Searching</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/database_searching.html#specialized-blast-programs",
    "href": "chapters/bioinformatics/database_searching.html#specialized-blast-programs",
    "title": "27  Database Searching",
    "section": "Specialized BLAST Programs",
    "text": "Specialized BLAST Programs\n\nBLASTN: Nucleotide-Nucleotide Searches\nBLASTN faces unique challenges compared to protein searches. The smaller alphabet size (4 bases vs. 20 amino acids) means that random matches are more common, requiring longer word sizes and different statistical treatments. BLASTN typically uses word sizes of 11-28 nucleotides and employs a simpler scoring scheme (+1 for matches, -3 for mismatches by default).\nRecent versions include task-specific optimizations: - Megablast: Optimized for highly similar sequences (&gt;95% identity) using word sizes of 28 or larger - Discontiguous Megablast: Uses non-contiguous word templates for increased sensitivity while maintaining speed - BLASTN-short: Optimized for short queries (&lt;30 nucleotides) with adjusted parameters\n\n\nBLASTX and TBLASTN: Translated Searches\nBLASTX translates a nucleotide query in all six reading frames and searches a protein database, while TBLASTN searches a translated nucleotide database with a protein query. These programs are essential for: - Identifying protein-coding regions in genomic sequences - Finding homologs across species with different codon usage - Detecting frameshift errors in sequences\nThe translation step adds computational overhead but provides much greater sensitivity than nucleotide-nucleotide comparisons for protein-coding sequences. The statistical framework must account for the effective increase in search space due to multiple reading frames.\n\n\nTBLASTX: Six-Frame Translation\nTBLASTX translates both query and database in all reading frames, performing protein-level comparisons of nucleotide sequences. While computationally intensive (36 times more comparisons than standard BLAST), TBLASTX can detect very distant coding sequence relationships and is particularly useful for: - Comparing EST databases - Finding novel genes in genomic sequences - Detecting pseudogenes and processed genes",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Database Searching</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/database_searching.html#advanced-applications-and-considerations",
    "href": "chapters/bioinformatics/database_searching.html#advanced-applications-and-considerations",
    "title": "27  Database Searching",
    "section": "Advanced Applications and Considerations",
    "text": "Advanced Applications and Considerations\n\nDatabase Contamination and Quality\nDatabase contamination poses a significant challenge for BLAST searches. Vector sequences, adapters, contaminants from other organisms, and low-quality regions can all lead to spurious matches. BLAST databases often include filters for common contaminants, but researchers must remain vigilant.\nLow-complexity regions (e.g., poly-A tails, simple repeats, coiled-coils) can produce statistically significant but biologically meaningless matches. BLAST incorporates filters like SEG for proteins and DUST for nucleotides that mask low-complexity regions before searching. While these filters reduce false positives, they can occasionally mask biologically important regions, so results should be interpreted accordingly.\n\n\nTaxonomic Restrictions and Organism-Specific Searches\nBLAST allows taxonomic restrictions to focus searches on specific organisms or groups. This is valuable for: - Finding orthologs in specific lineages - Avoiding contamination from unrelated organisms - Improving search speed by reducing database size - Identifying lineage-specific genes or gene families\nThe Entrez query system allows complex taxonomic queries, combining organism names, taxonomic IDs, and Boolean operators to precisely define the search space.\n\n\nBatch Searches and Automation\nLarge-scale projects often require searching thousands or millions of sequences. BLAST+ command-line tools support batch processing with customizable output formats suitable for downstream analysis. Key considerations for batch searches include: - Managing computational resources and parallelization - Handling temporary files and disk I/O - Parsing and storing results efficiently - Implementing quality control and error handling - Avoiding redundant searches through careful database design\n\n\nCloud and Web Services\nCloud-based BLAST services provide scalable infrastructure for large-scale searches. The NCBI BLAST web service handles millions of searches daily, while cloud platforms like AWS and Google Cloud offer BLAST as part of their bioinformatics toolkits. These services provide: - Automatic updates of databases - Scalable compute resources - RESTful APIs for programmatic access - Integration with other bioinformatics tools - Cost-effective solutions for occasional users",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Database Searching</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/database_searching.html#limitations-and-alternatives",
    "href": "chapters/bioinformatics/database_searching.html#limitations-and-alternatives",
    "title": "27  Database Searching",
    "section": "Limitations and Alternatives",
    "text": "Limitations and Alternatives\n\nLimitations of BLAST\nDespite its success, BLAST has inherent limitations that researchers must understand:\nSensitivity Limits: BLAST may miss very distant homologs, particularly those in the “twilight zone” below 20% sequence identity. The heuristic nature of the algorithm means it doesn’t guarantee finding all significant matches.\nLocal Alignment Focus: BLAST excels at finding local similarities but may miss relationships that require global alignment or complex rearrangements.\nSpeed vs. Sensitivity Trade-off: Parameter choices that increase sensitivity reduce speed, making some searches impractical for very large databases.\nStatistical Assumptions: The Karlin-Altschul statistics assume that sequences are random and that letter compositions match the background frequencies. Violations of these assumptions can lead to incorrect E-values.\n\n\nAlternative Approaches\nSeveral alternatives to BLAST address specific limitations:\nHMMER: Uses profile Hidden Markov Models (HMMs) for more sensitive detection of remote homologs. While historically slower than BLAST, HMMER3 achieves comparable speeds through algorithmic improvements.\nDIAMOND: Achieves speeds 500-20,000 times faster than BLAST for protein searches through double indexing and reduced alphabet representations, with only minor sensitivity loss.\nMMseqs2: Uses k-mer matching and vectorization for ultra-fast searches, particularly effective for clustering large sequence sets.\nLAST: Incorporates adaptive seeds and probabilistic alignments, providing better handling of repeats and more accurate E-values for certain types of sequences.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Database Searching</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/database_searching.html#best-practices-and-interpretation",
    "href": "chapters/bioinformatics/database_searching.html#best-practices-and-interpretation",
    "title": "27  Database Searching",
    "section": "Best Practices and Interpretation",
    "text": "Best Practices and Interpretation\n\nChoosing Appropriate Parameters\nSelecting optimal BLAST parameters requires understanding both the biological question and the nature of the sequences being searched:\nFor finding close homologs (&gt;70% identity): - Use default or stringent parameters - Consider Megablast for nucleotide searches - Higher word sizes acceptable (W=4-5 for proteins) - Standard substitution matrices (BLOSUM62)\nFor distant homolog detection (&lt;40% identity): - Use PSI-BLAST or DELTA-BLAST - Reduce word size (W=2 for proteins) - Lower threshold T values - Consider BLOSUM45 or BLOSUM50 matrices - Multiple iterations with careful threshold selection\nFor short sequences (&lt;30 residues): - Adjust E-value thresholds (shorter sequences have lower maximum scores) - Use BLASTN-short for nucleotides - Consider composition-based statistics - Reduce word size or use ungapped alignments\n\n\nInterpreting Results\nProper interpretation of BLAST results requires considering multiple factors beyond E-values:\nBiological Context: A mediocre E-value match to a protein from a related organism may be more significant than an excellent match to a protein from a distant organism if horizontal gene transfer is unlikely.\nAlignment Quality: Examine the actual alignment, not just scores. Look for: - Conservation of functionally important residues - Absence of gaps in secondary structure elements - Consistent domain architecture - Reasonable length coverage\nMultiple Evidence: Combine BLAST results with other evidence: - Domain predictions (InterPro, Pfam) - Structure predictions or comparisons - Synteny or genomic context - Expression patterns - Functional annotations\nReciprocal Searches: For ortholog identification, perform reciprocal BLAST searches. True orthologs should be mutual best hits between genomes.\n\n\nCommon Pitfalls\nSeveral common mistakes can lead to incorrect interpretations:\nOver-reliance on E-values: E-values are statistical measures, not biological proof. Low E-values can occur for non-homologous sequences with compositional bias.\nIgnoring Database Growth: As databases grow, E-values for the same alignment increase. Historical comparisons must account for database size changes.\nProfile Corruption in PSI-BLAST: Including false positives in PSI-BLAST iterations can lead to profile drift. Always inspect intermediate results and use conservative inclusion thresholds.\nMisunderstanding Coverage: High-scoring alignments covering only small portions of sequences may not indicate overall homology. Consider both local and global similarity.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Database Searching</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/database_searching.html#future-directions",
    "href": "chapters/bioinformatics/database_searching.html#future-directions",
    "title": "27  Database Searching",
    "section": "Future Directions",
    "text": "Future Directions\n\nIntegration with Structure Prediction\nThe revolution in protein structure prediction, exemplified by AlphaFold, is beginning to influence sequence searching strategies. Structure-aware sequence searching can: - Use predicted structures to identify remote homologs missed by sequence methods - Improve alignment accuracy in structurally conserved regions - Enable functional annotation based on structural similarity - Guide the design of more sensitive sequence search strategies\n\n\nMachine Learning Approaches\nDeep learning methods are being applied to sequence similarity detection: - Learned embeddings that capture evolutionary relationships - Neural networks that predict homology beyond traditional sequence similarity - Attention mechanisms that identify functionally important regions - Integration of multiple data types (sequence, structure, function) in unified models\nThese approaches show promise for detecting extremely remote homologies and understanding the full spectrum of sequence relationships.\n\n\nScaling to Massive Databases\nAs sequencing costs continue to plummet, sequence databases are growing exponentially. Future developments must address: - Efficient indexing strategies for databases with trillions of sequences - Distributed and parallel computing approaches - Compression techniques that allow searching without full decompression - Hierarchical search strategies that quickly eliminate irrelevant portions of the database - Real-time updating of search indices as new sequences are added\n\n\nMetagenomics and Environmental Sequencing\nThe explosion of metagenomic and environmental sequencing data presents unique challenges: - Searching against millions of incomplete and fragmented sequences - Dealing with sequences from unknown or poorly characterized organisms - Identifying novel gene families with no known homologs - Handling the enormous scale of environmental sequence databases - Developing statistical frameworks for searches against highly redundant databases",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Database Searching</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/database_searching.html#conclusion",
    "href": "chapters/bioinformatics/database_searching.html#conclusion",
    "title": "27  Database Searching",
    "section": "Conclusion",
    "text": "Conclusion\nBLAST and sequence database searching have fundamentally transformed biological research, enabling researchers to leverage the collective knowledge encoded in sequence databases. From its inception as a heuristic algorithm trading perfect accuracy for practical speed, BLAST has evolved into a sophisticated family of tools capable of detecting subtle evolutionary relationships across the tree of life.\nThe success of BLAST stems from its elegant balance of speed, sensitivity, and statistical rigor. The algorithm’s heuristic approach, based on the biological insight that related sequences share regions of local similarity, enables searches that would be computationally infeasible with exact methods. The robust statistical framework provides researchers with confidence in distinguishing biological relationships from chance similarities.\nUnderstanding BLAST’s parameters, statistics, and limitations is essential for effective use. The choice of word size, scoring matrices, and threshold values can dramatically affect search results. E-values must be interpreted in context, considering database size, query length, and sequence composition. The various BLAST programs and variants each have specific strengths and optimal use cases.\nAs we look to the future, sequence database searching faces new challenges and opportunities. The continued exponential growth of sequence databases demands ever more efficient algorithms. The integration of structural and functional information promises to reveal relationships invisible to sequence-based methods alone. Machine learning approaches may capture complex patterns of homology that escape current methods.\nYet despite these advances, the fundamental principle underlying BLAST remains unchanged: homologous sequences, shaped by evolution, retain detectable similarities that reveal their relationships. By enabling rapid identification of these relationships, BLAST continues to be an indispensable tool for understanding the molecular basis of life. Whether identifying the function of a newly discovered gene, tracing the evolution of a protein family, or discovering novel biological mechanisms, sequence database searching remains at the heart of modern biological discovery.\nThe story of BLAST is far from complete. As new sequencing technologies generate unprecedented amounts of data and new computational approaches emerge, the tools and techniques for sequence database searching will continue to evolve. However, the fundamental need—to find meaningful relationships in the vast sea of biological sequences—will remain central to biological research for generations to come.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Database Searching</span>"
    ]
  },
  {
    "objectID": "chapters/project/translation_project/index.html",
    "href": "chapters/project/translation_project/index.html",
    "title": "28  Translating ORFs",
    "section": "",
    "text": "Project files\nThis chapter is about translating DNA into protein. If bacteria can do it, so can you.\nIn this project you will write the code needed to translate an open reading frame (ORF) on a DNA sequence into into the corresponding sequence of amino acids.\nBegin by downloading the files you need for the project:\nhttps://munch-group.org/bioinformatics/supplementary/project_files\nThe file translationproject.py is for your code. The file test_translationproject.py is the script you use to test the functions you write for this project (see Chapter 18).\nIn this project you will need a data structure that pairs each codon to the amino acid it encodes. This is an obvious use of a dictionary and at the top of translationproject.py I have defined such a dictionary you can use. Defining it outside the functions means that it is visible inside all your functions (unless you define another variable called codon_map inside a function). Defining variables globally to your program sometimes make sense if some value can be considered a constant in your program and is never changed.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Translating ORFs</span>"
    ]
  },
  {
    "objectID": "chapters/project/translation_project/index.html#project-files",
    "href": "chapters/project/translation_project/index.html#project-files",
    "title": "28  Translating ORFs",
    "section": "",
    "text": "It is normally very bad programming style to access variables outside functions in this way because it may have all kinds of unexpected side effects across function calls. So make it a rule for yourself that code inside a function should never to access variables outside the function. The reason we define codon_map globally in this project is to help you understand that when Python cannot find a variable inside a function, it looks outside the function to find it. In this project functions will find codon_map in this way. However, as I already said, you should never do this yourself. The chance that you make an unexpected mistake is overwhelming.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Translating ORFs</span>"
    ]
  },
  {
    "objectID": "chapters/project/translation_project/index.html#translating-a-single-codon",
    "href": "chapters/project/translation_project/index.html#translating-a-single-codon",
    "title": "28  Translating ORFs",
    "section": "Translating a single codon",
    "text": "Translating a single codon\nWrite a function, translate_codon that takes one argument:\n\nA string, which is a codon.\n\nThe function should return:\n\nA string of length one (one character). If the string argument is a valid codon then this letter should the be an amino acid letter specified by the codon_map dictionary. Note that stop codons are represented by a star ('*'). If the string argument is not a valid codon, the function must return '?'.\n\nExample usage:\ntranslate_codon('ACG')\nshould return\n'T'\nBefore you start coding you should always outline for yourself intuitively what you need to do to complete the task at hand. In this case want to translate, or map, between a three letter string, codon, and the corresponding one letter string for the amino acid that the codon corresponds to. Notice that the keys in the codon_map dictionary are in upper case, so you must make sure that the keys you use are also in upper case. You can translate codon into an upper case version of itself using the upper() method.\nTry this out first:\ncodon = 'TTG'\namino_acid = codon_map[codon]\nprint(amino_acid)\nNow write the function so it uses the string parameter as a key to look up the corresponding amino acid letter and returns this letter. Before you go on, make a function that does only that.\nBefore you are completely done you need to make your function handle the situation when the argument to the function is not a key in the codon_map dictionary. Use an if-else construct to handle the two cases. The boolean expression must test if the function argument is a key in codon_map. Remember that you can use the in operator to do this.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Translating ORFs</span>"
    ]
  },
  {
    "objectID": "chapters/project/translation_project/index.html#splitting-an-open-reading-frame-into-codons",
    "href": "chapters/project/translation_project/index.html#splitting-an-open-reading-frame-into-codons",
    "title": "28  Translating ORFs",
    "section": "Splitting an open reading frame into codons",
    "text": "Splitting an open reading frame into codons\nTo translate an entire open reading frame into the corresponding amino acid sequence, you need to split the ORF sequence into codons. When we have done that we can translate each codon using the function translate_codon you just wrote.\nWrite a function, split_codons, that takes one argument:\n\nA string, which is an ORF sequence\n\nThe function must return:\n\nA list of strings. Each string must have length 3 and must represent the-non overlapping triplets in the same sequence as they appear in the string given as argument.\n\nExample usage:\nsplit_codons('ATGTATGCCTGA')\nshould return\n['ATG', 'TAT', 'GCC', 'TGA']\nDivide the problem into simpler tasks like above. You need to loop over the sequence to perform operations on it. Start by writing a function that prints each character:\ndef split_codons(orf):\n    for i in range(len(orf)):\n        print(orf[i])\nNow try to figure out how you can modify the function to make it move over the sequence in jumps of three. Look at the documentation for the range function to see how you can make it iterate over numbers with increments of three like this: 0, 3, 6, 9, 12, … . Modify your function so that it now prints every third character.\nWhat you want is obviously not every third character. You want three characters. I.e. every third character and the two characters that come right after. You can use the index in the for loop to get the corresponding codon using slicing. Modify your function so that it prints each codon.\nNow all that remains is to put each codon on a list that you can return from the function. You can define a list before your for-loop so you have a list to add codons to.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Translating ORFs</span>"
    ]
  },
  {
    "objectID": "chapters/project/translation_project/index.html#translating-an-open-reading-frame",
    "href": "chapters/project/translation_project/index.html#translating-an-open-reading-frame",
    "title": "28  Translating ORFs",
    "section": "Translating an open reading frame",
    "text": "Translating an open reading frame\nNow you can use the two functions split_codons and translate_codon to write a function that translates an ORF into a protein sequence.\nWrite a function, translate_orf, that takes one argument:\n\nA string, which is a DNA sequence.\n\nThe function must return\n\nA string, which is the protein sequence translated from the ORF sequence argument.\n\nExample usage:\ntranslate_orf('ATGGAGCTTANCAAATAG')\nshould return\n'MEL?K*'",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Translating ORFs</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/hidden_markov_models.html",
    "href": "chapters/bioinformatics/hidden_markov_models.html",
    "title": "29  Hidden Markov Models",
    "section": "",
    "text": "30 Introduction: Why Do We Need Hidden Markov Models in Biology?\nWhen you look at a DNA sequence like ATGCGTAACGTTTAAGGCCTAG, what do you see? At first glance, it appears to be a random string of four letters. However, this sequence contains a wealth of biological information hidden beneath its surface. It might encode a protein-coding gene, contain regulatory elements that control gene expression, or include structural features that affect how the DNA is packaged in the cell. The challenge facing computational biologists is how to automatically detect and annotate these functional elements from raw sequence data.\nConsider the human genome, which contains over 3 billion base pairs of DNA. Within this vast sequence lie approximately 20,000 protein-coding genes, hundreds of thousands of regulatory elements, and numerous other functional features. Manual annotation of such sequences is impossible, yet accurate identification of these elements is crucial for understanding how genes are regulated, how proteins are made, and how genetic variations contribute to disease.\nHidden Markov Models provide an elegant computational framework for solving these biological annotation problems. They excel at identifying patterns in sequences where the underlying biological function is not directly observable but must be inferred from the sequence content. For instance, when we see the sequence ATGAAATTTGCCTAG, we cannot directly observe whether each nucleotide is part of an exon, intron, or intergenic region. However, we can use the statistical properties of the sequence—such as codon usage patterns, presence of start and stop codons, and splice site signals—to infer the most likely functional annotation.\ngraph LR\n    A[DNA Sequence&lt;br/&gt;ATGAAATTTGCCTAG] --&gt; B[Hidden States&lt;br/&gt;Start-Coding-Coding-Stop]\n    B --&gt; C[Functional Annotation&lt;br/&gt;Gene Structure]\n    \n    style A fill:#e1f5fe\n    style B fill:#fff3e0\n    style C fill:#e8f5e8\n\n\n\n\ngraph LR\n    A[DNA Sequence&lt;br/&gt;ATGAAATTTGCCTAG] --&gt; B[Hidden States&lt;br/&gt;Start-Coding-Coding-Stop]\n    B --&gt; C[Functional Annotation&lt;br/&gt;Gene Structure]\n    \n    style A fill:#e1f5fe\n    style B fill:#fff3e0\n    style C fill:#e8f5e8\nThe power of HMMs lies in their ability to model sequential dependencies while handling uncertainty. Biological sequences exhibit complex patterns where the function of one region influences the likely function of neighboring regions. For example, in eukaryotic genes, exons are typically followed by introns, which are then followed by more exons. Promoter regions contain specific sequence motifs that tend to cluster together upstream of transcription start sites. CpG islands, regions with unusually high frequencies of cytosine-guanine dinucleotides, often mark the beginning of genes and play important roles in gene regulation.\nIn protein analysis, HMMs help us understand the relationship between amino acid sequence and protein structure. The sequence MKLILLFAIVSLVF might not immediately reveal its function, but an HMM trained on transmembrane proteins would recognize this as a likely signal peptide based on its hydrophobic character and typical length. Similarly, the alternating pattern of hydrophobic and hydrophilic regions in a protein sequence can reveal the presence of membrane-spanning α-helices.\nHMMs address three fundamental questions that arise repeatedly in computational biology:\nThese questions correspond to different biological applications: finding the most likely annotation of a sequence, evaluating how well a sequence fits a particular model, and determining the uncertainty in our predictions.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hidden Markov Models</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/hidden_markov_models.html#gene-finding-in-eukaryotic-genomes",
    "href": "chapters/bioinformatics/hidden_markov_models.html#gene-finding-in-eukaryotic-genomes",
    "title": "29  Hidden Markov Models",
    "section": "Gene Finding in Eukaryotic Genomes",
    "text": "Gene Finding in Eukaryotic Genomes\nGene finding in eukaryotic genomes represents one of the most sophisticated applications of HMMs in computational biology. Unlike bacterial genes, which consist of simple continuous coding sequences, eukaryotic genes have complex structures with multiple exons separated by introns, alternative splicing patterns, and regulatory elements.\n\nstateDiagram-v2\n    [*] --&gt; Intergenic\n    Intergenic --&gt; FiveUTR\n    FiveUTR --&gt; FirstExon\n    FirstExon --&gt; Intron\n    Intron --&gt; InternalExon\n    InternalExon --&gt; Intron\n    InternalExon --&gt; LastExon\n    LastExon --&gt; ThreeUTR\n    ThreeUTR --&gt; Intergenic\n    Intergenic --&gt; [*]\n    \n    note right of FirstExon\n        Contains start codon\n        Maintains reading frame\n        Codon usage bias\n    end note\n    \n    note right of Intron\n        GT...AG splice sites\n        Branch point sequence\n        Variable length\n    end note\n    \n    note right of InternalExon\n        Maintains reading frame\n        No start/stop codons\n        Triplet periodicity\n    end note\n\n\n\n\nstateDiagram-v2\n    [*] --&gt; Intergenic\n    Intergenic --&gt; FiveUTR\n    FiveUTR --&gt; FirstExon\n    FirstExon --&gt; Intron\n    Intron --&gt; InternalExon\n    InternalExon --&gt; Intron\n    InternalExon --&gt; LastExon\n    LastExon --&gt; ThreeUTR\n    ThreeUTR --&gt; Intergenic\n    Intergenic --&gt; [*]\n    \n    note right of FirstExon\n        Contains start codon\n        Maintains reading frame\n        Codon usage bias\n    end note\n    \n    note right of Intron\n        GT...AG splice sites\n        Branch point sequence\n        Variable length\n    end note\n    \n    note right of InternalExon\n        Maintains reading frame\n        No start/stop codons\n        Triplet periodicity\n    end note\n\n\n\n\n\n\nA comprehensive gene finding HMM must model all these features while maintaining computational efficiency. A basic eukaryotic gene finding model includes states representing different functional regions: 5’ untranslated regions (5’ UTR), first exons, introns, internal exons, last exons, and 3’ untranslated regions (3’ UTR). The emission probabilities reflect the different sequence characteristics of these regions. Exon states avoid in-frame stop codons and exhibit the triplet periodicity characteristic of protein-coding sequences, while intron states have more uniform nucleotide composition and may contain repetitive elements.\nThe transition probabilities encode biological knowledge about gene structure. The probability of transitioning from “first exon” to “intron” is high near GT dinucleotides (donor splice sites), while the probability of transitioning from “intron” to “internal exon” is high near AG dinucleotides (acceptor splice sites). These transition probabilities can be made position-specific to model the sequence conservation around splice sites more accurately.\nFor example, the donor splice site model might include a series of states \\(D_1, D_2, \\ldots, D_9\\) that model the conserved sequence around the splice site:\n\n\\(e_{D_1}(\\text{G}) \\approx 0.95\\), modeling the highly conserved G in the GT dinucleotide\n\\(e_{D_2}(\\text{T}) \\approx 0.95\\), modeling the highly conserved T in the GT dinucleotide\n\n\\(e_{D_3}\\) through \\(e_{D_9}\\) model the less conserved but still biased positions\n\nThis approach allows the HMM to recognize splice sites based on both the canonical GT-AG signals and the more subtle sequence preferences in the surrounding regions.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hidden Markov Models</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/hidden_markov_models.html#cpg-island-detection",
    "href": "chapters/bioinformatics/hidden_markov_models.html#cpg-island-detection",
    "title": "29  Hidden Markov Models",
    "section": "CpG Island Detection",
    "text": "CpG Island Detection\nCpG island detection provides another excellent example of sophisticated HMM design. CpG islands are regions of genomic DNA with elevated frequencies of cytosine-guanine dinucleotides that often mark gene promoters and regulatory regions. The challenge is that CpG islands are not uniformly CpG-rich but contain regions of varying CpG density, and their boundaries are often gradual rather than sharp.\n\nstateDiagram-v2\n    [*] --&gt; Background\n    Background --&gt; LowCpG\n    LowCpG --&gt; MediumCpG\n    MediumCpG --&gt; HighCpG\n    HighCpG --&gt; MediumCpG\n    MediumCpG --&gt; LowCpG\n    LowCpG --&gt; Background\n    Background --&gt; [*]\n    \n    note right of Background\n        P(CG) = 0.01\n        Typical genomic frequency\n    end note\n    \n    note right of LowCpG\n        P(CG) = 0.04\n        CpG island periphery\n    end note\n    \n    note right of MediumCpG\n        P(CG) = 0.08\n        Intermediate regions\n    end note\n    \n    note right of HighCpG\n        P(CG) = 0.15\n        CpG island core\n    end note\n\n\n\n\nstateDiagram-v2\n    [*] --&gt; Background\n    Background --&gt; LowCpG\n    LowCpG --&gt; MediumCpG\n    MediumCpG --&gt; HighCpG\n    HighCpG --&gt; MediumCpG\n    MediumCpG --&gt; LowCpG\n    LowCpG --&gt; Background\n    Background --&gt; [*]\n    \n    note right of Background\n        P(CG) = 0.01\n        Typical genomic frequency\n    end note\n    \n    note right of LowCpG\n        P(CG) = 0.04\n        CpG island periphery\n    end note\n    \n    note right of MediumCpG\n        P(CG) = 0.08\n        Intermediate regions\n    end note\n    \n    note right of HighCpG\n        P(CG) = 0.15\n        CpG island core\n    end note\n\n\n\n\n\n\nA sophisticated model might include multiple CpG island states representing different levels of CpG enrichment:\n\nHigh CpG state: \\(P(\\text{CG}) = 0.15\\), \\(P(\\text{other dinucleotides}) = 0.85/15\\)\nMedium CpG state: \\(P(\\text{CG}) = 0.08\\), \\(P(\\text{other dinucleotides}) = 0.92/15\\)\n\nLow CpG state: \\(P(\\text{CG}) = 0.04\\), \\(P(\\text{other dinucleotides}) = 0.96/15\\)\nBackground state: \\(P(\\text{CG}) = 0.01\\), \\(P(\\text{other dinucleotides}) = 0.99/15\\)\n\nThe transition structure allows gradual transitions between these states, modeling the fact that CpG islands often have cores of high CpG density surrounded by regions of intermediate CpG density.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hidden Markov Models</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/hidden_markov_models.html#transmembrane-protein-topology-prediction",
    "href": "chapters/bioinformatics/hidden_markov_models.html#transmembrane-protein-topology-prediction",
    "title": "29  Hidden Markov Models",
    "section": "Transmembrane Protein Topology Prediction",
    "text": "Transmembrane Protein Topology Prediction\nTransmembrane protein topology prediction illustrates how biophysical knowledge can be incorporated into HMM design. Transmembrane proteins contain hydrophobic α-helical segments that span cell membranes, separated by hydrophilic loops in the aqueous environment.\n\nstateDiagram-v2\n    [*] --&gt; OutsideLoop\n    OutsideLoop --&gt; TMHelix\n    TMHelix --&gt; InsideLoop\n    InsideLoop --&gt; TMHelix\n    TMHelix --&gt; OutsideLoop\n    OutsideLoop --&gt; [*]\n    InsideLoop --&gt; [*]\n    \n    note right of OutsideLoop\n        Hydrophilic residues\n        P(K,R,D,E) high\n        Negative charge bias\n    end note\n    \n    note right of TMHelix\n        Hydrophobic residues\n        P(L,I,V,F,A) high\n        Length: 20-25 residues\n    end note\n    \n    note right of InsideLoop\n        Positive-inside rule\n        P(K,R) &gt; P(D,E)\n        Cytoplasmic side\n    end note\n\n\n\n\nstateDiagram-v2\n    [*] --&gt; OutsideLoop\n    OutsideLoop --&gt; TMHelix\n    TMHelix --&gt; InsideLoop\n    InsideLoop --&gt; TMHelix\n    TMHelix --&gt; OutsideLoop\n    OutsideLoop --&gt; [*]\n    InsideLoop --&gt; [*]\n    \n    note right of OutsideLoop\n        Hydrophilic residues\n        P(K,R,D,E) high\n        Negative charge bias\n    end note\n    \n    note right of TMHelix\n        Hydrophobic residues\n        P(L,I,V,F,A) high\n        Length: 20-25 residues\n    end note\n    \n    note right of InsideLoop\n        Positive-inside rule\n        P(K,R) &gt; P(D,E)\n        Cytoplasmic side\n    end note\n\n\n\n\n\n\nThe design of an effective transmembrane protein HMM requires modeling several biological constraints. First, the hydrophobic nature of membrane-spanning regions must be captured through appropriate emission probabilities. Transmembrane helix states have high emission probabilities for hydrophobic amino acids (L, I, V, F, A) and low probabilities for charged amino acids (K, R, D, E).\nSecond, the length constraints of transmembrane helices must be modeled. Membrane-spanning α-helices typically contain 20-25 amino acids, corresponding to the thickness of the lipid bilayer. This can be modeled using a series of transmembrane states or explicit length distributions.\nThird, the “positive-inside rule” must be incorporated. This rule states that the cytoplasmic side of transmembrane proteins contains more positively charged amino acids than the extracellular side. This can be modeled by having different inside and outside loop states with different emission probabilities for charged amino acids:\n\nInside loop: \\(P(\\text{K}) = 0.08\\), \\(P(\\text{R}) = 0.06\\), \\(P(\\text{D}) = 0.04\\), \\(P(\\text{E}) = 0.05\\)\nOutside loop: \\(P(\\text{K}) = 0.04\\), \\(P(\\text{R}) = 0.03\\), \\(P(\\text{D}) = 0.08\\), \\(P(\\text{E}) = 0.09\\)",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hidden Markov Models</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/hidden_markov_models.html#signal-peptide-prediction",
    "href": "chapters/bioinformatics/hidden_markov_models.html#signal-peptide-prediction",
    "title": "29  Hidden Markov Models",
    "section": "Signal Peptide Prediction",
    "text": "Signal Peptide Prediction\nSignal peptide prediction represents another important application where biological knowledge guides HMM design. Signal peptides are short amino acid sequences that direct newly synthesized proteins to specific cellular locations. They typically consist of three regions: a positively charged N-terminal region, a hydrophobic core region, and a polar C-terminal region containing the cleavage site.\n\nstateDiagram-v2\n    [*] --&gt; NTerminal\n    NTerminal --&gt; HydrophobicCore\n    HydrophobicCore --&gt; CTerminal\n    CTerminal --&gt; CleavageSite\n    CleavageSite --&gt; [*]\n    \n    note right of NTerminal\n        Positively charged\n        P(K,R) high\n        Length: 1-5 residues\n    end note\n    \n    note right of HydrophobicCore\n        Hydrophobic residues\n        P(L,I,V,F,A) high\n        Length: 7-15 residues\n    end note\n    \n    note right of CTerminal\n        Polar residues\n        Small, uncharged\n        Length: 3-7 residues\n    end note\n    \n    note right of CleavageSite\n        Specific cleavage rules\n        Small residues at -1,-3\n    end note\n\n\n\n\nstateDiagram-v2\n    [*] --&gt; NTerminal\n    NTerminal --&gt; HydrophobicCore\n    HydrophobicCore --&gt; CTerminal\n    CTerminal --&gt; CleavageSite\n    CleavageSite --&gt; [*]\n    \n    note right of NTerminal\n        Positively charged\n        P(K,R) high\n        Length: 1-5 residues\n    end note\n    \n    note right of HydrophobicCore\n        Hydrophobic residues\n        P(L,I,V,F,A) high\n        Length: 7-15 residues\n    end note\n    \n    note right of CTerminal\n        Polar residues\n        Small, uncharged\n        Length: 3-7 residues\n    end note\n    \n    note right of CleavageSite\n        Specific cleavage rules\n        Small residues at -1,-3\n    end note\n\n\n\n\n\n\nAn HMM for signal peptide prediction might include states for each of these regions with appropriate emission probabilities and transition structures that model the sequential organization and typical length ranges of each region.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hidden Markov Models</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/hidden_markov_models.html#profile-hmms-for-protein-family-analysis",
    "href": "chapters/bioinformatics/hidden_markov_models.html#profile-hmms-for-protein-family-analysis",
    "title": "29  Hidden Markov Models",
    "section": "Profile HMMs for Protein Family Analysis",
    "text": "Profile HMMs for Protein Family Analysis\nProfile HMMs represent one of the most successful extensions of the basic HMM framework. They adapt the basic structure to model multiple sequence alignments, enabling sensitive detection of remote homology and accurate classification of protein sequences into functional families.\n\ngraph TD\n    subgraph \"Profile HMM Structure\"\n        M1[Match₁] --&gt; M2[Match₂]\n        M1 --&gt; I1[Insert₁]\n        M1 --&gt; D2[Delete₂]\n        I1 --&gt; M2\n        I1 --&gt; I1\n        I1 --&gt; D2\n        D2 --&gt; M2\n        D2 --&gt; D2\n        \n        M2 --&gt; M3[Match₃]\n        M2 --&gt; I2[Insert₂]\n        M2 --&gt; D3[Delete₃]\n    end\n    \n    note right of M1\n        Emit according to\n        alignment column\n        conservation\n    end note\n    \n    note right of I1\n        Emit according to\n        background\n        frequencies\n    end note\n    \n    note right of D2\n        Silent state\n        No emission\n    end note\n    \n    style M1 fill:#e8f5e8\n    style I1 fill:#fff3e0\n    style D2 fill:#ffebee\n\n\n\n\ngraph TD\n    subgraph \"Profile HMM Structure\"\n        M1[Match₁] --&gt; M2[Match₂]\n        M1 --&gt; I1[Insert₁]\n        M1 --&gt; D2[Delete₂]\n        I1 --&gt; M2\n        I1 --&gt; I1\n        I1 --&gt; D2\n        D2 --&gt; M2\n        D2 --&gt; D2\n        \n        M2 --&gt; M3[Match₃]\n        M2 --&gt; I2[Insert₂]\n        M2 --&gt; D3[Delete₃]\n    end\n    \n    note right of M1\n        Emit according to\n        alignment column\n        conservation\n    end note\n    \n    note right of I1\n        Emit according to\n        background\n        frequencies\n    end note\n    \n    note right of D2\n        Silent state\n        No emission\n    end note\n    \n    style M1 fill:#e8f5e8\n    style I1 fill:#fff3e0\n    style D2 fill:#ffebee\n\n\n\n\n\n\nIn a profile HMM, we have three types of states at each alignment column that model different evolutionary events. Match states (M) emit amino acids according to the conservation pattern in that alignment column. For example, if an alignment column contains mostly hydrophobic amino acids, the corresponding match state would have high emission probabilities for L, I, V, F, and A. Insertion states (I) emit amino acids according to background frequencies, modeling insertions relative to the consensus sequence. Deletion states (D) emit no amino acids (silent states), modeling deletions relative to the consensus sequence.\nThe transition probabilities between these states model the frequency of insertions and deletions in the protein family. For alignment column \\(i\\), typical transitions include:\n\n\\(M_{i-1} \\to M_i\\): normal progression through the alignment\n\\(M_{i-1} \\to I_{i-1}\\): insertion after match state\n\n\\(M_{i-1} \\to D_i\\): deletion of alignment column \\(i\\)\n\\(I_{i-1} \\to M_i\\): end of insertion, return to consensus\n\\(I_{i-1} \\to I_{i-1}\\): extension of insertion\n\\(D_{i-1} \\to D_i\\): extension of deletion\n\nThis structure allows profile HMMs to identify sequences that match the overall pattern of a protein family even when they contain insertions, deletions, or substitutions relative to the consensus sequence.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hidden Markov Models</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/hidden_markov_models.html#pair-hmms-for-sequence-alignment",
    "href": "chapters/bioinformatics/hidden_markov_models.html#pair-hmms-for-sequence-alignment",
    "title": "29  Hidden Markov Models",
    "section": "Pair HMMs for Sequence Alignment",
    "text": "Pair HMMs for Sequence Alignment\nPair HMMs extend the framework to model the evolutionary relationship between two sequences simultaneously, finding applications in sequence alignment and comparative genomics. This is particularly useful for identifying conserved regions between species, which likely represent functionally important elements.\n\nstateDiagram-v2\n    [*] --&gt; Match\n    Match --&gt; Match\n    Match --&gt; InsertX\n    Match --&gt; InsertY\n    InsertX --&gt; Match\n    InsertX --&gt; InsertX\n    InsertY --&gt; Match\n    InsertY --&gt; InsertY\n    Match --&gt; [*]\n    InsertX --&gt; [*]\n    InsertY --&gt; [*]\n    \n    note right of Match\n        Emit from both sequences\n        Models aligned positions\n        Substitution patterns\n    end note\n    \n    note right of InsertX\n        Emit from sequence X only\n        Gap in sequence Y\n        Insertion events\n    end note\n    \n    note right of InsertY\n        Emit from sequence Y only\n        Gap in sequence X\n        Deletion events\n    end note\n\n\n\n\nstateDiagram-v2\n    [*] --&gt; Match\n    Match --&gt; Match\n    Match --&gt; InsertX\n    Match --&gt; InsertY\n    InsertX --&gt; Match\n    InsertX --&gt; InsertX\n    InsertY --&gt; Match\n    InsertY --&gt; InsertY\n    Match --&gt; [*]\n    InsertX --&gt; [*]\n    InsertY --&gt; [*]\n    \n    note right of Match\n        Emit from both sequences\n        Models aligned positions\n        Substitution patterns\n    end note\n    \n    note right of InsertX\n        Emit from sequence X only\n        Gap in sequence Y\n        Insertion events\n    end note\n    \n    note right of InsertY\n        Emit from sequence Y only\n        Gap in sequence X\n        Deletion events\n    end note\n\n\n\n\n\n\nThe states in a pair HMM typically represent different types of evolutionary events:\n\nMatch state: emits aligned amino acids or nucleotides from both sequences\nInsertion in sequence 1: emits from sequence 1 only (gap in sequence 2)\nInsertion in sequence 2: emits from sequence 2 only (gap in sequence 1)\n\nThe emission probabilities for the match state capture the substitution patterns between the two sequences:\n\\[\ne_M(a,b) = P(\\text{emitting amino acid } a \\text{ from sequence 1 and amino acid } b \\text{ from sequence 2})\n\\tag{38.1}\\]\nFor closely related sequences, this probability would be high when \\(a = b\\) (identical amino acids) and lower for substitutions. For distantly related sequences, the emission probabilities would reflect the amino acid substitution matrices derived from evolutionary studies.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hidden Markov Models</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/hidden_markov_models.html#hidden-semi-markov-models",
    "href": "chapters/bioinformatics/hidden_markov_models.html#hidden-semi-markov-models",
    "title": "29  Hidden Markov Models",
    "section": "Hidden Semi-Markov Models",
    "text": "Hidden Semi-Markov Models\nHidden semi-Markov models (HSMMs) relax the geometric length distribution assumption of standard HMMs by explicitly modeling state durations. This extension is particularly valuable for biological applications where feature lengths follow non-geometric distributions, such as protein domains or gene exons.\nInstead of transition probabilities between individual positions, HSMMs use duration probabilities \\(P(d)\\) for spending exactly \\(d\\) time steps in a state, and transition probabilities between different state types. The forward algorithm becomes:\n\\[\nf_l(i) = \\sum_d \\sum_k f_k(i-d) \\times a_{kl} \\times P(d) \\times \\prod_{j=i-d+1}^{i} e_l(x_j)\n\\tag{38.2}\\]\nThis allows modeling of complex length distributions while maintaining computational tractability. For example, exon lengths in human genes could be modeled using empirical length distributions derived from known genes, rather than the geometric distributions implicit in standard HMMs.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hidden Markov Models</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/hidden_markov_models.html#phylogenetic-hmms-for-comparative-genomics",
    "href": "chapters/bioinformatics/hidden_markov_models.html#phylogenetic-hmms-for-comparative-genomics",
    "title": "29  Hidden Markov Models",
    "section": "Phylogenetic HMMs for Comparative Genomics",
    "text": "Phylogenetic HMMs for Comparative Genomics\nModern comparative genomics applications use sophisticated HMM variants to analyze multiple genome alignments simultaneously. These phylogenetic HMMs model the evolution of functional elements across multiple species, allowing identification of conserved regulatory elements, detection of positive selection, and reconstruction of ancestral genome organization.\n\ngraph TB\n    subgraph \"Evolutionary States\"\n        C1[Coding under purifying selection]\n        U1[UTR under moderate constraint]\n        R1[Regulatory under strong constraint]\n        N1[Neutral sequences]\n        P1[Positive selection]\n    end\n    \n    subgraph \"Species Tree\"\n        S1[Human] \n        S2[Chimp]\n        S3[Mouse]\n        S4[Rat]\n    end\n    \n    C1 --&gt; S1\n    C1 --&gt; S2\n    C1 --&gt; S3\n    C1 --&gt; S4\n    \n    style C1 fill:#e8f5e8\n    style R1 fill:#e3f2fd\n    style N1 fill:#fff3e0\n\n\n\n\ngraph TB\n    subgraph \"Evolutionary States\"\n        C1[Coding under purifying selection]\n        U1[UTR under moderate constraint]\n        R1[Regulatory under strong constraint]\n        N1[Neutral sequences]\n        P1[Positive selection]\n    end\n    \n    subgraph \"Species Tree\"\n        S1[Human] \n        S2[Chimp]\n        S3[Mouse]\n        S4[Rat]\n    end\n    \n    C1 --&gt; S1\n    C1 --&gt; S2\n    C1 --&gt; S3\n    C1 --&gt; S4\n    \n    style C1 fill:#e8f5e8\n    style R1 fill:#e3f2fd\n    style N1 fill:#fff3e0\n\n\n\n\n\n\nFor example, a phylogenetic HMM for analyzing mammalian genome alignments might include states representing:\n\nCoding sequences under purifying selection\nUntranslated regions under moderate constraint\n\nRegulatory elements under strong constraint\nNeutral sequences under no constraint\nRegions under positive selection\n\nThe emission probabilities at each state model the expected substitution patterns under different evolutionary pressures, while the transition probabilities model the spatial organization of functional elements along chromosomes.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Hidden Markov Models</span>"
    ]
  },
  {
    "objectID": "chapters/web/new_hmm_exercise/index.html",
    "href": "chapters/web/new_hmm_exercise/index.html",
    "title": "30  Membrane proteins",
    "section": "",
    "text": "Using TMHMM…",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Membrane proteins</span>"
    ]
  },
  {
    "objectID": "chapters/project/orf_project/index.html",
    "href": "chapters/project/orf_project/index.html",
    "title": "31  Finding genes",
    "section": "",
    "text": "Finding Open Reading Frames\nThis chapter is a programming project where you will find open reading frames in the genome of a particularly virulent strain of E. coli.\nIn this project, you will analyze DNA to identify the open reading frames (ORFs) and the proteins they encode.\nDownload the files you need for this project:\nhttps://munch-group.org/bioinformatics/supplementary/project_files\nYou also need to download the two project files:\nPut all three files in a folder dedicated to this project. On most computers you can right-click on the link and choose “Save file as…” or “Download linked file”.\nFour your convenience, the file orfproject.py already contains three global constants (variables that must never be changed by the code). One is a dictionary codon_map, which maps codons to letters that represent amino acids. The other two are a string, start_codon, and a list, stop_codons. You can refer to these three variables in your code, but obviously, never change them.\nThe project has three parts.\nStart by reading through the exercise before you do anything else. That way you have a good overview of the tasks ahead. Here is a mind map of how we split the larger problem into smaller bits and how they fit together:",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Finding genes</span>"
    ]
  },
  {
    "objectID": "chapters/project/orf_project/index.html#finding-open-reading-frames",
    "href": "chapters/project/orf_project/index.html#finding-open-reading-frames",
    "title": "31  Finding genes",
    "section": "",
    "text": "Find the start positions of ORFs in a DNA sequence\nThe first task is to write a function that finds all the possible positions where an ORF can begin.\nWrite a function, find_start_positions, which takes one argument:\n\nA string, which is a DNA sequence.\n\nThe function must return:\n\nA list of integers, which represent the indexes of the first base in start codons in the DNA sequence argument.\n\nExample usage:\nfind_start_positions('TATGCATGATG')\nshould return\n[1, 5, 8]\nYour function should contain a for-loop that iterates over all possible positions in a DNA string where a codon can begin. Not surprisingly, these are all the positions except for the last two. So start out with this:\ndef find_start_positions(seq):\n    for i in range(len(seq) - 2):\n        print(i)\nNow, instead of just printing i, try and make it print the three bases following i using the slicing technique:\ntriplet = seq[i:i+3]\nI.e. if your sequence is 'TATGCATGATG' it should first print 'TAT' then 'ATG' then 'TGC' and so on.\nWhen you have this working you should change the code so that triplets are only printed if they are start codons. You can use an if-statement that tests if each triplet is equal to start_codon.\nThen try and make your function print i only when i is the first base of a start codon.\nFinally, modify the function so all the relevant values of i are collected in a list using the same technique as in the split_codons(orf) function, and then return this list from the function.\n\n\nFinding the next occurrence of some codon in an ORF\nNow that you can find where the ORFs begin in our sequence you must also be able to identify where each of these end. As you know, an ORF ends at any of three different stop codons in the same reading frame as the start codon. So, starting at the start codon of the ORF, we need to be able to find the next occurrence of some specific codon. I.e. you should look at all codons after the start codon and find the first occurrence of some specified codon. If the function does not find that codon in the string it should return None.\nWrite a function, find_next_codon, that takes three arguments:\n\nA string, which is the DNA sequence.\nAn integer, which is the index in the sequence where the ORF starts.\nA string, which is the codon to find the next occurrence of.\n\nThe function must return:\n\nAn integer, which is the index of the first base in the next in-frame occurrence of the codon. If the function does not find that codon in the string it should return None.\n\nExample usage:\nfind_next_codon('AAAAATTTAATTTAA', 1, 'TTT')\nshould return\n10\nYour function should contain a for-loop that iterates over all the relevant starts of codons. Remember that no valid codon can start at the last two positions in the sequence. E.g. if the second argument is 7 and the length of the sequence is 20 then the relevant indexes are 7, 10, 13, 16.\nStart by writing a function just with a for-loop that lets you print these indexes produced by range. Figure out how to make the range function iterate over the appropriate numbers.\ndef find_next_codon(seq, start, codon):\n    for idx in range( ?? ):\n        print(idx)\nWhen you have that working, use the slicing technique to instead print the codons that start at each index.\nFinally, add an if-statement that tests if each codon is equal to codon. When this is true, the function should return the value of idx.\n\n\nFinding the first stop codon in an ORF\nNow that you can find the next occurrence of any codon, you are well set up to write a function that finds the index for the beginning of the next in-frame stop codon in an ORF.\nWrite a function, find_next_stop_codon, that takes two arguments:\n\nA string, which is the DNA sequence.\nAn integer, which is the index in the sequence where the ORF starts.\n\nThe function must return:\n\nAn integer, which is the index of the first base in the next in-frame stop codon. If there is no in-frame stop codon in the sequence the function should return None.\n\nExample usage:\nfind_next_stop_codon('AAAAATAGATGAAAA', 2)\nshould return\n5\nHere is some inspiration:\n\nYou should define a list to hold the indexes for the in-frame stop codons we find.\nThen we loop over the three possible stop codons to find the next in-frame occurrence of each one from the start index. You can use find_next_codon for this. Remember that it returns None if it does not find any. If it does find a position you can add it to your list.\nAt the end, you should test if you have any indexes in your list.\nIf you do, you should return the smallest index in the list. I.e the ones closest to the start codon.\nIf you did not find any stop codons the function must return None to indicate this.\n\n\n\nFinding ORFs\nNow you can write a function that uses find_start_positions and find_next_stop_codon to extract the start and end indexes of each ORF in a genomic sequence.\nWrite a function, find_orfs, that takes one argument:\n\nA string, which is a DNA sequence.\n\nThe function should return:\n\nA list, which contains lists with two integers. The list returned must contain a list for each ORF in the sequence argument. These lists each contain two integers. The first integer represents the start of the ORF, the second represents the end. The function should handle both uppercase and lowercase sequences.\n\nExample usage:\nfind_orfs(\"AAAATGGGGTAGAATGAAATGA\")\nshould return\n[[3, 9], [13, 19]]\nStart by using find_start_positions to get a list of all the start positions in sequence:\ndef find_orfs(seq):\n    start_positions = find_start_positions(seq)\nWhen you have that working, add a for-loop that iterates over the start positions. Inside the for-loop, you can then get the next in-frame stop codon for each start position by calling find_next_stop_codon. Try to print the start and end indexes you find to make sure the code does what you think.\nFinally, you need to add a [start, stop] list for each ORF to the big list that the function returns. To append a list to a list you do write something like this:\norf_coordinate_list.append([start, stop])\nTest your function. Chances are that some of the end positions you get are None. This is because some of the start codons were not followed by an in-frame stop codon. Add an if-statement to your function that controls that only start-stop pairs with a valid stop coordinate are added to the list of results.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Finding genes</span>"
    ]
  },
  {
    "objectID": "chapters/project/orf_project/index.html#translation-of-open-reading-frames",
    "href": "chapters/project/orf_project/index.html#translation-of-open-reading-frames",
    "title": "31  Finding genes",
    "section": "Translation of open reading frames",
    "text": "Translation of open reading frames\nWe need to translate the reading frames we find into the proteins they may encode. So why not use the code you already wrote in the programming project where you translated open frames? Copy the content of translationproject.py into orfproject.py. Now you can use the function translate_orf to translate your ORFs.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Finding genes</span>"
    ]
  },
  {
    "objectID": "chapters/project/orf_project/index.html#put-everything-together",
    "href": "chapters/project/orf_project/index.html#put-everything-together",
    "title": "31  Finding genes",
    "section": "Put everything together",
    "text": "Put everything together\n\nRead in genomic sequences\nThe file e_coli_O157_H157_str_Sakai.fasta contains the genome that we want to analyze to find open reading frames. This is an especially nasty strain of Escherichia coli O157:H7 isolated after a massive outbreak of infection in school children in Sakai City, Japan, associated with consumption of white radish sprouts.\nYou can use the function below to read the genome sequence into a string.\ndef read_genome(file_name):\n    f = open(file_name, 'r')\n    lines = f.readlines()\n    header = lines.pop(0)\n    substrings = []\n    for line in lines:\n        substrings.append(line.strip())\n    genome = ''.join(substrings)\n    f.close()\n    return genome\nNow for the grand finale: Using read_genome, find_orfs and translate_orf you can write a function that finds all protein sequences produced by open reading frames in the genome.\nWrite a function, find_candidate_proteins, that takes one argument:\n\nA string, which is a genome DNA sequence.\n\nThe function must return\n\nA list of strings, which each represent a possible protein sequence.\n\nNote that this is a full genome so finding all possible proteins will take a while (~5 min.). You can start by working on the first 1000 bases:\nExample usage:\ngenome = read_genome('e_coli_O157_H157_str_Sakai.fasta')\nfirst_1000_bases = genome[:1000]\nfind_candidate_proteins(first_1000_bases)\nshould return\n['MSLCGLKKESLTAASELVTCRE*', 'MKRISTTITTTITTTITITITTGNGAG*',\n 'MQNVFCGLPIFWKAMPGRGRWPPSSLPPPKSPTTWWR*', 'MPGRGRWPPSSLPPPKSPTTWWR*',\n 'MLYPISAMPNVFLPNF*', 'MPNVFLPNF*', 'MSCMALVC*', 'MALVC*']\nThe function should call find_orfs to get the list of start-end pairs. For each index pair, you must then slice the ORF out of the sequence (remember that the end index represents the first base in the stop codon), translate the ORF to protein, and add it to a list of proteins that the function can return.\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\nTo check your result note that all returned sequences should start with a start codon 'M', end with a stop codon '*' and contain no stop codons in the middle.\n\n\n\n\n\nOn your own\nThis is where this project ends, but you can continue if you like. Given a long list of candidate proteins of all sizes, what would you do to narrow down your prediction to a smaller set of very likely genes? If you have some ideas, then try them out.\n\nMaybe you can rank them by length? What is the expected minimum length of proteins?\nMaybe you can look for a Shine-Delgarno motif upstream of the start codon? You know how to do that from the lectures.\nYou can also try to BLAST them against the proteins in Genbank. The true ones should have some homologs in other species.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Finding genes</span>"
    ]
  },
  {
    "objectID": "chapters/web/neural_networks/index.html",
    "href": "chapters/web/neural_networks/index.html",
    "title": "33  Neural networks",
    "section": "",
    "text": "Machine learning comes in many forms. Whether you want to call them artificial intelligence or just models is a matter of taste. Hidden Markov models are one class of models, and neural networks are another.\nAt this point, you already know the basics of neural networks, especially feed-forward networks, and how we can train such a network of sigmoid neurons to work as a classifier. In this exercise, you will play with neural networks and explore how features of input data and hidden layers affect the properties and the neural network’s performance.\nFollow the above link into the Neural Network playground. Start by reading the text below the dashboard and look over the dashboard’s components. At the top, there is a panel with dropdown menus controlling the properties of the network and selecting the type of problem to solve. We will start with an activation type of “TanH” (similar to sigmoid activation) and a problem type of “Classification” (already set if you have not used the site before). You can pick the data set to work on in the “Data” column on the left side. Select the top left one (blue dots surrounded by orange ones), and leave the other controls as they are.\nYou see the network layout in the middle, with the input layer on the left and the output layer on the right. The data is a set of colored points in a two-dimensional coordinate system. The points are split into a training set and a test set. In each epoch of the training, the model parameters are changed, and the resulting change in performance is evaluated by running the model on the test data. The points included in the training data are the ones you see on the right over the output. There is a tick box in the output panel that lets you see the test data too. If you drag the “Data” slider “Ratio of training to test data” to the left, you can see how the training data set shrinks as you include more data in your test set. Leave it at 50%.\nThe neurons in the input layer fire if the data shows a particular feature. In the case of feature X1, the neuron produces output when the data exhibits a gradient of orange points on the left and blue points on the right. Other features trigger the other input neurons, as their thumbnails show.\n\nExercise 33-1\nTrim the network down to a single hidden layer (-1 layer) with just two neurons, and make sure only the X1 feature is selected in the input layer. In the output on the right, the color of each point shows which class it belongs to. The background color represents the classification made by your neural network.\nPress play to start the training. Pay close attention to how the weights and outputs change from their initial states. Run the network for 100-200 epochs, then stop and inspect it.\n\nThe two neurons in the hidden layer should produce output for different features - try hovering over them and think about how the single feature creates these activations.\nDo the weights or the biases differ the most? You can hover over the lines/points or look at their color.\nWhat is the Test Loss noted under output?\nDo you think adding more features, neurons, or layers will improve the classification?\n\n\n\nExercise 33-2\nWe may need more features. Try adding the input node capturing the X2 feature and run this model for 200 epochs.\n\nIs the Test Loss better now (Test Loss is the loss function of the test data).\nThe Output prediction is quite different now; look at the neurons and consider how adding the X2 Feature changed the other neuron outputs.\nIt seems to have quite a different fit - will adding more neurons make it even better?\n\n\n\nExercise 33-3\nTwo neurons may not allow for enough flexibility. Add more neurons to the hidden layer and run the model for 200 epochs.\n\nHow many neurons must you add to gain Test Loss under &gt; 0.005?\n\n\n\nExercise 33-4\nNow lets see how good you are:\n\nChallenge: How low can you go in total Neurons and Features and still produce a good fit (Test Loss under 0.005)? Consider which features work well with the data especially.\nChallenge: Try creating a model that performs poorly despite having at least three features and four neurons (poorly being Test Loss over 0.1).\n\n\n\nExercise 33-5\nThe ring-shaped classification problem is relatively easy (for neural networks). Now, it is time to challenge it with a more complex problem. Pick the spiral pattern (bottom left option under Data). This pattern is much more complicated, so a Test Loss below 0.05 (rather than 0.005) is good. Using more Features and Neurons will also take longer for each epoch, so you need to be more patient. It also requires more Epochs to train, so wait for 500-1000 Epochs before deciding how well the network performs. While you wait, you can watch the Test Loss graph in the output panel.\n\nTry fitting a model using all features, and decide how many neurons and layers to use. How many neurons do you need to do a good fit? Hint: You may need more than one hidden layer.\nLook at the neurons in the last layer - does any of the neurons reflect the spiral of the data well?\nTry removing some of the features the network uses the least - does it still perform well?\nSummarize the changes you had to make to the network to make it fit the spiral data. Do these changes capture the non-linear pattern in the data?\n\n\n\nExercise 33-6\nLet’s wrap up with a different kind of problem - regression. In contrast to classification, we do not try to predict a discrete class but rather a specific numerical value. At the top left, click on problem type and choose “Regression”. Of the datasets, pick the more difficult dataset to the right, listed as “Multi Gaussian”. Include only the X1 and X2 features, but try different network architectures. Let the training run between 200 and 500 Epochs when testing your network.\n\nCan you get the Test Loss below 0.005 with only a single hidden layer?\nDoes adding more hidden layers improve the model?\nWhich additional features enhance the model the most?\nDoes the addition of some features only improve models with more than one hidden layer?",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Neural networks</span>"
    ]
  },
  {
    "objectID": "chapters/project/folding_project/index.html",
    "href": "chapters/project/folding_project/index.html",
    "title": "34  Primer analysis",
    "section": "",
    "text": "Count the number of bases in your candidate primer\nSay you have a short sequence that you want to use as a PCR primer. To evaluate if it a suitable primer, you need to know the melting temperature of the sequence. You also need to make sure that it will not fold up on itself so that it cannot bind to the DNA sequence you want to amplify.\nDownload the files you need for this project:\nhttps://munch-group.org/bioinformatics/supplementary/project_files\nPut the files in a folder dedicated to this project. On most computers you can right-click on the link and choose “Save file as…” or “Download linked file”.\nBefore you can compute the melting temperature, you need to determine how many times each base occurs in the sequence. You can assume that the only characters in the string are A, T, G, and C.\nWrite a function, count_bases, which takes one argument:\nThe function must return:\nExample usage: If the function is called like this:\nthen it should return (not necessarily with key-value pairs in the same order):",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Primer analysis</span>"
    ]
  },
  {
    "objectID": "chapters/project/folding_project/index.html#count-the-number-of-bases-in-your-candidate-primer",
    "href": "chapters/project/folding_project/index.html#count-the-number-of-bases-in-your-candidate-primer",
    "title": "34  Primer analysis",
    "section": "",
    "text": "A string, which is a DNA sequence.\n\n\n\nA dictionary, which in which keys are strings that represent bases and values are integers that represent the number of occurrences of each base. If a base is not found in the sequence, its count must be zero.\n\n\ncount_bases(\"ATGG\")\n\n{\"A\": 1, \"C\": 0, \"G\": 2, \"T\": 1}",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Primer analysis</span>"
    ]
  },
  {
    "objectID": "chapters/project/folding_project/index.html#compute-the-melting-temperature",
    "href": "chapters/project/folding_project/index.html#compute-the-melting-temperature",
    "title": "34  Primer analysis",
    "section": "Compute the melting temperature",
    "text": "Compute the melting temperature\nKnowing the base composition in your sequence, you can now calculate the melting temperature the double-stranded DNA that forms when your primer pairs up with the sequence to amplify. If the primer has less than 14 bases the formula for calculating melting temperature is:\n\\[ Temp = (A + T) * 2 + (G + C) * 4 \\]\nand if the primer has 14 bases or more it is calculated like this:\n\\[ Temp = 64.9 + 41 * (G + C - 16.4) / (A + T + G + C) \\]\nThe A, T, C, and G in the formulas represent the numbers of A, T, C and G in the DNA primer.\nYou must write a function that applies these two formulas appropriately by taking the length of the primer into account.\nWrite a function, melting_temp, which takes one argument:\n\nA string, which is a DNA sequence (your primer).\n\nThe function must return:\n\nA number, which represents the melting temperature of the double-stranded DNA corresponding to the DNA string given as argument.\n\nExample usage: If the function is called like this:\nmelting_temp(\"ATG\")\nthen it should return:\n8",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Primer analysis</span>"
    ]
  },
  {
    "objectID": "chapters/project/folding_project/index.html#reverse-complement-the-sequence",
    "href": "chapters/project/folding_project/index.html#reverse-complement-the-sequence",
    "title": "34  Primer analysis",
    "section": "Reverse complement the sequence",
    "text": "Reverse complement the sequence\nIt is possible that one part of the primer forms base pairs with another part of the primer to form a hairpin structure. To figure out if this can happen to your primer, you need to be able to find the reverse complement of DNA sequence. The reverse complement of a DNA sequence is one where the sequence of bases is first reversed, and then each base is replaced with its Watson-Crick complementary base.\nWrite a function, reverse_complement, which takes one argument:\n\nA string, which is a DNA sequence.\n\nThe function must return:\n\nA string, which represents the reverse complement of the DNA string given as argument.\n\nExample usage: If the function is called like this:\nreverse_complement(\"AATTC\")\nthen it should return:\n\"GAATT\"",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Primer analysis</span>"
    ]
  },
  {
    "objectID": "chapters/project/folding_project/index.html#check-for-hairpins",
    "href": "chapters/project/folding_project/index.html#check-for-hairpins",
    "title": "34  Primer analysis",
    "section": "Check for hairpins",
    "text": "Check for hairpins\nYou would like to be able to determine if your primer can fold to form a hairpin with some specified minimum number of consequtive base pairs. We assume that hairpin loops are always at least four bases long and that base pairs in the hairpin can only be Watson-Crick basepairs. Here is an example of a hairpin with five basepairs and a loop of four bases (four Cs):\n        C C\n      C     C\n       A - T\n       T - A\n       A - T\n       T - A\n       A - T\nTo test if a sequence can form a hairpin with at least four consequtive base pairs, you need to test if the sequence contains any subsequence of length four whose reverse complement is identical to another nonoverlapping subsequence. To take into account that the hairpin loop is at least four bases long, any such two subsequences must be separated by at least four bases.\nWrite a function, has_hairpin, which takes two arguments:\n\nA string, which is a DNA sequence.\nAn integer, which represents the minimum number of consequtive base pairs in the hairpins to search for.\n\nThe function must return:\n\nTrue if the sequence contains a hairpin of at least the specified length and False otherwise.\n\nExample usage: If the function is called like this:\nprint(has_hairpin(\"ATATACCCCTATAT\", 4))\nthen it should return:\nTrue\nThis is a hard one, so I will give you a bit of help. Here is the function with some parts missing.\ndef has_hairpin(s, k):\n    looplen = 4\n    for i in range(len(s)-k+1):\n        subs = # Hint A\n        right = # Hint B\n        revcl = reverse_complement(subs)\n        if revcl in right[looplen:]:\n            return True\n    return False\nHint A: Here you need to extract a substring of length k starting at i. Hint B: Here you need to extract all the sequence to the right of substr.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Primer analysis</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/assembly_and_mapping.html",
    "href": "chapters/bioinformatics/assembly_and_mapping.html",
    "title": "35  Genome Sequencing",
    "section": "",
    "text": "Introduction: The Genomic Revolution\nThe ability to read the complete genetic information of organisms has fundamentally transformed our understanding of biology. From the first complete genome of a bacterium in 1995 to the routine sequencing of human genomes today, the technological and computational advances in genome sequencing have been nothing short of revolutionary. This transformation has been driven by parallel innovations in sequencing technologies that generate the raw data, assembly algorithms that reconstruct complete genomes from fragmented reads, and mapping algorithms that align sequencing reads to reference genomes for comparative analysis.\nThe challenge of genome sequencing and assembly can be likened to reconstructing a massive book that has been shredded into millions of tiny pieces. Each fragment contains only a few words or sentences, many fragments overlap with others, some portions of the text are repeated multiple times throughout the book, and to complicate matters further, some fragments contain errors—letters that have been changed or are missing. The computational task is to take these millions of fragments and reconstruct the original text as accurately and completely as possible.\nThis challenge becomes even more complex when we consider the scale of modern genomic projects. A human genome consists of approximately 3 billion base pairs, and modern sequencing technologies might break this into hundreds of millions or even billions of short reads, each typically ranging from 50 to several thousand base pairs in length. The computational requirements for processing this data are substantial, often requiring sophisticated algorithms, high-performance computing resources, and careful optimization of both time and memory usage.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Genome Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/assembly_and_mapping.html#evolution-of-dna-sequencing-technologies",
    "href": "chapters/bioinformatics/assembly_and_mapping.html#evolution-of-dna-sequencing-technologies",
    "title": "35  Genome Sequencing",
    "section": "Evolution of DNA Sequencing Technologies",
    "text": "Evolution of DNA Sequencing Technologies\n\nFirst Generation: Sanger Sequencing\nThe foundation of DNA sequencing was laid by Frederick Sanger in 1977 with the development of chain-termination sequencing, commonly known as Sanger sequencing. This method uses DNA polymerase to synthesize complementary strands of template DNA, with the crucial innovation being the incorporation of chain-terminating dideoxynucleotides (ddNTPs). When a ddNTP is incorporated into the growing DNA strand, synthesis terminates because ddNTPs lack the 3’-OH group necessary for forming the next phosphodiester bond.\nSanger sequencing produces high-quality reads with accuracy exceeding 99.9% and read lengths of 700-1000 base pairs. These characteristics made it the gold standard for the Human Genome Project, completed in 2003. The method’s high accuracy and relatively long read lengths simplified the assembly process significantly, as longer reads are more likely to span repetitive regions and provide unambiguous placement information. However, Sanger sequencing is expensive and low-throughput, typically processing only 96 or 384 samples simultaneously, making it impractical for large-scale genomic studies or routine clinical applications.\nThe assembly of genomes using Sanger sequencing data typically employed overlap-layout-consensus (OLC) algorithms. These algorithms identified overlaps between reads, arranged them in a layout graph representing their relative positions, and derived consensus sequences from the overlapping regions. The longer read lengths meant that even complex repetitive regions could often be resolved, though the most challenging repeats still required additional techniques such as paired-end sequencing or physical mapping.\n\n\nSecond Generation: High-Throughput Short-Read Technologies\nThe landscape of genomics changed dramatically with the introduction of second-generation sequencing technologies, also called next-generation sequencing (NGS). These platforms, including Illumina (originally Solexa), Ion Torrent, and SOLiD, revolutionized sequencing by enabling massive parallelization, dramatically reducing both cost and time while increasing throughput by several orders of magnitude.\nIllumina sequencing, which has become the dominant platform, uses a sequencing-by-synthesis approach with reversible terminator chemistry. DNA fragments are attached to a flow cell surface and amplified in situ to create clusters of identical sequences. During each sequencing cycle, fluorescently labeled nucleotides with reversible terminators are added. After incorporation of a single nucleotide, the cluster is imaged to determine which base was added, then the terminator and fluorescent groups are cleaved to allow the next cycle. This process typically generates reads of 50-300 base pairs with error rates around 0.1-1%.\nThe dramatic increase in throughput came with a significant trade-off: much shorter read lengths compared to Sanger sequencing. These short reads presented new computational challenges for genome assembly. Repetitive sequences longer than the read length became impossible to resolve unambiguously, leading to fragmented assemblies with many gaps. The sheer volume of data also demanded new algorithmic approaches that could handle billions of reads efficiently.\nTo address these challenges, de Bruijn graph-based assemblers emerged as the dominant approach for short-read assembly. Rather than explicitly computing overlaps between millions of reads, these algorithms decompose reads into k-mers (subsequences of length k) and build a graph where nodes represent k-mers and edges connect k-mers that overlap by k-1 bases. This transformation reduces the assembly problem to finding an Eulerian path through the graph, which can be solved efficiently even for large datasets.\n\n\nThird Generation: Long-Read Technologies\nThe limitations of short-read sequencing in resolving complex genomic regions drove the development of third-generation sequencing technologies that produce much longer reads. Pacific Biosciences (PacBio) and Oxford Nanopore Technologies (ONT) represent the two major platforms in this category, each using fundamentally different approaches to achieve long-read sequencing.\nPacBio’s Single Molecule Real-Time (SMRT) sequencing observes DNA synthesis in real-time using zero-mode waveguides (ZMWs). Each ZMW is a nanoscale observation chamber where a single DNA polymerase molecule synthesizes DNA using fluorescently labeled nucleotides. The system detects the fluorescent signal as each nucleotide is incorporated, with the time between incorporations providing information about polymerase kinetics that can help identify base modifications. Modern PacBio systems generate reads with median lengths of 10-25 kilobases, with some reads exceeding 100 kilobases.\nOxford Nanopore sequencing uses an entirely different principle: DNA strands are threaded through biological nanopores embedded in synthetic membranes. As the DNA translocates through the pore, it disrupts the ionic current flowing through the pore. Different k-mers produce characteristic current patterns that can be decoded to determine the DNA sequence. Nanopore sequencing can produce extremely long reads, with reports of reads exceeding 2 megabases, though typical read lengths are 10-30 kilobases.\nBoth long-read technologies initially suffered from high error rates (10-15%), primarily consisting of insertions and deletions rather than substitutions. However, recent improvements in chemistry and base-calling algorithms have reduced error rates significantly. PacBio’s HiFi (High Fidelity) reads, generated by circular consensus sequencing, achieve accuracy exceeding 99.9% by repeatedly sequencing the same DNA molecule. Similarly, improvements in nanopore chemistry and base-calling have pushed accuracy above 95% for single reads, with consensus accuracy approaching 99.9%.\n\n\nFourth Generation: Emerging Technologies\nThe field continues to evolve with emerging technologies that promise to further transform genomic analysis. Synthetic long reads, exemplified by 10x Genomics’ linked-read technology, use molecular barcoding to associate short reads derived from the same long DNA molecule. This approach combines the accuracy and cost-effectiveness of short-read sequencing with some of the benefits of long-range information.\nSingle-cell sequencing technologies enable genomic analysis at unprecedented resolution, revealing cell-to-cell heterogeneity in complex tissues and tumor samples. Spatial transcriptomics and spatial genomics technologies add another dimension by preserving spatial information about where in a tissue sample each sequence originated.\nDirect RNA sequencing, particularly using nanopore technology, eliminates the need for reverse transcription and can detect RNA modifications directly. This capability opens new avenues for studying epitranscriptomics and RNA processing.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Genome Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/assembly_and_mapping.html#genome-assembly-algorithmic-foundations",
    "href": "chapters/bioinformatics/assembly_and_mapping.html#genome-assembly-algorithmic-foundations",
    "title": "35  Genome Sequencing",
    "section": "Genome Assembly: Algorithmic Foundations",
    "text": "Genome Assembly: Algorithmic Foundations\n\nThe Assembly Problem\nGenome assembly is fundamentally a complex computational problem that involves reconstructing a long string (the genome) from a collection of shorter substrings (sequencing reads) that may contain errors and have unknown relative positions. The challenge is compounded by several factors inherent to biological genomes:\nRepetitive sequences are perhaps the most significant challenge. Genomes contain various types of repeats, from short tandem repeats to large segmental duplications and transposable elements. When a repeat is longer than the read length, it becomes impossible to determine unambiguously which copy of the repeat each read originated from, leading to assembly ambiguities and potential misassemblies.\nSequencing errors introduce noise into the assembly process. These errors can create false overlaps between reads, spurious branches in assembly graphs, and challenges in distinguishing true genomic variation from sequencing artifacts. Error correction is therefore a crucial preprocessing step in most assembly pipelines.\nHeterozygosity in diploid and polyploid genomes adds another layer of complexity. Assemblers must distinguish between sequences from different haplotypes, which may differ at heterozygous sites. Some assemblers attempt to phase haplotypes, producing separate assemblies for each chromosome copy, while others produce a consensus assembly that may lose haplotype-specific information.\nCoverage variation across the genome can result from biases in DNA extraction, library preparation, or sequencing. Regions with very high coverage may be collapsed repeats or duplications, while low-coverage regions may be assembled poorly or missed entirely. Assemblers must be robust to these coverage variations while still being sensitive enough to detect true copy number variations.\n\n\nOverlap-Layout-Consensus Algorithms\nThe Overlap-Layout-Consensus (OLC) approach was the dominant paradigm for genome assembly during the Sanger sequencing era and has experienced a renaissance with long-read technologies. The OLC pipeline consists of three main phases:\nIn the overlap phase, the algorithm identifies all significant overlaps between pairs of reads. For n reads, a naive all-against-all comparison requires O(n²) comparisons, which becomes computationally prohibitive for large datasets. Practical implementations use various heuristics to reduce the search space, such as k-mer indexing to identify candidate overlapping pairs quickly. The overlap computation must be sensitive enough to detect true overlaps despite sequencing errors while avoiding false overlaps from repetitive sequences.\nThe layout phase constructs an overlap graph where nodes represent reads and edges represent overlaps. The algorithm then identifies paths through this graph that represent contiguous sequences (contigs). The challenge is to find paths that use each read approximately once while respecting the overlap relationships. In practice, the overlap graph often contains complex structures such as bubbles (representing heterozygosity or errors) and repeats (creating branches and cycles) that must be resolved.\nThe consensus phase derives the final sequence from the layout of overlapping reads. This involves multiple sequence alignment of all reads assigned to each position, followed by calling the consensus base at each position. The consensus algorithm must weight evidence appropriately, considering base quality scores and coverage depth, while being robust to systematic errors that might affect multiple reads similarly.\nModern OLC assemblers like Canu and FALCON have adapted these principles for long, error-prone reads. They typically begin with an error correction phase that uses overlaps between noisy long reads to generate consensus sequences with much lower error rates. The corrected reads are then assembled using modified OLC algorithms that can handle the characteristics of long-read data, such as higher residual error rates and non-uniform error distributions.\n\n\nDe Bruijn Graph Assembly\nDe Bruijn graph-based assembly emerged as the solution to handling the massive datasets generated by short-read sequencing technologies. The key insight is to transform the assembly problem from an overlap graph traversal to an Eulerian path problem, which can be solved more efficiently.\nThe process begins by decomposing all reads into k-mers, subsequences of length k. For a read of length L, this produces L-k+1 overlapping k-mers. The choice of k is crucial: smaller values of k increase connectivity in the graph but lose resolving power for repeats, while larger values of k better resolve repeats but may create gaps in coverage due to sequencing errors.\nThe de Bruijn graph is constructed with nodes representing (k-1)-mers and directed edges representing k-mers. An edge from node u to node v exists if there is a k-mer whose first k-1 bases match u and whose last k-1 bases match v. In this representation, the genome corresponds to an Eulerian path that visits every edge exactly once, which can be found in linear time in the number of edges.\nReal sequencing data creates numerous complications in the idealized de Bruijn graph. Sequencing errors create spurious k-mers that appear as dead-end branches (tips) or low-coverage paths. These must be identified and removed through various graph cleaning procedures. Repeats create bubbles and complex tangles in the graph that may have multiple valid traversals. Heterozygosity creates bubble structures where paths diverge and reconverge.\nPopular de Bruijn graph assemblers like SPAdes and MEGAHIT implement sophisticated strategies to handle these challenges. They typically use multiple k-mer sizes to balance the trade-offs between connectivity and repeat resolution, employ coverage information to identify and correct errors, and use paired-end information to resolve ambiguities in graph traversal.\n\n\nString Graph Assembly\nString graph assembly represents a middle ground between OLC and de Bruijn graph approaches, aiming to combine their respective advantages. The string graph, introduced by Eugene Myers, provides an exact representation of the assembly problem without the information loss inherent in k-mer decomposition.\nIn a string graph, nodes represent reads (or more generally, strings), and edges represent suffix-prefix overlaps. However, unlike the traditional overlap graph, the string graph removes redundant information through a process called transitive reduction. If read A overlaps with read B, and read B overlaps with read C, and the A-to-C overlap is implied by the A-to-B and B-to-C overlaps, then the A-to-C edge is removed as redundant.\nThis reduction dramatically simplifies the graph while preserving all necessary information for assembly. The resulting graph is typically much smaller than a full overlap graph, making it more tractable for large datasets. The assembly problem becomes finding a path through the string graph that visits each node exactly once, similar to finding a Hamiltonian path.\nString graph assemblers like SGA (String Graph Assembler) and modern implementations in assemblers like Hifiasm have shown particular promise for long-read assembly. They can preserve read-level information that is lost in de Bruijn graphs while being more scalable than traditional OLC approaches. The string graph framework also naturally accommodates reads of varying lengths and can elegantly handle both error-prone long reads and accurate short reads in hybrid assembly approaches.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Genome Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/assembly_and_mapping.html#advanced-assembly-strategies",
    "href": "chapters/bioinformatics/assembly_and_mapping.html#advanced-assembly-strategies",
    "title": "35  Genome Sequencing",
    "section": "Advanced Assembly Strategies",
    "text": "Advanced Assembly Strategies\n\nHybrid Assembly\nHybrid assembly leverages multiple data types to overcome the limitations of individual sequencing technologies. The most common approach combines accurate short reads with long, potentially error-prone reads to achieve both high accuracy and long-range continuity.\nOne strategy uses short reads to correct errors in long reads before assembly. Tools like LoRDEC and FMLRC identify k-mers from accurate short reads and use them to correct regions in long reads that contain rare or absent k-mers. The corrected long reads can then be assembled using long-read assemblers, benefiting from improved accuracy while maintaining long-range information.\nAnother approach performs separate assemblies and then merges or scaffolds them. Short reads might be assembled into accurate but fragmented contigs, which are then connected using long-read information. Alternatively, a long-read assembly might provide the backbone structure, with short reads used to polish the consensus sequence and correct remaining errors.\nMore sophisticated hybrid assemblers like MaSuRCA and Unicycler integrate multiple data types throughout the assembly process. They might use short reads to build an initial de Bruijn graph, then use long reads to resolve ambiguities and connect components. This tight integration can produce superior results compared to sequential approaches but requires careful balancing of different data types.\n\n\nDiploid and Polyploid Assembly\nMost assemblers traditionally produced a single consensus sequence, effectively collapsing the multiple copies of each chromosome in diploid or polyploid organisms. However, preserving haplotype information is crucial for understanding genetic variation, inheritance patterns, and allele-specific expression.\nPhased diploid assembly aims to reconstruct both haplotypes separately. This requires distinguishing reads originating from maternal versus paternal chromosomes, a process called phasing. Long reads that span multiple heterozygous sites provide direct phasing information, as variants observed on the same read must come from the same haplotype. Linked-read technologies provide similar long-range information through molecular barcoding.\nGraph-based representations like variation graphs can elegantly represent multiple haplotypes simultaneously. Instead of linear sequences, these graphs encode variation as alternative paths. Assemblers like Shasta and Hifiasm construct assembly graphs that preserve heterozygous regions as bubbles, which can later be phased using additional information.\nPolyploid assembly presents even greater challenges, as distinguishing between homeologs (corresponding chromosomes from different subgenomes) and alleles (variants of the same chromosome) requires careful analysis of sequence divergence patterns. Specialized assemblers like FALCON-Phase and ALLHiC have been developed to handle the complexities of polyploid genomes.\n\n\nMetagenome Assembly\nMetagenomic assembly reconstructs genomes from environmental samples containing multiple organisms, presenting unique challenges beyond single-organism assembly. The organisms in a sample may vary in abundance by several orders of magnitude, creating extreme coverage variation. Closely related strains may differ by only a few SNPs, while distantly related organisms contribute completely distinct sequences.\nMetagenomic assemblers like metaSPAdes and MEGAHIT modify traditional assembly approaches to handle these challenges. They must be sensitive enough to assemble low-abundance organisms while distinguishing between closely related strains. Coverage patterns across contigs help group sequences from the same organism (binning), as sequences from the same genome should have similar abundance.\nStrain-level resolution requires distinguishing between very similar sequences that may differ at only a few positions. Some assemblers maintain strain-level variation as bubbles in the assembly graph rather than forcing a linear consensus. Others use co-abundance patterns across multiple samples to resolve strains, as different strains may vary independently in abundance.\nLong-read metagenomics is particularly promising as longer reads can span strain-specific variations and provide linkage information. However, the higher error rates of long reads can make it challenging to distinguish true strain variation from sequencing errors, requiring careful algorithm design.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Genome Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/assembly_and_mapping.html#read-mapping-principles-and-algorithms",
    "href": "chapters/bioinformatics/assembly_and_mapping.html#read-mapping-principles-and-algorithms",
    "title": "35  Genome Sequencing",
    "section": "Read Mapping: Principles and Algorithms",
    "text": "Read Mapping: Principles and Algorithms\n\nThe Mapping Problem\nRead mapping, also called read alignment, is the process of determining where sequencing reads originate in a reference genome. This is fundamental to resequencing projects where the goal is to identify variations relative to a known reference rather than assembling a genome de novo. The challenge is to align millions or billions of reads quickly and accurately, accounting for both sequencing errors and true biological variation.\nThe mapping problem differs from de novo assembly in several important ways. We have a reference sequence that provides a scaffold for placing reads, eliminating much of the ambiguity inherent in assembly. However, reads must be aligned despite differences from the reference caused by SNPs, insertions, deletions, and sequencing errors. The reference genome may also contain errors or represent a different individual or strain than the one being sequenced.\nMapping algorithms must balance several competing objectives. Speed is crucial when processing billions of reads, but accuracy cannot be sacrificed. The algorithm must be sensitive enough to map reads despite variations and errors but specific enough to avoid incorrect mappings. For reads that map equally well to multiple locations (multi-mapping reads), the algorithm must either choose the best location or report all possibilities.\n\n\nHash-Based Mapping\nEarly read mapping algorithms like BLAST used hash tables to index either the reference genome or the reads. While accurate, these approaches were too slow for the massive datasets generated by high-throughput sequencing. Modern hash-based mappers use various strategies to achieve the necessary speed.\nSeed-and-extend is a common strategy where the mapper first identifies exact matches (seeds) between reads and the reference, then extends these seeds to full alignments. The seeds can be simple k-mers or more sophisticated patterns like spaced seeds that are more sensitive to homologous sequences. The extension phase typically uses dynamic programming for optimal local alignment.\nMinimizer-based indexing reduces memory requirements and improves cache efficiency. Instead of indexing all k-mers, only a subset called minimizers are indexed. A minimizer is the lexicographically smallest k-mer in a window of w consecutive k-mers. This selection ensures that identical sequences will share minimizers while dramatically reducing the index size.\nModern hash-based mappers like minimap2 have proven particularly effective for long reads. They use techniques like chaining to connect multiple seed matches into coherent alignments, accommodating the higher error rates and longer lengths of third-generation sequencing reads. The algorithm identifies colinear chains of anchors (matching k-mers or minimizers) and performs base-level alignment only for the most promising chains.\n\n\nFM-Index and Burrows-Wheeler Transform\nThe Burrows-Wheeler Transform (BWT) and its associated FM-index revolutionized short-read mapping by enabling exact matching in time proportional to the read length rather than the genome size. This remarkable property makes BWT-based mappers extremely efficient for short reads.\nThe BWT is a reversible transformation that rearranges a text to group similar contexts together, improving compressibility and enabling efficient pattern matching. For a genome sequence, the BWT is constructed by considering all rotations of the sequence (with a terminal symbol added), sorting them lexicographically, and taking the last column of the sorted matrix.\nThe FM-index augments the BWT with additional data structures that enable backward search. Given a pattern (read), the search starts from the last character and extends backward, maintaining an interval in the BWT that corresponds to all positions where the pattern occurs. Each character extension narrows this interval until either the pattern is fully matched (the interval size gives the number of occurrences) or the interval becomes empty (no matches).\nBWA (Burrows-Wheeler Aligner) and Bowtie pioneered BWT-based read mapping. They handle mismatches and gaps through various strategies such as backtracking (trying different paths when mismatches are encountered) or seed-and-extend (using exact matching for seeds, then extending with dynamic programming). These tools can map millions of short reads per hour while maintaining high accuracy.\n\n\nSuffix Arrays and Enhanced Suffix Arrays\nSuffix arrays provide another efficient data structure for read mapping. A suffix array is a sorted array of all suffixes of the reference genome. Binary search on the suffix array enables pattern matching in O(m log n) time, where m is the pattern length and n is the genome size. Enhanced suffix arrays add additional information to achieve O(m) search time, matching the theoretical performance of suffix trees with better practical memory usage.\nThe LCP (Longest Common Prefix) array stores the length of the longest common prefix between adjacent suffixes in the sorted order. This information accelerates pattern matching and enables efficient computation of exact matches of varying lengths. Child tables and other auxiliary structures can further improve query performance.\nSuffix array-based mappers like STAR (Spliced Transcripts Alignment to a Reference) excel at split-read mapping, where reads span splice junctions in RNA-seq data. The suffix array enables efficient identification of maximal mappable prefixes—the longest prefix of a read that matches the genome exactly. By iteratively finding maximal mappable prefixes, the algorithm can identify split alignments where different parts of a read map to different genomic locations.\n\n\nHandling Mapping Ambiguity\nMulti-mapping reads that align equally well to multiple genomic locations present significant challenges for downstream analysis. These arise from repetitive sequences, segmental duplications, and paralogous genes. Different strategies exist for handling multi-mappers, each with trade-offs.\nThe simplest approach reports only the best alignment (or a random selection among equally good alignments). This ensures each read contributes at most once to coverage calculations but may bias against repetitive regions. Alternatively, reporting all alignments preserves complete information but complicates downstream analysis and may inflate coverage in repetitive regions.\nProbabilistic assignment methods distribute multi-mapping reads among potential locations based on additional evidence. Local coverage, paired-end constraints, and base quality scores can inform these assignments. Expectation-maximization algorithms iteratively refine the probability distributions, though convergence is not guaranteed to find the global optimum.\nFor RNA-seq and other quantification applications, some tools avoid explicit assignment altogether. Instead, they maintain ambiguity through the analysis, using statistical models to estimate expression levels that are consistent with the observed alignments. This approach can provide more accurate quantification, especially for gene families with similar sequences.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Genome Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/assembly_and_mapping.html#quality-assessment-and-validation",
    "href": "chapters/bioinformatics/assembly_and_mapping.html#quality-assessment-and-validation",
    "title": "35  Genome Sequencing",
    "section": "Quality Assessment and Validation",
    "text": "Quality Assessment and Validation\n\nAssembly Metrics and Evaluation\nAssessing assembly quality requires multiple metrics that capture different aspects of completeness, contiguity, and accuracy. No single metric provides a complete picture, and the relative importance of different metrics depends on the intended use of the assembly.\nContiguity metrics measure how successfully the assembler connected reads into longer sequences. N50, the length such that half the assembly is contained in contigs of this length or longer, is widely used but has limitations. It doesn’t account for misassemblies and can be artificially inflated by aggressive scaffolding. NG50, which normalizes by estimated genome size rather than assembly size, provides a more comparable metric across assemblies.\nCompleteness assessment examines whether all expected genomic content is present. BUSCO (Benchmarking Universal Single-Copy Orthologs) searches for conserved genes expected to be present in single copy. The percentage of complete, fragmented, and missing BUSCOs provides insight into assembly completeness and correctness. For specific projects, domain-specific gene sets may provide more relevant completeness measures.\nAccuracy evaluation requires comparison to a truth set, which may be a high-quality reference genome or long-range experimental data. Tools like QUAST compute various metrics including misassembly breakpoints, where assembled contigs disagree with the reference structure. However, differences from the reference may represent true biological variation rather than assembly errors.\nK-mer-based analysis provides reference-free quality assessment. The k-mer spectrum of reads can be compared to that of the assembly to identify missing or duplicated sequences. Unusual k-mer coverage patterns may indicate collapsed repeats or contamination. Tools like KAT and Merqury provide comprehensive k-mer-based evaluation.\n\n\nMapping Quality and Validation\nMapping quality scores estimate the probability that a read is mapped to the correct location. These phred-scaled scores (MQ = -10 log₁₀ P(error)) are crucial for distinguishing reliable from uncertain alignments. A mapping quality of 30 indicates 99.9% confidence in the mapping location.\nFactors influencing mapping quality include the uniqueness of the mapped region, the number of mismatches or gaps in the alignment, and the presence of alternative mapping locations. Paired-end reads provide additional constraints—both reads should map in the correct orientation and distance, improving mapping confidence.\nValidation of mapping results can use several approaches. Simulated reads with known origins enable direct measurement of mapping accuracy. Spike-in controls with known sequences and quantities provide ground truth for both mapping and quantification. Orthogonal data types, such as long reads or optical mapping, can validate the mapping of challenging regions.\nSystematic biases in mapping must be identified and corrected. GC bias, where reads from high or low GC regions map less efficiently, can skew coverage profiles. Mappability bias occurs when reads from repetitive regions are unmappable or map ambiguously. Understanding and accounting for these biases is crucial for accurate variant calling and quantification.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Genome Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/assembly_and_mapping.html#applications-in-resequencing",
    "href": "chapters/bioinformatics/assembly_and_mapping.html#applications-in-resequencing",
    "title": "35  Genome Sequencing",
    "section": "Applications in Resequencing",
    "text": "Applications in Resequencing\n\nVariant Discovery\nResequencing projects aim to identify genetic variations by comparing newly sequenced individuals to a reference genome. This includes single nucleotide polymorphisms (SNPs), small insertions and deletions (indels), and larger structural variations. The process involves read mapping followed by variant calling algorithms that distinguish true variants from sequencing errors and mapping artifacts.\nSNP calling examines each genomic position to determine whether the observed bases differ from the reference. Statistical models account for base quality scores, mapping quality, and coverage depth to distinguish true variants from errors. Population-based calling jointly analyzes multiple samples, leveraging linkage disequilibrium and allele frequency patterns to improve accuracy.\nIndel calling is more challenging than SNP calling because indels cause alignment ambiguities. The same indel can often be placed at multiple positions in repetitive sequences, and alignment algorithms may represent the same variant differently. Realignment around indel candidates and standardization of variant representation (left-alignment and parsimony) are crucial for accurate indel calling.\nStructural variant detection requires specialized approaches as large variants may not be captured by standard short-read alignment. Split-read mapping identifies reads where parts align to different genomic locations. Discordant paired-end reads, where the insert size or orientation differs from expected, signal structural variants. Read depth analysis can identify copy number variations. Long reads excel at structural variant detection as they can span entire variants and resolve complex rearrangements.\n\n\nPopulation Genomics\nLarge-scale resequencing projects have transformed our understanding of genetic variation within and between populations. Projects like the 1000 Genomes Project, gnomAD, and TOPMed have cataloged millions of variants, providing crucial resources for medical genetics and evolutionary studies.\nPopulation-scale analysis requires efficient computational approaches. Joint variant calling across thousands of samples improves power for rare variant detection but poses computational challenges. Incremental approaches that allow adding new samples without reprocessing existing data are increasingly important. Cloud-based platforms enable distributed processing of massive datasets.\nHaplotype phasing at the population scale leverages patterns of linkage disequilibrium to separate maternal and paternal chromosomes. Statistical phasing algorithms like SHAPEIT and Eagle use population data to phase even short-read data accurately over long distances. This phasing enables haplotype-based analyses including selection scans and identity-by-descent mapping.\nGenotype imputation uses reference panels to infer unobserved variants, enabling integration of datasets generated with different technologies. Modern imputation servers provide easy access to imputation using large reference panels, dramatically increasing the power of genome-wide association studies without additional sequencing costs.\n\n\nClinical Applications\nClinical resequencing has moved from research to routine practice, with applications in cancer genomics, rare disease diagnosis, and pharmacogenomics. Each application has specific requirements for accuracy, turnaround time, and interpretability.\nTumor sequencing presents unique challenges as samples contain mixtures of cancer and normal cells, with the cancer cells potentially harboring multiple subclones. Somatic variant callers must distinguish germline variants present in all cells from somatic mutations present only in cancer cells. The variant allele frequency provides information about tumor purity and clonal structure. Deep sequencing may be required to detect low-frequency subclonal mutations.\nRare disease diagnosis often involves trio sequencing (affected child and both parents) to identify de novo mutations or compound heterozygotes. Rapid turnaround is crucial for acute cases, driving development of streamlined pipelines that can deliver results within days. Interpretation remains challenging, requiring integration of variant databases, functional predictions, and phenotype matching.\nPharmacogenomic testing identifies variants affecting drug metabolism and response. Star allele calling for genes like CYP2D6 is complicated by the presence of gene duplications, deletions, and hybrid genes. Accurate copy number assessment and phasing of variants into star alleles requires specialized tools beyond standard variant calling pipelines.\n\n\nTranscriptome Analysis\nRNA-seq applies resequencing principles to study gene expression, alternative splicing, and RNA editing. The mapping challenge is complicated by the need to align reads across splice junctions and the absence of introns in the reference.\nSplice-aware aligners like STAR and HISAT2 can identify novel splice junctions by split-read mapping. They build indexes that enable efficient mapping across known junctions while maintaining the ability to discover new ones. The challenge is distinguishing true novel junctions from alignment artifacts, particularly in genes with many isoforms.\nQuantification of gene and transcript expression requires careful handling of multi-mapping reads from homologous genes. Tools like Salmon and kallisto use pseudoalignment—determining which transcripts a read is compatible with without full base-level alignment—to achieve rapid quantification. Expectation-maximization algorithms allocate multi-mapping reads probabilistically based on the overall evidence.\nAllele-specific expression analysis combines RNA-seq with genetic variation data to measure expression differences between alleles. This requires phased genotypes and careful mapping that accounts for reference bias—the tendency for reads matching the reference allele to map more readily than those with alternative alleles.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Genome Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/assembly_and_mapping.html#computational-challenges-and-solutions",
    "href": "chapters/bioinformatics/assembly_and_mapping.html#computational-challenges-and-solutions",
    "title": "35  Genome Sequencing",
    "section": "Computational Challenges and Solutions",
    "text": "Computational Challenges and Solutions\n\nScalability and Performance\nThe exponential growth in sequencing data demands continuous improvements in computational efficiency. Modern sequencing projects generate datasets measuring in terabytes, requiring algorithms that scale linearly or better with data size.\nParallel processing is essential for handling large-scale data. Most modern tools support multithreading for shared-memory parallelism on single machines. Distributed computing frameworks like Apache Spark enable processing across clusters, though the overhead of data distribution must be carefully managed. GPU acceleration has shown promise for specific tasks like base-calling and alignment, offering substantial speedups for suitable algorithms.\nMemory efficiency is often the limiting factor for genome-scale analysis. Succinct data structures like the FM-index and minimizer-based indexing reduce memory requirements while maintaining query performance. Streaming algorithms that process data in a single pass eliminate the need to store entire datasets in memory. External memory algorithms use disk storage for datasets too large for RAM, though careful design is needed to minimize I/O overhead.\nAlgorithmic improvements continue to drive performance gains. Heuristics that quickly eliminate unlikely solutions reduce the search space. Approximate algorithms trade small accuracy losses for substantial speed improvements. Machine learning approaches learn patterns from data to guide algorithmic decisions, though care must be taken to ensure generalization to new datasets.\n\n\nError Correction and Quality Control\nSequencing errors remain a fundamental challenge despite improvements in technology. Error correction strategies must balance removing errors while preserving true biological variation.\nK-mer-based error correction identifies rare k-mers likely to represent errors. By examining k-mer frequencies across all reads, algorithms can identify and correct errors that create unique or low-frequency k-mers. The challenge is setting appropriate frequency thresholds that remove errors without eliminating real variants from low-coverage regions or rare haplotypes.\nConsensus-based correction leverages multiple observations of the same genomic region. For high-coverage short-read data, simple voting can identify the likely correct base at each position. For long reads, self-correction uses overlaps between noisy long reads to generate consensus sequences. The challenge is handling systematic errors that affect multiple reads similarly.\nQuality score recalibration improves the accuracy of base quality scores, which are crucial for downstream analysis. Machine learning models can learn patterns of errors specific to each sequencing run and adjust quality scores accordingly. Covariates such as sequence context, position in the read, and machine cycle influence error rates and can be incorporated into recalibration models.\n\n\nData Integration and Multi-Omics\nModern genomic studies increasingly integrate multiple data types to provide comprehensive biological insights. This integration poses computational and statistical challenges.\nVertical integration combines different molecular measurements from the same samples—genome sequencing, RNA-seq, ChIP-seq, and methylation data. Each data type has distinct characteristics and error profiles. Integration methods must account for technical differences while identifying biological correlations. Dimensionality reduction techniques and multi-view learning approaches help identify shared patterns across data types.\nHorizontal integration combines the same data type across multiple conditions, time points, or individuals. Batch effects—systematic differences between datasets generated at different times or locations—must be identified and corrected. Normalization methods attempt to remove technical variation while preserving biological signal.\nGraph-based integration represents relationships between different biological entities as networks. Genome graphs encode genetic variation, while gene regulatory networks capture interactions between genes. Integrating these networks with experimental data enables systems-level understanding of biological processes.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Genome Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/assembly_and_mapping.html#future-directions-and-emerging-technologies",
    "href": "chapters/bioinformatics/assembly_and_mapping.html#future-directions-and-emerging-technologies",
    "title": "35  Genome Sequencing",
    "section": "Future Directions and Emerging Technologies",
    "text": "Future Directions and Emerging Technologies\n\nTelomere-to-Telomere Assembly\nRecent achievements in complete, telomere-to-telomere (T2T) assembly of human chromosomes represent a major milestone. The T2T consortium’s complete assembly of the CHM13 human genome filled gaps that persisted since the original Human Genome Project, including centromeres, satellite arrays, and segmental duplications.\nThese achievements required combining multiple technologies: ultra-long nanopore reads spanning hundreds of kilobases, accurate PacBio HiFi reads providing base-level accuracy, and Hi-C data confirming long-range structure. Assembly algorithms had to be adapted to handle the unique challenges of satellite DNA and other complex repeats.\nThe methods developed for T2T assembly are being applied to other organisms and to diploid human genomes. The challenge now is to make T2T assembly routine rather than heroic, enabling complete genome assembly for diverse species and populations.\n\n\nPangenome References\nThe limitations of using a single linear reference genome are increasingly recognized. Genetic variation means that any individual may have sequences absent from the reference, and population-specific variants may be systematically missed when using a reference from a different population.\nPangenome references aim to capture the full genetic diversity of a species. Graph-based references represent variation as alternative paths through a genome graph. Each path represents a possible haplotype, and new sequences can be mapped to the graph structure rather than a linear reference.\nComputational challenges include efficiently indexing and querying genome graphs, which are substantially more complex than linear sequences. Visualization and interpretation of graph-based results require new tools and conceptual frameworks. Standards for representing and exchanging genome graphs are still evolving.\n\n\nReal-Time and Portable Sequencing\nNanopore sequencing’s portability and real-time data generation enable new applications. The MinION device, small enough to fit in a pocket, has been used for field sequencing in remote locations, real-time pathogen identification during disease outbreaks, and even DNA sequencing on the International Space Station.\nReal-time analysis requires streaming algorithms that can process data as it’s generated. Base-calling, quality control, and initial analysis must keep pace with data generation. Edge computing approaches perform analysis on the sequencing device or local computers rather than requiring cloud resources.\nApplications in clinical diagnostics demand rapid turnaround from sample to answer. Streamlined workflows that integrate sample preparation, sequencing, and analysis are being developed. The challenge is maintaining accuracy and completeness while minimizing time to result.\n\n\nMachine Learning and AI\nArtificial intelligence is transforming multiple aspects of genome sequencing and analysis. Deep learning models improve base-calling accuracy for both short and long reads. Convolutional neural networks learn to recognize patterns in signal data that correlate with specific sequences.\nAssembly algorithms increasingly use machine learning to make decisions about graph traversal and repeat resolution. Models trained on high-quality assemblies can guide assembly of new genomes. Learned heuristics can dramatically reduce the search space for optimal solutions.\nVariant calling benefits from machine learning approaches that integrate multiple features to distinguish true variants from artifacts. Deep learning models can learn complex patterns that are difficult to capture with hand-crafted rules. The challenge is ensuring models generalize well to new datasets and don’t perpetuate biases present in training data.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Genome Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/assembly_and_mapping.html#conclusion",
    "href": "chapters/bioinformatics/assembly_and_mapping.html#conclusion",
    "title": "35  Genome Sequencing",
    "section": "Conclusion",
    "text": "Conclusion\nThe fields of genome sequencing, assembly, and read mapping have undergone remarkable transformation over the past two decades. From the years-long effort to sequence the first human genome to routine sequencing of thousands of genomes, technological and computational advances have democratized genomics.\nThe evolution of sequencing technologies from Sanger to current long-read platforms has continuously presented new computational challenges while enabling previously impossible analyses. Each generation of technology has required new algorithmic approaches, from overlap-layout-consensus for long Sanger reads, to de Bruijn graphs for short reads, to string graphs and sophisticated error correction for noisy long reads.\nAssembly algorithms have evolved from simple greedy approaches to sophisticated methods that model the complex structure of real genomes. Modern assemblers must handle polyploidy, metagenomes, and contamination while producing accurate, contiguous assemblies. The achievement of telomere-to-telomere assemblies represents a major milestone, though challenges remain in making such complete assemblies routine.\nRead mapping has progressed from slow alignment algorithms to methods that can map billions of reads in hours. The development of the FM-index and other efficient data structures enabled the analysis of population-scale datasets. Handling mapping ambiguity, particularly for repetitive regions and multi-mapping reads, remains an active area of research.\nThe applications of resequencing continue to expand, from basic research to clinical practice. Large-scale population studies have cataloged human genetic variation, enabling precision medicine approaches. Clinical sequencing is becoming routine for cancer treatment and rare disease diagnosis. Agricultural applications use resequencing to accelerate crop improvement and understand adaptation.\nLooking forward, several trends will shape the field. Long-read technologies will continue to improve in accuracy and throughput while decreasing in cost. Real-time, portable sequencing will enable new applications in field research and point-of-care diagnostics. Pangenome references will better represent species diversity than single linear references. Machine learning will increasingly guide algorithmic decisions and extract patterns from massive datasets.\nThe computational challenges will continue to evolve with the technology. As sequencing becomes cheaper and more ubiquitous, the bottleneck shifts from data generation to analysis and interpretation. Efficient algorithms, scalable infrastructure, and user-friendly tools will be crucial for democratizing genomic analysis. Integration of genomic data with other omics data types will require new computational frameworks.\nThe journey from the first genome assemblies to routine clinical sequencing exemplifies the power of interdisciplinary collaboration between biology, computer science, mathematics, and engineering. As we continue to push the boundaries of what’s possible in genome sequencing and analysis, these collaborations will remain essential for transforming raw sequence data into biological understanding and medical breakthroughs.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Genome Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/project/assembly_project/index.html",
    "href": "chapters/project/assembly_project/index.html",
    "title": "36  Genome assembly",
    "section": "",
    "text": "Read and analyze the sequencing reads\nThis chapter is a programming project where you will assemble a small genomic sequence from a set of short sequencing reads.\nIn genome assembly, many short sequences (reads) from a sequencing machine are assembled into long sequences – ultimately chromosomes. This is done by ordering overlapping reads so that they together represent genomic sequences. For example, given these three reads: AGGTCGTAG, CGTAGAGCTGGGAG, GGGAGGTTGAAA, ordering them based on their overlap like this\nproduces the following genomic sequence:\nReal genome assembly is, of course, more sophisticated than what we do here, but the idea is the same. To limit the complexity of the problem, we make two simplifying assumptions:\nThe second assumption implies that overlaps are always of this type:\nand never of this type:\nIn this project, you will be asked to write functions that solve the problem of assembling a genomic sequence. Each function solves a small problem, and you may need to call these functions inside other functions to put together solutions to larger subproblems.\nDownload the files you need for this project:\nhttps://munch-group.org/bioinformatics/supplementary/project_files\nYou also need to download the two project files:\nPut the files in a folder dedicated to this project. On most computers you can right-click on the link and choose “Save file as…” or “Download linked file”.\nNow open each file in your editor and look at what is in sequencing_reads.txt. (Do not change it in any way, and do not save it after viewing. If your editor asks you if you want to save it before closing, say no.) How many sequences are there in each file?\nThe project is split into four parts:\nHere is an overview of the functions you will write in each part of the project and of which functions are used by other functions.\nMake sure to read the entire exercise and understand what you are supposed to do before you begin!\nThe first task is to read and parse the input data. The sequence reads for the mini-assembly are in the file sequencing_reads.txt. The first two lines of the file look like this:\nEach line represents a read. The first field on each line is the name of the read, and the second field is the read sequence itself. So for the first line, Read1 is the name, and ATGCG... is the sequence.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Genome assembly</span>"
    ]
  },
  {
    "objectID": "chapters/project/assembly_project/index.html#read-and-analyze-the-sequencing-reads",
    "href": "chapters/project/assembly_project/index.html#read-and-analyze-the-sequencing-reads",
    "title": "36  Genome assembly",
    "section": "",
    "text": "Read1 GGCTCCCCACGGGGTACCCATAACTTGACAGTAGATCTCGTCCAGACCCCTAGC\nRead2 CTTTACCCGGAAGAGCGGGACGCTGCCCTGCGCGATTCCAGGCTCCCCACGGG\n\n\nRead the sequencing reads into your program\nWrite a function, read_data, that takes one argument:\n\nA string, which is the name of the data file.\n\nThe function must return\n\nA dictionary, where the keys are the names of reads and the values are the associated read sequences. Both keys and values must be strings.\n\nExample usage:\nread_data('sequencing_reads.txt')\nshould return a dictionary with the following content (maybe not with key-value pairs in that order)\n{'Read1': 'GGCTCCCCACGGGGTACCCATAACTTGACAGTAGATCTCGTCCAGACCCCTAGC',\n 'Read3': 'GTCTTCAGTAGAAAATTGTTTTTTTCTTCCAAGAGGTCGGAGTCGTGAACACATCAGT',\n 'Read2': 'CTTTACCCGGAAGAGCGGGACGCTGCCCTGCGCGATTCCAGGCTCCCCACGGG',\n 'Read5': 'CGATTCCAGGCTCCCCACGGGGTACCCATAACTTGACAGTAGATCTC',\n 'Read4': 'TGCGAGGGAAGTGAAGTATTTGACCCTTTACCCGGAAGAGCG',\n 'Read6': 'TGACAGTAGATCTCGTCCAGACCCCTAGCTGGTACGTCTTCAGTAGAAAATTGTTTTTTTCTTCCAAGAGGTCGGAGT'}\nHere is some scaffolding code to get you started:\ndef read_data(file_name):\n    input_file = open(file_name)\n    # ...\n    for line in input_file:\n         # ...    \n    # ...\n    \n    input_file.close() \n    \nThe line variable in the for loop holds each line in the file, including the \\n newline character at the end. You can use the’ split’ method of strings to split each line into the name of the read and the read sequence. You can see the documentation for that method by typing pydoc str.split in your terminal.\n\n\nCompute the mean length of reads\nAfter writing that function, we would like to get an idea about the length of the reads. There are often too many reads to look at manually, so we need to make a function that computes the mean length of the reads.\nWrite a function, mean_length, that takes one argument:\n\nA dictionary, in which keys are read names and values are read sequences (this is a dictionary like that returned by read_data).\n\nThe function must return\n\nA float, which is the average length of the sequence reads.\n\nOne way to do this is to loop over the keys in the dictionary like this:\ndef mean_length(reads):\n    count = 0\n    total = 0\n    for name in reads:\n        seq = reads[name]\n        # ...\n        # ...\n    # ...\n    return total / count\nRemember that you can use the len function to find the length of a read.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Genome assembly</span>"
    ]
  },
  {
    "objectID": "chapters/project/assembly_project/index.html#compute-overlaps-between-reads",
    "href": "chapters/project/assembly_project/index.html#compute-overlaps-between-reads",
    "title": "36  Genome assembly",
    "section": "Compute overlaps between reads",
    "text": "Compute overlaps between reads\nThe next step is to determine which reads overlap each other. We need a function that takes two read sequences and computes their overlap to do that. Remember that in the input data, none of the reads are completely nested in another read.\n\nCompute the overlap between two reads\nWe know there are no sequencing errors so that the sequence match will be perfect in the overlap. To compute the overlap between the 3’ (right) end of the left read with the 5’ (left) end of the righthand read, you need to loop over all possible overlaps, honoring that one sequence is the left one and the other is the right one. In the for loop, start with the largest possible overlap ( min(len(left), len(right))) and evaluate smaller and smaller overlaps until you find an exact match.\nWrite a function, get_overlap, that takes two arguments\n\nA string, which is the lefthand read sequence.\nA string, which is the righthand read sequence.\n\nThe function must return\n\nA string, which is the overlapping sequence. If there is no overlap, it should return an empty string.\n\nExample usage:\ns1 = \"CGATTCCAGGCTCCCCACGGGGTACCCATAACTTGACAGTAGATCTC\"\ns2 = \"GGCTCCCCACGGGGTACCCATAACTTGACAGTAGATCTCGTCCAGACCCCTAGC\"\nget_overlap(s1, s2)\nshould return the string\n'GGCTCCCCACGGGGTACCCATAACTTGACAGTAGATCTC'\nand get_overlap(s2, s1)\nshould return the string\n'C'\nFrom these two examples, it seems that s1 and s2 overlap and that s1 is the left and s2 is the right. Treating s2 as the left one and s1 as the right one only gives an overlap of one base (we expect a few bases of overlap even for unrelated sequences).\n\n\n\n\nCompute all read overlaps\nWhen you have written get_overlap, you can use it to evaluate the overlap between all pairs of reads in both left-right and right-left orientations.\nWrite a function, get_all_overlaps, that takes one argument:\n\nA dictionary with read data as returned by read_data.\n\nThe function must return\n\nA dictionary of dictionaries specifying the number of overlapping bases for a pair of reads in a specific left-right orientation. Computing the overlap of a read to itself is meaningless and must not be included. Assuming the resulting dictionary of dictionaries is called d, then d['Read2'] will be a dictionary where keys are the names of reads that have an overlap with read 'Read2' when 'Read2' is put in the left position, and the values for these keys are the number of overlapping bases for those reads.\n\nExample usage: assuming that reads is a dictionary returned by read_data then:\nget_all_overlaps(reads)\nshould return the following dictionary of dictionaries (but not necessarily with the same ordering of the key-value pairs):\n{'Read1': {'Read3': 0, 'Read2': 1, 'Read5': 1, 'Read4': 0, 'Read6': 29},\n 'Read3': {'Read1': 0, 'Read2': 0, 'Read5': 0, 'Read4': 1, 'Read6': 1},\n 'Read2': {'Read1': 13, 'Read3': 1, 'Read5': 21, 'Read4': 0, 'Read6': 0},\n 'Read5': {'Read1': 39, 'Read3': 0, 'Read2': 1, 'Read4': 0, 'Read6': 14},\n 'Read4': {'Read1': 1, 'Read3': 1, 'Read2': 17, 'Read5': 2, 'Read6': 0},\n 'Read6': {'Read1': 0, 'Read3': 43, 'Read2': 0, 'Read5': 0, 'Read4': 1}}\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\nYou can use the get_overlap function you just made to find the overlap between a pair of reads. To generate all combinations of reads, you need two for-loops. One looping over reads in the left position, and another (inside the first one) looping over reads in the right position. Remember that we do not want the overlap of a read to itself, so there should be an if-statement checking that the left and right reads are not the same.\n\n\n\n\n\nPrint overlaps as a nice table\nThe dictionary returned by get_all_overlaps is a bit messy. We want to print it in a nice matrix-like format so we can better see which pairs overlap in which orientations.\nThis pretty_print function should take one argument:\n\nA dictionary of dictionaries as returned by get_all_overlaps.\n\nThe function should not return anything but must print a matrix exactly as shown in the example below with nicely aligned and right-justified columns. The first column must hold names of reads in left orientation. The top row holds names of reads in right orientation. The remaining cells must each have the number of overlapping bases for a left-right read pair. The diagonal corresponds to overlaps with the read itself. You must put dashes in these cells.\nExample usage: assuming that overlaps is a dictionary of dictionaries returned by get_all_overlaps then:\npretty_print(overlaps)\nshould print exactly\n       Read1  Read2  Read3  Read4  Read5  Read6 \nRead1      -      1      0      0      1     29 \nRead2     13      -      1      0     21      0\nRead3      0      0      -      1      0      1\nRead4      1     17      1      -      2      0 \nRead5     39      1      0      0      -     14 \nRead6      0      0     43      1      0      -\nThis function is hard to get completely right. So, to spare you the frustration, this one is on me:\ndef pretty_print(d):\n    print('      ', end='')\n    for j in sorted(d):\n        print(\"{: &gt;6}\".format(j), end='')\n    print()\n    for i in sorted(d):\n        print(\"{: &gt;6}\".format(i), end='')\n        for j in sorted(d):\n            if i == j:\n                s = '     -'\n            else:\n                s = \"{: &gt;6}\".format(d[str(i)][str(j)])\n            print(s, end='')\n        print()\nMake sure you understand how it works. You can look up in the documentation what \"{: &gt;6}\".format(i) does.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Genome assembly</span>"
    ]
  },
  {
    "objectID": "chapters/project/assembly_project/index.html#find-the-correct-order-of-reads",
    "href": "chapters/project/assembly_project/index.html#find-the-correct-order-of-reads",
    "title": "36  Genome assembly",
    "section": "Find the correct order of reads",
    "text": "Find the correct order of reads\nNow that we know how the reads overlap, we can chain them pair by pair from left to right to get the order in which they represent the genomic sequence. To do this, we take the first (left-most) read and identify which read has the largest overlap at its right end. Then we take that read and find the read with the largest overlap to the right end of that - until we reach the rightmost (last) read.\n\nFind the first read\nThe first thing you need to do is to identify the first (leftmost) read so we know where to start. This read is identified as the one that has no significant (&gt;2) overlaps to its left end (it only has a good overlap when positioned to the left of other reads). In the example output from pretty_print above, the first read would be read 'Read4' because the 'Read4' column has no significant overlaps (no one larger than two).\nWe break the problem in two and first write a function that gets all the overlaps to the left end of a read (i.e., when it is in the right position):\nWrite a function, get_left_overlaps, that takes two arguments:\n\nA dictionary of dictionaries as returned from get_all_overlaps.\nA string, which represents the name of a read.\n\nThe function must return\n\nA sorted list of integers, which represent the overlaps of other reads to its left end.\n\nExample usage: assuming that overlaps is a dictionary of dictionaries returned by get_all_overlaps then.\nget_left_overlaps(overlaps, 'Read1')\nshould return\n[0, 0, 1, 13, 39]\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\nOnce you have made a list of left overlaps, you can use the built-in function sorted to make a sorted version of the list that you can return from the function.\n\n\n\nOK, now that we have a function that can find all the overlaps to the left end of a given read, all we need to do is find the particular read with no significant (&gt;2) overlaps to its left end.\nWrite a function, find_first_read, that takes one argument:\n\nA dictionary of dictionaries as returned from get_all_overlaps.\n\nThe function must return\n\nA string containing the name of the first read.\n\nExample usage: assuming that overlaps is a dictionary of dictionaries returned by get_all_overlaps then.\nfind_first_read(overlaps)\nshould return\n'Read4'\n\n\nFind the order of reads\nNow that we have the first read, we can find the correct order of the reads. We want a list of the read names in the right order.\nGiven the first (left) read, the next read is the one that has the largest overlap to the right end of that read. We use our dictionary of overlaps to figure out which read that is. If the first read is 'Read4', then overlaps['Read4'] is a dictionary of reads with overlap to the right end of read 'Read4'. So, to find the name of the read with the largest overlap, you must write a function that finds the key associated with the largest value in a dictionary. We do that first:\nWrite a function, find_key_for_largest_value, that takes one argument:\n\nA dictionary.\n\nThe function must return the key associated with the largest value in the dictionary argument.\nHaving written find_key_for_largest_value, you can use it as a tool in the function that finds the order of reads:\nWrite a function, find_order_of_reads, that takes two arguments:\n\nA string, which is the name of the first (left-most) read (that is returned by find_first_read).\nA dictionary of dictionaries of all overlaps (that returned by get_all_overlaps).\n\nThe function must return\n\nA list of strings, which are read names in the order in which they represent the genomic sequence.\n\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\nYou know the first read is given by the first argument to the function. You also know that you can find the next read in the chain of overlapping reads by using the find_key_for_largest_value function. You should keep adding reads to the chain as long as the overlap is larger than two (you can use a for loop with an if statement inside to check that the overlap is larger than 2).\n\n\n\nExample usage: assuming that overlaps is a dictionary of dictionaries returned by get_all_overlaps then:\nfind_order_of_reads('Read4', overlaps)\nshould return:\n['Read4', 'Read2', 'Read5', 'Read1', 'Read6', 'Read3']\nBefore you implement the function, make sure you understand why this is the right list of read names.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Genome assembly</span>"
    ]
  },
  {
    "objectID": "chapters/project/assembly_project/index.html#reconstruct-the-genomic-sequence",
    "href": "chapters/project/assembly_project/index.html#reconstruct-the-genomic-sequence",
    "title": "36  Genome assembly",
    "section": "Reconstruct the genomic sequence",
    "text": "Reconstruct the genomic sequence\nNow that you have the number of overlapping bases between reads and the correct order of the reads, you can reconstruct the genomic sequence.\n\nReconstruct the genomic sequence from the reads\nWrite a function, reconstruct_sequence, that takes three arguments:\n\nA list of strings, which are the names of reads in the order identified by find_order_of_reads.\nA dictionary, with read data as returned from read_data.\nA dictionary of dictionaries with overlaps as returned from get_all_overlaps.\n\nThe function must return\n\nA string, which is the genomic sequence.\n\nExample usage: assuming that order is the list of strings returned by find_order_of_reads, that reads is the dictionary returned by read_data and that overlaps is a dictionary of dictionaries returned by get_all_overlaps then:\nreconstruct_sequence(order, reads, overlaps)    \nshould return one long DNA string (I had to break it in three to make it fit on the page):\nTGCGAGGGAAGTGAAGTATTTGACCCTTTACCCGGAAGAGCGGGACGCTGCCCTGCGCGATT\nCCAGGCTCCCCACGGGGTACCCATAACTTGACAGTAGATCTCGTCCAGACCCCTAGCTGGTA\nCGTCTTCAGTAGAAAATTGTTTTTTTCTTCCAAGAGGTCGGAGTCGTGAACACATCAGT\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\nIterate over the reads in order and use the overlap information to extract and join the appropriate parts of the reads.\n\n\n\n\n\nPutting the whole thing together\nNow that you have written functions to handle each step, you can write one last function that uses them to complete the entire assembly.\nWrite a function, assemble_genome, that takes one argument:\n\nA string, which is the name of a file with sequencing reads in the format described at the beginning of this project description.\n\nThe function must return\n\nA string, which is the genome assembled from the sequencing reads\n\nExample usage:\nassemble_genome('sequencing_reads.txt')\nshould return the assembled genome:\nTGCGAGGGAAGTGAAGTATTTGACCCTTTACCCGGAAGAGCGGGACGCTGCCCTGCGCGATT\nCCAGGCTCCCCACGGGGTACCCATAACTTGACAGTAGATCTCGTCCAGACCCCTAGCTGGTA\nCGTCTTCAGTAGAAAATTGTTTTTTTCTTCCAAGAGGTCGGAGTCGTGAACACATCAGT",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Genome assembly</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/pairwise_alignment.html",
    "href": "chapters/bioinformatics/pairwise_alignment.html",
    "title": "37  Pairwise Alignment",
    "section": "",
    "text": "The alignment problem\nThis chapter is about finding the best way to line up two biological sequences to reveal their similarities and differences. Alignment is fundamental to understanding how sequences are related evolutionarily and what functions they might perform.\nImagine you are looking at two DNA sequences, and you want to figure out if they are related. Maybe they are the same gene from two different species, or maybe they are two different versions of a gene in the same population. How do you compare them? You could just look at them and see if they look similar, but that is not very precise. What you really want is a systematic way to line them up so you can see exactly where they match and where they differ. This is the problem of sequence alignment, and it is one of the most fundamental operations in all of bioinformatics. The question seems simple enough: given two biological sequences, what is the best way to align them from beginning to end to reveal their similarities and differences? But as you will see, this apparently simple question leads to some fascinating computational challenges and beautiful algorithmic solutions. The alignment problem is deceptively tricky because sequences evolve over time through mutations, insertions, and deletions, so two sequences that descended from the same ancestral sequence may now look quite different.\nSeq1: ATGCGATCG-A\n      |||·| ||| |\nSeq2: ATGGG-TCGGA\n\nMatches: 7, Mismatches: 2, Gaps: 2\n\n\n\nFigure 37.1\nThe problem of sequence alignment arises from a fundamental principle of molecular evolution. Biological sequences—whether DNA, RNA, or protein sequences—evolve through time via mutations. These mutations include substitutions, where one nucleotide or amino acid is replaced by another, and insertions and deletions (collectively called indels), where material is added or removed from the sequence. When we observe two sequences today, they may have diverged from a common ancestor millions or even billions of years ago, and the evolutionary processes that have shaped these sequences leave traces in the form of similarities and differences that alignment algorithms seek to uncover. By finding the optimal alignment between sequences, we can infer their evolutionary relationship, identify functionally important regions that have been conserved over evolutionary time, and predict the structure and function of unknown sequences based on their similarity to sequences we already understand. The better we can align sequences, the better we can understand their biology. So getting alignment right really matters.\nGlobal alignment, as distinguished from local alignment, seeks to align entire sequences from their first position to their last. This approach makes sense when comparing sequences that you expect to be similar over their entire length, such as homologous proteins from closely related species or different versions (alleles) of the same gene within a population. When you perform a global alignment, you are making the assumption that the sequences correspond to each other from beginning to end, and your job is to figure out the best way to match them up, inserting gaps where necessary to accommodate insertions and deletions. The requirement to align complete sequences introduces unique computational challenges, as the algorithm must consider all possible ways of introducing gaps to accommodate insertions and deletions while maintaining the overall correspondence between sequences. As you will see, the number of possible alignments grows astronomically with sequence length, so we need clever algorithms to find the best alignment without having to explicitly consider every single possibility.\nThe development of efficient algorithms for global sequence alignment represents one of the early triumphs of computational biology. The application of dynamic programming to this problem, first introduced by Needleman and Wunsch in 1970, provided an elegant solution that guarantees finding the optimal alignment while completely avoiding the need to enumerate the astronomical number of possible alignments that a brute-force approach would require. This algorithmic innovation not only solved a practical problem—allowing biologists to align sequences that would otherwise be impossible to compare systematically—but also established a computational paradigm that has been applied to numerous other problems in bioinformatics and beyond. The dynamic programming approach to sequence alignment is a beautiful example of how the right algorithm can transform an apparently intractable problem into one that can be solved efficiently. As you work through this chapter, you will see exactly how this transformation works, and hopefully you will develop an appreciation for the elegance of the solution.\nWhy does all of this matter? Because sequence alignment is not just an abstract computational problem—it is a fundamental tool that underlies much of modern biology and medicine. Global sequence alignment is essential to evolutionary biology, providing quantitative measures of sequence similarity that can be used to infer phylogenetic relationships and understand how species are related. When you align sequences from different species, you can identify conserved regions that have been maintained by natural selection because they are functionally important, and you can identify variable regions that have accumulated neutral or adaptive mutations over evolutionary time. One of the most common applications is functional annotation—inferring the function of a newly sequenced gene or protein based on its similarity to sequences of known function. The logic is straightforward: proteins with similar sequences typically fold into similar three-dimensional structures and perform similar biochemical functions, so if you find that your unknown sequence aligns well with a protein of known function, that is strong evidence that they share that function. In medical genetics, sequence alignment plays a crucial role in identifying disease-causing mutations by aligning a patient’s gene sequence with a reference sequence to identify variations that may be pathogenic. In the era of whole-genome sequencing, alignment algorithms help us compare entire genomes to reveal patterns of conserved gene order, identify corresponding genes in different species, and understand the evolutionary forces that have shaped genome structure. So as you learn about the algorithms and mathematics of sequence alignment in this chapter, keep in mind that these tools have very real applications in understanding the biology of life and in improving human health.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Pairwise Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/pairwise_alignment.html#why-alignment-is-computationally-hard",
    "href": "chapters/bioinformatics/pairwise_alignment.html#why-alignment-is-computationally-hard",
    "title": "37  Pairwise Alignment",
    "section": "Why alignment is computationally hard",
    "text": "Why alignment is computationally hard\nYou might think that aligning two sequences would be straightforward. After all, if you just want to line them up and see how similar they are, why not just do it? The problem is that there are so many different ways to align two sequences that trying them all would take longer than the age of the universe—even for sequences that are not particularly long. Let me show you why. If you have two sequences and you are not allowed to insert any gaps, then there is only one way to align them: position one in the first sequence lines up with position one in the second sequence, position two with position two, and so on. Easy. But biological sequences do not work like that. Evolution introduces insertions and deletions, so if you want to find the true evolutionary relationship between two sequences, you need to allow gaps. And once you allow gaps, the number of possible alignments explodes.\nThink about it this way. For two sequences of lengths m and n, every possible alignment corresponds to a way of inserting gaps into both sequences such that they end up the same length and then lining them up. How many ways can you do that? It turns out that the number of possible global alignments can be expressed using a formula involving binomial coefficients, and while I could write out the formula for you, what really matters is how fast this number grows. Even for relatively short sequences of 100 nucleotides or amino acids each, the number of possible alignments exceeds 10^60. To put that in perspective, the observable universe contains roughly 10^80 atoms, so we are talking about a number that is not unimaginably far from that scale. This combinatorial explosion means that any algorithm that tries to enumerate and evaluate all possible alignments will fail spectacularly for sequences of any biologically relevant length. If you tried to check all possible alignments of two 100-residue sequences, evaluating one alignment per nanosecond, you would still be working on the problem billions of years after the heat death of the universe. So clearly, we need a smarter approach.\n\nVisualizing the alignment space\nHere is a helpful way to think about the alignment problem. Imagine a graph where each path from the starting point to the ending point represents one possible alignment. In this graph, you start at position \\((0,0)\\), which represents the beginning of both sequences. From there, you can move in three directions. You can move diagonally, which represents aligning one nucleotide or amino acid from each sequence. You can move horizontally, which represents putting a gap in the vertical sequence (so you advance in the horizontal sequence but not the vertical one). Or you can move vertically, which represents putting a gap in the horizontal sequence. Every path through this graph, from the start to the end, corresponds to one valid alignment. The total number of paths through this graph—that is, the number of possible alignments—is astronomical, as we just saw. But notice something important: while the number of paths is enormous, the graph itself has a very regular structure.\nThis regular structure is the key to solving the problem efficiently. Every node in the graph represents a partial alignment—a state where you have aligned some prefix of the first sequence with some prefix of the second sequence. From each node, you have only three choices for how to extend the alignment: match or mismatch (diagonal move), insert a gap in one sequence (horizontal move), or insert a gap in the other sequence (vertical move). The graph representation reveals that although there are exponentially many paths through the graph, there are only a polynomial number of nodes. Specifically, if the sequences have lengths \\(m\\) and \\(n\\), there are only \\((m+1) \\times (n+1)\\) possible nodes in the graph. This observation is crucial. It means that while we cannot afford to enumerate all possible paths (alignments), we can afford to visit all possible nodes (partial alignments). This is the insight that makes dynamic programming work: we can systematically explore the alignment space in a way that guarantees finding the optimal path without explicitly enumerating every single path. The key is to recognize that many different paths pass through the same nodes, and we can exploit this overlap to avoid redundant work.\n\n\n\n\n\n\n\n\nFigure 37.2: The alignment graph for aligning sequences ‘ATC’ and ‘AGC’. Each path from (0,0) to (3,3) represents a possible alignment. Diagonal moves align characters (match/mismatch), horizontal moves insert a gap in the vertical sequence, and vertical moves insert a gap in the horizontal sequence. Two example paths are shown in different colors.\n\n\n\n\n\n\n\nHow fast is fast enough?\nNow that we have established that exhaustive search is hopeless, let me tell you the good news. The dynamic programming algorithm for sequence alignment, which we will explore in detail later, has a time complexity of \\(O(mn)\\) for two sequences of lengths \\(m\\) and \\(n\\). What does this mean? It means that the time required to align two sequences grows as the product of their lengths. This is quadratic growth, which is vastly better than the exponential growth of exhaustive search. If you double the length of both sequences, the time required goes up by a factor of four. That is manageable. For protein sequences, which typically have a few hundred amino acids, and for shorter DNA sequences, the \\(O(mn)\\) complexity is very manageable on modern computers. You can align two sequences of length 1000 in a fraction of a second. The space complexity is also \\(O(mn)\\), meaning that the amount of memory you need grows quadratically with sequence length as well.\nOf course, quadratic complexity is not perfect. For genomic sequences that can contain millions or billions of base pairs, this scaling behavior does start to present challenges. Aligning two sequences of length one million would require a matrix with a trillion cells, which starts to push the limits of memory on typical computers. This is why, when we need to align very long sequences like entire chromosomes or genomes, we often turn to heuristic methods that trade off guaranteed optimality for speed. But for most everyday bioinformatics tasks involving genes and proteins, the \\(O(mn)\\) complexity of the Needleman-Wunsch algorithm is perfectly acceptable. It is worth noting, however, that when you extend the alignment problem to more than two sequences—that is, multiple sequence alignment—the problem becomes significantly harder. In fact, multiple sequence alignment with arbitrary scoring schemes has been proven to be NP-hard, which means that it is unlikely that we will ever find an algorithm that can solve it efficiently for large numbers of sequences. This theoretical result underscores the fundamental difficulty of alignment and explains why multiple sequence alignment remains an active area of research where people continue to develop better heuristics and approximation algorithms.\n\n\n\n\n\n\n\n\nFigure 37.3: Comparison of exponential (exhaustive search) versus quadratic (dynamic programming) time complexity for sequence alignment. The number of operations required for exhaustive search grows astronomically with sequence length, while the O(mn) dynamic programming approach remains practical even for moderately long sequences. The shaded region indicates the practical range for typical computers.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Pairwise Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/pairwise_alignment.html#counting-all-possible-alignments",
    "href": "chapters/bioinformatics/pairwise_alignment.html#counting-all-possible-alignments",
    "title": "37  Pairwise Alignment",
    "section": "Counting all possible alignments",
    "text": "Counting all possible alignments\nLet me make the magnitude of the alignment problem more concrete by showing you exactly how many possible alignments exist for sequences of different lengths. The number of global alignments between two sequences can be calculated using the concept of lattice paths. Think of it this way: an alignment is a path through a rectangular grid from coordinates \\((0,0)\\) to \\((m,n)\\), where \\(m\\) and \\(n\\) are the lengths of the two sequences. Each step in the path represents one of three operations: moving right (inserting a gap in the first sequence), moving down (inserting a gap in the second sequence), or moving diagonally (aligning two residues). The total number of such paths is given by what mathematicians call the Delannoy number, which has a formula involving binomial coefficients. But rather than dwell on the formula, let me just show you how these numbers grow. For two sequences of length 10, there are about 8.4 million possible alignments. Not too bad. For sequences of length 20, there are about 14 trillion alignments. Getting worse. For sequences of length 50, there are approximately \\(5.7 \\times 10^{35}\\) alignments. And for sequences of length 100—which is still quite short by biological standards—there are roughly \\(9 \\times 10^{73}\\) possible alignments. That is a number so large that it defies intuition.\nThe growth of this number is exponential. In fact, for two sequences of equal length \\(n\\), the number of alignments grows approximately as \\(9^n\\) (more precisely, like \\(3^{2n}\\) divided by a slowly growing factor). This exponential growth is what makes exhaustive search impossible. But it gets even worse when you think about what is actually involved in finding the optimal alignment. You cannot just count the alignments—you have to score each one to determine which is best. Scoring an alignment takes time proportional to the alignment length, which is \\(O(m+n)\\). So if you wanted to exhaustively search all alignments, you would need \\(O((m+n) \\times D(m,n))\\) time, where \\(D(m,n)\\) is that astronomical Delannoy number. For two sequences of length 100, even if you could evaluate one alignment per nanosecond, you would need more time than has elapsed since the Big Bang to check them all. This is why we absolutely need a better algorithm.\nThe key insight that saves us is this: while there are exponentially many possible alignments, there are only polynomially many distinct subproblems that we actually need to solve. What do I mean by subproblems? A subproblem is the task of aligning some prefix of the first sequence with some prefix of the second sequence. How many such prefixes are there? Well, the first sequence has \\(m+1\\) possible prefixes (the empty prefix, the first nucleotide, the first two nucleotides, and so on, up to the entire sequence). Similarly, the second sequence has \\(n+1\\) possible prefixes. So there are \\((m+1) \\times (n+1)\\) possible subproblems—and that is a polynomial number, not an exponential number. For sequences of length 100, that is about 10,000 subproblems, which is eminently manageable. This observation is the foundation of the dynamic programming approach. Instead of enumerating all possible complete alignments, we will systematically solve all possible subproblems and use those solutions to build up the solution to the full problem. This changes the problem from one that requires exponential time to one that requires only polynomial time.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Pairwise Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/pairwise_alignment.html#what-makes-an-alignment-optimal",
    "href": "chapters/bioinformatics/pairwise_alignment.html#what-makes-an-alignment-optimal",
    "title": "37  Pairwise Alignment",
    "section": "What makes an alignment optimal?",
    "text": "What makes an alignment optimal?\nBefore we can talk about finding the optimal alignment, we need to be clear about what we mean by “optimal.” An alignment is considered optimal if it achieves the highest possible score among all valid alignments, where the score is calculated according to some predetermined scoring scheme. The scoring scheme is crucial here. It is how we formalize our biological assumptions about what makes sequences similar. A good scoring scheme rewards alignments that reflect true evolutionary relationships while penalizing alignments that merely reflect random similarity. Think about it this way: we assign a score to each possible alignment, and then we look for the alignment with the best score. But how do we score an alignment? Typically, we give positive scores for matching nucleotides or amino acids (or at least higher scores than mismatches), and we penalize gaps because insertions and deletions are evolutionary events that have some cost. The exact numbers we use for these scores and penalties embody our understanding of molecular evolution. The scoring scheme is our way of telling the algorithm what we think is biologically important.\nThe optimality criterion reflects our understanding of molecular evolution. Sequences that are more similar are presumed to be more closely related evolutionarily, and the optimal alignment should reveal this relationship by maximizing measured similarity while accounting for the evolutionary events—substitutions, insertions, and deletions—that have occurred since the sequences diverged from their common ancestor. Here is the formal definition. Let \\(A\\) be the set of all possible alignments between sequences \\(S_1\\) and \\(S_2\\). Each alignment \\(a\\) in \\(A\\) can be assigned a score based on our scoring function. The optimal alignment \\(a^*\\) is the one that maximizes this score. We write this mathematically as \\(a^* = \\underset{a\\in A}{\\operatorname{argmax}} \\text{Score}(a)\\) over all alignments \\(a\\) in \\(A\\). This definition assumes we are maximizing similarity, which is the usual convention. Alternatively, we could frame the problem as minimizing distance, in which case we would seek the minimum score, but the principle is the same either way.\nNow, here is something important to understand: the optimal alignment represents our best hypothesis about how the sequences are related evolutionarily, but it is only optimal with respect to the chosen scoring scheme. Different scoring schemes can produce different optimal alignments for the same pair of sequences. This is not a bug—it is a feature. It reflects the fact that different evolutionary scenarios may be best modeled by different scoring parameters. For example, if you are comparing very closely related sequences, you might want to use harsh penalties for mismatches and gaps because you expect almost everything to match. But if you are comparing distantly related sequences, you might want to use more permissive scoring because you expect many differences to have accumulated over evolutionary time. The choice of scoring scheme should reflect your biological assumptions about the sequences you are aligning. This is why sequence alignment is not just a computational problem—it requires biological judgment as well.\nOne more thing to keep in mind: the optimal alignment may not be unique. Multiple different alignments may achieve the same optimal score, particularly for sequences with repetitive regions or regions of low complexity. This non-uniqueness can actually be biologically meaningful—it may reflect genuine uncertainty about the evolutionary relationship between sequences. When multiple optimal alignments exist, most algorithms will just report one of them (typically the first one encountered during the search), but it is worth remembering that other equally good alignments might exist. Some sophisticated implementations can enumerate all optimal alignments or sample from the set of near-optimal alignments, which can be useful for assessing confidence in the alignment. The existence of multiple optimal alignments also raises important questions about how much we should trust specific features of any single reported alignment, especially in regions where the sequences are difficult to align.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Pairwise Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/pairwise_alignment.html#scoring-matrices-and-gap-penalties",
    "href": "chapters/bioinformatics/pairwise_alignment.html#scoring-matrices-and-gap-penalties",
    "title": "37  Pairwise Alignment",
    "section": "Scoring matrices and gap penalties",
    "text": "Scoring matrices and gap penalties\nNow that we have established what an optimal alignment is—the one with the best score—we need to talk about how we actually compute the score of an alignment. This is where scoring matrices and gap penalties come in. A scoring matrix is a table that tells you the score for aligning each possible pair of residues. For protein sequences, this is a 20 by 20 matrix because there are 20 amino acids. For DNA sequences, it is a 4 by 4 matrix for the four nucleotides. The scoring matrix embodies our assumptions about which substitutions are more or less likely during evolution. For DNA, we might use a simple scheme like +1 for a match and -1 for a mismatch, though more sophisticated schemes account for the fact that transitions (A to G or C to T) are more common than transversions (A to C, A to T, G to C, or G to T). For proteins, the scoring schemes are more complex because amino acids have different chemical properties, and some substitutions are much more likely than others. A substitution between two chemically similar amino acids—like leucine and isoleucine, which are both hydrophobic—is much more likely to be tolerated than a substitution between chemically very different amino acids—like aspartate (acidic) and lysine (basic).\nThe most commonly used scoring matrices for protein alignment are the PAM (Point Accepted Mutation) and BLOSUM (BLOcks SUbstitution Matrix) families. These matrices were derived from empirical observations of substitution patterns in aligned sequences known to be homologous. The idea is simple: look at a large collection of aligned protein sequences, count how often each type of substitution occurs, and use those counts to estimate the probability that two residues are aligned because of homology versus by random chance. The elements of a substitution matrix S are typically log-odds scores. Specifically, S(a,b) is the logarithm of the ratio of two probabilities: the probability of observing residues a and b aligned in homologous sequences divided by the probability of observing them aligned by chance. If this ratio is greater than one, the log is positive, meaning that observing this pair is evidence for homology. If the ratio is less than one, the log is negative, meaning that this pair is more likely to occur by chance than by homology. This log-odds formulation has nice mathematical properties and allows us to add up scores across an alignment to get a total alignment score.\nThe choice of scoring matrix has a big effect on the resulting alignment. Different scoring matrices are appropriate for sequences at different evolutionary distances. Matrices derived from closely related sequences, like BLOSUM80, have high scores for identities and harsh penalties for mismatches, making them suitable for detecting recent homologs where you expect high sequence identity. Matrices derived from more divergent sequences, like BLOSUM45, are more permissive of substitutions and are better suited for detecting remote homologs where many substitutions have accumulated over evolutionary time. The number in the matrix name gives you a hint about the evolutionary distance it is designed for: BLOSUM62 is a general-purpose matrix suitable for moderate evolutionary distances, while BLOSUM80 is for closely related sequences and BLOSUM45 for distantly related ones. Choosing the right matrix for your specific biological question is important and requires some knowledge of how diverged you expect your sequences to be. In practice, BLOSUM62 is a safe default choice for many applications, but you should be aware that this choice affects your results.\n\n\n\n\n\n\n\n\nFigure 37.4: Partial BLOSUM62 substitution matrix showing scores for a subset of amino acids. Positive scores (green) indicate that observing this pair is evidence for homology, while negative scores (red) suggest the pair is more likely due to chance. The diagonal elements (self-matches) have the highest scores. Notice that chemically similar amino acids (like I and L, both hydrophobic) have positive off-diagonal scores.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Pairwise Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/pairwise_alignment.html#gap-penalties",
    "href": "chapters/bioinformatics/pairwise_alignment.html#gap-penalties",
    "title": "37  Pairwise Alignment",
    "section": "Gap penalties",
    "text": "Gap penalties\nBeyond the scoring matrix, which tells us how to score aligned pairs of residues, we also need to decide how to penalize gaps. A gap represents an insertion or deletion event in the evolutionary history of the sequences, and we need some way to account for the cost of introducing gaps into the alignment. The simplest approach is called linear gap costs. Under this model, the cost of a gap is directly proportional to its length. If you have a gap of length k and your gap penalty is d, then the cost of that gap is k times d. So a gap of length 3 costs three times as much as a gap of length 1. The total score of an alignment under linear gap costs is the sum of all the substitution scores from the scoring matrix minus the gap penalty times the total number of gap positions. Mathematically, that is: Score equals the sum over all aligned pairs of S(a_i, b_j) minus d times the number of gaps, where S(a_i, b_j) is the substitution score for aligning residues a_i and b_j from the scoring matrix, and d is the gap penalty.\nNow, the linear gap model has a significant biological limitation that you should be aware of. It assumes that each gap position corresponds to an independent insertion or deletion event. So a gap of length 3 is treated as if three separate single-nucleotide indels occurred. But in reality, that is often not how evolution works. Insertions and deletions often occur in larger chunks—a single mutational event might insert or delete multiple residues at once. This means that the evolutionary cost of opening a gap (creating a new gap where there was not one before) is typically higher than the cost of extending an existing gap (making an existing gap one residue longer). The linear gap model does not capture this distinction. It penalizes each gap position equally, whether it is opening a new gap or extending an existing one. Despite this limitation, linear gap costs are still useful. They are mathematically simple, which makes them a good starting point for understanding how alignment algorithms work. They also lead to elegant and efficient dynamic programming algorithms, as you will see shortly.\n\n\n\n\nGap Penalty Models Comparison\n============================================================\n\nLINEAR GAP PENALTY (d = 2):\n------------------------------------------------------------\nAlignment 1 (three separate single-base gaps):\n  Seq1: ATGC---GAT\n  Seq2: ATGCGGGGA-\n  Cost: 3 × d = 3 × 2 = 6\n\nAlignment 2 (one three-base gap):\n  Seq1: ATGC---GAT\n  Seq2: ATGCGGGGA-\n  Cost: 3 × d = 3 × 2 = 6\n\n  → Both alignments have the same cost!\n\n\nAFFINE GAP PENALTY (d_open = 5, d_extend = 1):\n------------------------------------------------------------\nAlignment 1 (three separate gaps):\n  Seq1: A-TGC-GA-T\n  Seq2: ATGGCGGGAT\n  Cost: 3 × (d_open + 0 × d_extend) = 3 × 5 = 15\n\nAlignment 2 (one three-base gap):\n  Seq1: ATGC---GAT\n  Seq2: ATGCGGGGA-\n  Cost: d_open + 2 × d_extend = 5 + 2 = 7\n\n  → Alignment 2 (fewer, longer gaps) is preferred!\n  → This better reflects how evolution actually works\n\n\n\nFigure 37.5\n\n\n\nIn the dynamic programming framework, linear gap costs lead to a particularly simple recurrence relation. When we are trying to compute the score for aligning prefixes of length \\(i\\) and \\(j\\), we have exactly three possibilities to consider. First, we could align position \\(i\\) from the first sequence with position \\(j\\) from the second sequence—this corresponds to a diagonal move in our alignment graph. Second, we could align position \\(i\\) from the first sequence with a gap—this corresponds to a horizontal move. Third, we could align a gap with position \\(j\\) from the second sequence—this corresponds to a vertical move. Each of these three possibilities can be evaluated in constant time using our scoring matrix and gap penalty, and we just take the maximum of the three. This gives us our \\(O(mn)\\) time complexity: we have \\(O(mn)\\) cells to fill in our dynamic programming matrix, and each cell requires constant time to compute, so the total time is \\(O(mn)\\). Nice and simple.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Pairwise Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/pairwise_alignment.html#the-principle-of-optimality",
    "href": "chapters/bioinformatics/pairwise_alignment.html#the-principle-of-optimality",
    "title": "37  Pairwise Alignment",
    "section": "The principle of optimality",
    "text": "The principle of optimality\nNow we come to the key insight that makes dynamic programming work for sequence alignment. It is called the principle of optimality, and it states that an optimal solution to a problem contains optimal solutions to its subproblems. Let me make this concrete. Suppose you have found the optimal alignment of two sequences \\(S_1\\) and \\(S_2\\). This alignment is the best possible—it has the highest score among all possible alignments. Now, pick any portion of this alignment. Maybe you look at the portion from position \\(i\\) to position \\(j\\) in the first sequence and from position \\(k\\) to position \\(l\\) in the second sequence. The principle of optimality says that this portion must itself be an optimal alignment of the corresponding subsequences \\(S_1[i..j]\\) and \\(S_2[k..l]\\). Think about why this must be true. If that portion were not optimal, that would mean there exists some better way to align those particular subsequences. But then you could take your overall alignment, cut out the suboptimal portion, plug in the better alignment of those subsequences, and you would have an alignment with a better total score than your supposedly optimal alignment. That is a contradiction. So every portion of an optimal alignment must itself be an optimal alignment of the corresponding subsequences.\nThis might seem like an abstract property, but it has profound implications for algorithm design. Let me show you why. The principle of optimality tells us that we can build up an optimal alignment by combining optimal alignments of smaller pieces. This is the essence of dynamic programming: solve small problems first, store their solutions, and then use those solutions to solve bigger problems. For sequence alignment, here is how it works. Instead of trying to find the optimal alignment of the full sequences all at once, we systematically compute optimal alignments of all possible pairs of prefixes. What is a prefix? It is just the first \\(k\\) characters of a sequence for some \\(k\\). Every sequence of length \\(m\\) has \\(m+1\\) prefixes: the empty sequence, the first character, the first two characters, and so on up to the entire sequence. So if we have two sequences of lengths \\(m\\) and \\(n\\), there are \\((m+1) \\times (n+1)\\) possible pairs of prefixes. For each pair of prefixes, we compute the optimal alignment score and store it. Then, when we need to compute the optimal alignment of longer prefixes, we can use the stored results for shorter prefixes. This transforms the problem from one requiring exponential time to one requiring only polynomial time.\nLet me prove this principle of optimality to you so you can see why it is true. We will use proof by contradiction. Suppose we have an optimal alignment \\(A^*\\) of sequences \\(S_1\\) and \\(S_2\\), and suppose we look at some portion of this alignment covering positions \\(i_1\\) to \\(i_2\\) in the first sequence and \\(j_1\\) to \\(j_2\\) in the second sequence. Now, assume for the sake of contradiction that this portion is not optimal. That means there exists some other alignment \\(B\\) of these subsequences that has a better score than the corresponding portion of \\(A^*\\). Okay, so what can we do? We can construct a new alignment \\(A'\\) by taking \\(A^*\\) and replacing the portion from \\(i_1\\) to \\(i_2\\) and \\(j_1\\) to \\(j_2\\) with \\(B\\) while leaving everything else unchanged. What is the score of this new alignment \\(A'\\)? Well, it is the score of \\(A^*\\) minus the score of the portion we removed plus the score of \\(B\\). Since \\(B\\) has a higher score than the portion we removed, \\(A'\\) has a higher score than \\(A^*\\). But wait—we said \\(A^*\\) was optimal, meaning it had the highest possible score. So we have a contradiction. Therefore, our assumption must have been wrong, and every portion of an optimal alignment must itself be optimal. This principle is what makes dynamic programming possible for sequence alignment.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Pairwise Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/pairwise_alignment.html#the-recurrence-relation",
    "href": "chapters/bioinformatics/pairwise_alignment.html#the-recurrence-relation",
    "title": "37  Pairwise Alignment",
    "section": "The recurrence relation",
    "text": "The recurrence relation\nThe principle of optimality leads directly to a recursive formulation of the alignment problem. Let \\(F(i,j)\\) denote the score of the optimal alignment of the first \\(i\\) characters of sequence \\(S_1\\) with the first \\(j\\) characters of sequence \\(S_2\\). In other words, \\(F(i,j)\\) is the optimal score for aligning the prefix \\(S_1[1..i]\\) with the prefix \\(S_2[1..j]\\). Now think about how we could compute \\(F(i,j)\\). If we have an optimal alignment of these two prefixes, what are the possible ways it could end? There are exactly three possibilities. First, the alignment could end with position \\(i\\) from \\(S_1\\) aligned to position \\(j\\) from \\(S_2\\)—we call this a match if the two residues are the same or a mismatch if they are different. Second, the alignment could end with position \\(i\\) from \\(S_1\\) aligned to a gap—this corresponds to a deletion from \\(S_1\\) or equivalently an insertion into \\(S_2\\). Third, the alignment could end with a gap aligned to position \\(j\\) from \\(S_2\\)—this corresponds to an insertion into \\(S_1\\) or equivalently a deletion from \\(S_2\\). These are the only three possibilities, and by the principle of optimality, the best alignment must come from taking the best of these three options.\nThis gives us our fundamental recurrence relation:\n\\[\nF(i,j) = \\max \\begin{cases}\nF(i-1,j-1) + S(S_1[i], S_2[j]) & \\text{(match/mismatch)} \\\\\nF(i-1,j) - d & \\text{(deletion)} \\\\\nF(i,j-1) - d & \\text{(insertion)}\n\\end{cases}\n\\]\nHere, \\(S(S_1[i], S_2[j])\\) is the score from our substitution matrix for aligning the residue at position \\(i\\) in sequence one with the residue at position \\(j\\) in sequence two, and \\(d\\) is our gap penalty. We take the maximum of these three values, and that is our optimal score for aligning the first \\(i\\) positions of \\(S_1\\) with the first \\(j\\) positions of \\(S_2\\). This recurrence relation is the heart of the Needleman-Wunsch algorithm, and understanding it is crucial to understanding how dynamic programming solves the alignment problem.\n\nflowchart TD\n    A[\"F(i-1,j-1)&lt;br/&gt;diagonal&lt;br/&gt;+S(S₁[i],S₂[j])\"] --&gt;|match/mismatch| D[\"MAX\"]\n    B[\"F(i-1,j)&lt;br/&gt;above&lt;br/&gt;-d\"] --&gt;|deletion| D\n    C[\"F(i,j-1)&lt;br/&gt;left&lt;br/&gt;-d\"] --&gt;|insertion| D\n    D --&gt; E[\"F(i,j)\"]\n\n    style A fill:#e1f5ff,stroke:#0066cc,stroke-width:2px\n    style B fill:#ffe1e1,stroke:#cc0000,stroke-width:2px\n    style C fill:#e1ffe1,stroke:#00cc00,stroke-width:2px\n    style D fill:#fff4e1,stroke:#ff9900,stroke-width:3px\n    style E fill:#f0e1ff,stroke:#9900cc,stroke-width:3px,font-weight:bold\n\n\n\n\n\nflowchart TD\n    A[\"F(i-1,j-1)&lt;br/&gt;diagonal&lt;br/&gt;+S(S₁[i],S₂[j])\"] --&gt;|match/mismatch| D[\"MAX\"]\n    B[\"F(i-1,j)&lt;br/&gt;above&lt;br/&gt;-d\"] --&gt;|deletion| D\n    C[\"F(i,j-1)&lt;br/&gt;left&lt;br/&gt;-d\"] --&gt;|insertion| D\n    D --&gt; E[\"F(i,j)\"]\n\n    style A fill:#e1f5ff,stroke:#0066cc,stroke-width:2px\n    style B fill:#ffe1e1,stroke:#cc0000,stroke-width:2px\n    style C fill:#e1ffe1,stroke:#00cc00,stroke-width:2px\n    style D fill:#fff4e1,stroke:#ff9900,stroke-width:3px\n    style E fill:#f0e1ff,stroke:#9900cc,stroke-width:3px,font-weight:bold\n\n\n\n\nFigure 37.6: Computing F(i,j) requires only three previously computed values: F(i-1,j-1) from the diagonal, F(i-1,j) from above, and F(i,j-1) from the left. This local dependency structure is what makes dynamic programming efficient—each cell depends only on its immediate neighbors, not on the entire matrix.\n\n\n\n\n\nOf course, every recurrence relation needs base cases—initial values that we can compute directly without using the recurrence. For sequence alignment, the base cases are straightforward:\n\\[\nF(0,0) = 0\n\\]\n\\[\nF(i,0) = -i \\times d \\quad \\text{for } i &gt; 0\n\\]\n\\[\nF(0,j) = -j \\times d \\quad \\text{for } j &gt; 0\n\\]\nThe first base case says that aligning two empty sequences has a score of zero—there is nothing to align, so nothing to score. The other base cases say that aligning a sequence of length \\(k\\) with an empty sequence requires introducing \\(k\\) gaps, each with penalty \\(d\\). These base cases give us the initial values along the top edge and left edge of our dynamic programming matrix, and from there we can use the recurrence relation to fill in the rest of the matrix row by row or column by column.\n\n\n\n\n\n\n\n\nFigure 37.7: A complete dynamic programming matrix for aligning ‘AGC’ with ‘ATC’ using match=+1, mismatch=-1, gap=-2. The matrix is filled row by row using the recurrence relation. Each cell shows the optimal score for aligning the corresponding prefixes. Arrows indicate which predecessor contributed the maximum score: ↖ = diagonal (match/mismatch), ↑ = up (deletion), ← = left (insertion).\n\n\n\n\n\nNow, here is an important point about the recursive structure. You might be tempted to implement this recurrence directly as a recursive function. But if you did that without memoization, you would get exponential time complexity. Why? Because the same subproblems would be computed over and over again. For example, \\(F(i-1,j-1)\\) could be reached through multiple different paths in the recursion tree. You might compute it once when computing \\(F(i,j)\\), and then compute it again when computing \\(F(i,j-1)\\), and yet again when computing \\(F(i-1,j)\\). All these redundant computations would add up to exponential time. This is why we use dynamic programming instead of naive recursion. Dynamic programming solves each subproblem exactly once and stores the result in a table, so we never have to recompute the same subproblem. This is what reduces the time complexity from exponential to polynomial.\nOne final note about the recurrence relation: while it allows us to compute the score of the optimal alignment, recovering the alignment itself—the actual sequence of matches, mismatches, and gaps—requires some additional bookkeeping. As we compute \\(F(i,j)\\) for each cell, we need to remember which of the three options (match/mismatch, deletion, or insertion) gave us the maximum score. We can store this information in a separate traceback matrix, where each cell records which predecessor cell we came from. Then, after we have filled in the entire scoring matrix and know that \\(F(m,n)\\) is the optimal score, we can recover the actual alignment by following the traceback pointers backwards from position \\((m,n)\\) to position \\((0,0)\\). Each step of the traceback tells us one operation in the alignment: diagonal move means match or mismatch, vertical move means gap in the horizontal sequence, horizontal move means gap in the vertical sequence. By following these traceback pointers, we reconstruct the optimal alignment.\n\n\n\n\n\n\n\n\nFigure 37.8: Traceback visualization showing the optimal path (in bold red) through the DP matrix from (3,3) back to (0,0). Following this path backwards reconstructs the alignment: A-GC vs ATC-, with score -1. The path interpretation: 1. Start at (3,3), 2. ↖ diagonal: C ↔︎ C (match), 3. ↖ diagonal: G ↔︎ T (mismatch), 4. ↑ up: A ↔︎ - (gap in seq2), 5. ↖ diagonal: A ↔︎ A (match)",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Pairwise Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/pairwise_alignment.html#the-needleman-wunsch-algorithm",
    "href": "chapters/bioinformatics/pairwise_alignment.html#the-needleman-wunsch-algorithm",
    "title": "37  Pairwise Alignment",
    "section": "The Needleman-Wunsch algorithm",
    "text": "The Needleman-Wunsch algorithm\nNow we are ready to see how all of these pieces fit together into the Needleman-Wunsch algorithm. Published in 1970 by Saul Needleman and Christian Wunsch, this was the first application of dynamic programming to biological sequence alignment, and it remains one of the foundational algorithms in bioinformatics. The algorithm systematically fills a two-dimensional table where entry \\((i,j)\\) contains the score of the optimal alignment of the first \\(i\\) characters of \\(S_1\\) with the first \\(j\\) characters of \\(S_2\\). Once the table is filled, we can read off the optimal score from the bottom-right corner, and we can recover the actual alignment by tracing back through the table. The algorithm has two phases: the forward phase, where we fill in the scoring matrix, and the traceback phase, where we reconstruct the optimal alignment. Let me walk you through both phases step by step.\nPhase 1: Fill the scoring matrix\n# Initialize base cases\nF[0,0] = 0\nfor i = 1 to m:\n    F[i,0] = -i × d\n    Traceback[i,0] = UP\nfor j = 1 to n:\n    F[0,j] = -j × d\n    Traceback[0,j] = LEFT\n\n# Fill the matrix\nfor i = 1 to m:\n    for j = 1 to n:\n        match = F[i-1,j-1] + S[S₁[i], S₂[j]]\n        delete = F[i-1,j] - d\n        insert = F[i,j-1] - d\n\n        F[i,j] = max(match, delete, insert)\n\n        if F[i,j] == match:\n            Traceback[i,j] = DIAGONAL\n        else if F[i,j] == delete:\n            Traceback[i,j] = UP\n        else:\n            Traceback[i,j] = LEFT\nIn the forward phase, we start by initializing the base cases. We set \\(F(0,0)\\) equal to zero because aligning two empty sequences has a score of zero. Then, for \\(i\\) from 1 to \\(m\\), we set \\(F(i,0)\\) equal to \\(-i \\times d\\), representing the cost of aligning a prefix of \\(S_1\\) with the empty sequence. Similarly, for \\(j\\) from 1 to \\(n\\), we set \\(F(0,j)\\) equal to \\(-j \\times d\\). These initializations fill in the top row and leftmost column of our matrix. Now comes the main computation. We have two nested loops: the outer loop over \\(i\\) from 1 to \\(m\\), and the inner loop over \\(j\\) from 1 to \\(n\\). For each cell \\((i,j)\\), we compute three quantities. First, match equals \\(F(i-1,j-1) + S(S_1[i], S_2[j])\\)—this is the score if we align position \\(i\\) with position \\(j\\). Second, delete equals \\(F(i-1,j) - d\\)—this is the score if we align position \\(i\\) with a gap. Third, insert equals \\(F(i,j-1) - d\\)—this is the score if we align a gap with position \\(j\\). We set \\(F(i,j)\\) equal to the maximum of these three values. Crucially, we also record which of the three operations gave us the maximum, because we will need this information during traceback. This recording can be done with a separate traceback matrix that stores a pointer or code indicating whether we came from the diagonal, from above, or from the left.\n\n\n\n\n\n\n\n\nFigure 37.9: Progressive stages of the Needleman-Wunsch algorithm. (a) Empty matrix, (b) Base cases initialized, (c) After first row computed, (d) Complete matrix with all cells filled. The matrix is filled systematically row by row, with each cell depending only on previously computed neighbors.\n\n\n\n\n\nPhase 2: Traceback to recover the alignment\n# Start at bottom-right cell\ni = m\nj = n\nalignment₁ = \"\"\nalignment₂ = \"\"\n\n# Follow traceback pointers\nwhile i &gt; 0 or j &gt; 0:\n    if Traceback[i,j] == DIAGONAL:\n        alignment₁ = S₁[i] + alignment₁\n        alignment₂ = S₂[j] + alignment₂\n        i = i - 1\n        j = j - 1\n    else if Traceback[i,j] == UP:\n        alignment₁ = S₁[i] + alignment₁\n        alignment₂ = \"-\" + alignment₂\n        i = i - 1\n    else:  # LEFT\n        alignment₁ = \"-\" + alignment₁\n        alignment₂ = S₂[j] + alignment₂\n        j = j - 1\n\nreturn (alignment₁, alignment₂, F[m,n])\nAfter the forward phase completes, \\(F(m,n)\\) contains the score of the optimal alignment of the full sequences. But we usually also want to know what the actual alignment is, not just its score. This is where the traceback phase comes in. We start at cell \\((m,n)\\) and work our way back to cell \\((0,0)\\), following the traceback pointers we recorded during the forward phase. At each step, we look at the traceback pointer for the current cell. If it points diagonally (to cell \\((i-1,j-1)\\)), that means we aligned \\(S_1[i]\\) with \\(S_2[j]\\), so we add this pair to our alignment. If it points upward (to cell \\((i-1,j)\\)), that means we aligned \\(S_1[i]\\) with a gap, so we add a gap character in the second sequence. If it points leftward (to cell \\((i,j-1)\\)), that means we aligned a gap with \\(S_2[j]\\), so we add a gap character in the first sequence. We continue following these pointers until we reach \\((0,0)\\), at which point we have reconstructed the entire alignment. Note that we build the alignment from right to left (prepending characters), so it comes out in the correct order. The beauty of this algorithm is that it guarantees finding the optimal alignment in \\(O(mn)\\) time by cleverly avoiding all the redundant computation that a naive recursive approach would entail.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Pairwise Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/pairwise_alignment.html#understanding-memoization-and-the-dynamic-programming-matrix",
    "href": "chapters/bioinformatics/pairwise_alignment.html#understanding-memoization-and-the-dynamic-programming-matrix",
    "title": "37  Pairwise Alignment",
    "section": "Understanding memoization and the dynamic programming matrix",
    "text": "Understanding memoization and the dynamic programming matrix\nThe magic of dynamic programming lies in a technique called memoization, which just means storing the results of computations so you do not have to repeat them. For sequence alignment, each subproblem corresponds to aligning a prefix of one sequence with a prefix of the other sequence. The key insight is simple but powerful: once we have computed the optimal score for aligning the first \\(i\\) characters of \\(S_1\\) with the first \\(j\\) characters of \\(S_2\\), we store that score in a table, and whenever we need it again, we just look it up instead of recomputing it. The storage mechanism is a two-dimensional array \\(F\\), where \\(F[i,j]\\) stores the score of the optimal alignment of \\(S_1[1..i]\\) with \\(S_2[1..j]\\). This array is our memoization table, and it is what transforms an exponential-time recursive algorithm into a polynomial-time iterative one. Without memoization, we would recompute the same subproblems over and over again, leading to exponential time complexity. With memoization, we compute each subproblem exactly once and then reuse the stored result whenever we need it.\nThe dynamic programming approach builds solutions to larger problems from solutions to smaller problems in a systematic way. The key is to compute things in an order that ensures all the values we need are already available when we need them. For sequence alignment, this usually means filling the matrix row by row from top to bottom or column by column from left to right, though any order that respects the dependencies between cells will work. What are these dependencies? For each cell \\(F[i,j]\\), we need three previously computed values: \\(F[i-1,j-1]\\) (the diagonal predecessor), \\(F[i-1,j]\\) (the predecessor directly above), and \\(F[i,j-1]\\) (the predecessor directly to the left). As long as we fill cells in an order that ensures these three predecessors are already computed when we need them, we are fine. This local dependency structure is what makes the algorithm efficient. Each cell only depends on three immediate neighbors, not on the entire matrix, so we can compute cells incrementally without having to store or process unnecessary information.\nThe completed dynamic programming matrix is more than just a tool for computing the optimal score—it contains rich information about the relationship between the sequences. The bottom-right cell \\(F[m,n]\\) gives us the optimal alignment score, but every other cell also tells us something interesting. \\(F[i,j]\\) gives the optimal score for aligning the first \\(i\\) characters of \\(S_1\\) with the first \\(j\\) characters of \\(S_2\\), so the entire matrix gives us optimal scores for all possible pairs of prefixes. This can be useful if, for example, you want to find the best alignment of a short prefix of one sequence with the full other sequence. The pattern of values in the matrix also reveals regions of high and low similarity between the sequences. High scores in a region indicate strong similarity, while low scores indicate dissimilarity. You can even use the matrix to identify alternative alignments that score nearly as well as the optimal one by tracing near-optimal paths through the matrix. This can be valuable for assessing how confident we should be in specific features of the reported alignment.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Pairwise Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/pairwise_alignment.html#variations-and-extensions",
    "href": "chapters/bioinformatics/pairwise_alignment.html#variations-and-extensions",
    "title": "37  Pairwise Alignment",
    "section": "Variations and Extensions",
    "text": "Variations and Extensions\n\nAffine Gap Penalties\nWhile linear gap costs provide a simple model, affine gap penalties offer a more biologically realistic representation of insertion and deletion events. Under the affine gap model, the cost of a gap of length \\(k\\) is:\n\\[\n\\text{Gap cost} = d_{\\text{open}} + (k-1) \\times d_{\\text{extend}}\n\\]\nwhere \\(d_{\\text{open}}\\) is the gap opening penalty and \\(d_{\\text{extend}}\\) is the gap extension penalty, with typically \\(d_{\\text{open}} \\gg d_{\\text{extend}}\\).\nImplementing affine gap penalties requires a modified dynamic programming approach with three matrices instead of one: - \\(M[i,j]\\): Best score ending with \\(S_1[i]\\) aligned to \\(S_2[j]\\) - \\(I_x[i,j]\\): Best score ending with \\(S_1[i]\\) aligned to a gap - \\(I_y[i,j]\\): Best score ending with a gap aligned to \\(S_2[j]\\)\nThe recurrence relations become more complex but maintain the same \\(O(mn)\\) time complexity.\n\n\nLocal Alignment\nWhile global alignment aligns entire sequences, local alignment (Smith-Waterman algorithm) finds the best-matching subsequences within two sequences. This is achieved through two key modifications:\n\nAllow the alignment to start at any position by initializing all edges to 0\nAllow the alignment to end at any position by taking the maximum over the entire matrix\n\nThe recurrence relation becomes:\n\\[\nF(i,j) = \\max \\begin{cases}\n0 & \\text{(start new alignment)} \\\\\nF(i-1,j-1) + S(S_1[i], S_2[j]) \\\\\nF(i-1,j) - d \\\\\nF(i,j-1) - d\n\\end{cases}\n\\]",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Pairwise Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/pairwise_alignment.html#conclusion",
    "href": "chapters/bioinformatics/pairwise_alignment.html#conclusion",
    "title": "37  Pairwise Alignment",
    "section": "Conclusion",
    "text": "Conclusion\nPairwise global sequence alignment represents a fundamental operation in computational biology, providing the foundation for understanding evolutionary relationships, predicting function, and identifying important biological features. The application of dynamic programming to this problem, first demonstrated by Needleman and Wunsch, transformed an exponentially complex problem into one solvable in polynomial time, enabling the analysis of biological sequences at scale.\nThe elegance of the dynamic programming solution lies in its exploitation of the principle of optimality—the recognition that optimal alignments contain optimal sub-alignments. This principle allows us to build up the solution systematically, storing and reusing the scores of sub-alignments to avoid redundant computation. The resulting algorithm, with its \\(O(mn)\\) time complexity, makes it feasible to align sequences of biologically relevant length, from individual genes to entire chromosomes.\nThe scoring framework for alignment, encompassing substitution matrices and gap penalties, provides a flexible system for modeling different evolutionary scenarios. While simple scoring schemes like linear gap costs have limitations, they serve as important baselines and teaching tools. More sophisticated models, such as affine gap penalties and position-specific scoring, better capture biological reality but require more complex algorithms.\nThe impact of pairwise global alignment extends far beyond its original application. The algorithmic framework has inspired solutions to numerous other problems in bioinformatics, from RNA structure prediction to gene finding. The conceptual framework—representing biological problems as optimization problems solvable by dynamic programming—has become a cornerstone of computational biology.\nLooking forward, sequence alignment continues to evolve in response to new challenges and opportunities. The massive scale of modern sequence databases requires continued algorithmic innovation. The integration of machine learning approaches promises to capture complex patterns in sequence evolution that traditional methods might miss. The increasing availability of structural information provides new constraints that can improve alignment accuracy.\nAs we enter an era where sequencing entire genomes is routine and comparing thousands of genomes is feasible, the principles established by the study of pairwise global alignment remain relevant. The fundamental challenge—finding meaningful similarities between sequences despite the noise introduced by billions of years of evolution—continues to drive innovation in computational biology. The solutions to this challenge, built on the foundation of dynamic programming and optimal alignment, will continue to provide insights into the molecular basis of life for years to come.\nThe story of pairwise global sequence alignment is thus not just a tale of algorithmic elegance but a testament to the power of computational thinking in biology. By formulating biological questions in precise mathematical terms and developing efficient algorithms to answer them, we gain not just practical tools but deeper understanding of the principles governing molecular evolution. As sequencing technologies continue to advance and our databases continue to grow, the methods and insights derived from the study of sequence alignment will remain central to our efforts to decode the information written in the language of DNA and proteins.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Pairwise Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/multiple_alignment.html",
    "href": "chapters/bioinformatics/multiple_alignment.html",
    "title": "38  Multiple Alignment",
    "section": "",
    "text": "Introduction: The Fundamental Challenge\nMultiple sequence alignment (MSA) represents one of the most fundamental and challenging problems in computational biology. While aligning two sequences is computationally tractable, the extension to multiple sequences introduces profound computational and biological complexities that have engaged researchers for decades. The importance of MSA cannot be overstated: it forms the foundation for phylogenetic analysis, protein structure prediction, identification of conserved regions, and understanding evolutionary relationships among organisms.\nThe biological motivation for multiple alignment stems from the observation that sequences related by evolution share common ancestry and often preserve functionally important regions. These conserved regions, whether coding for critical protein domains or regulatory elements, appear as columns of similar or identical residues in a properly constructed alignment. By aligning multiple related sequences simultaneously, we can identify these conserved patterns that might be obscured in pairwise comparisons, reveal evolutionary relationships, and make functional predictions about unknown sequences.\nConsider the challenge: given a set of DNA or protein sequences that share evolutionary history but have diverged through substitutions, insertions, and deletions, we must arrange them in a matrix where each row represents a sequence and columns represent homologous positions. The alignment must optimize some scoring function that reflects biological reality while handling sequences of different lengths, managing gaps that represent insertion/deletion events, and maintaining computational feasibility even as the number of sequences grows.\nThe computational complexity of this problem escalates dramatically with the number of sequences. While dynamic programming provides an exact solution for pairwise alignment in O(n²) time for sequences of length n, the extension to k sequences requires O(nᵏ) time and space—quickly becoming intractable even for modest numbers of sequences. This exponential growth has driven the development of numerous heuristic approaches that trade optimality guarantees for computational feasibility, each with its own strengths, weaknesses, and underlying assumptions about sequence evolution.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/multiple_alignment.html#the-computational-complexity-problem",
    "href": "chapters/bioinformatics/multiple_alignment.html#the-computational-complexity-problem",
    "title": "38  Multiple Alignment",
    "section": "The Computational Complexity Problem",
    "text": "The Computational Complexity Problem\n\nTheoretical Foundations\nThe multiple sequence alignment problem can be formally defined as follows: given k sequences S₁, S₂, …, Sₖ over an alphabet Σ (typically 4 nucleotides or 20 amino acids), find an alignment that maximizes a scoring function. An alignment extends each sequence by inserting gaps (-) such that all resulting sequences have the same length, and the score is typically the sum of all pairwise scores (sum-of-pairs) or the score relative to a consensus sequence.\nThe exact solution using dynamic programming extends the Needleman-Wunsch algorithm to k dimensions. For k sequences of length n, we construct a k-dimensional matrix where each cell (i₁, i₂, …, iₖ) represents the optimal alignment of prefixes S₁[1..i₁], S₂[1..i₂], …, Sₖ[1..iₖ]. Each cell must consider 2ᵏ - 1 possible predecessors (corresponding to all possible combinations of gap/residue for each sequence, excluding the all-gap case), leading to the time complexity:\n\\[T(n,k) = O(2^k \\cdot n^k)\\]\nThe space complexity is similarly prohibitive:\n\\[S(n,k) = O(n^k)\\]\nFor just 10 sequences of length 100, this would require 10²⁰ cells—far beyond the capacity of any conceivable computer. Even with optimizations like linear space algorithms that reduce space complexity to O(kn²), the time complexity remains exponential in the number of sequences.\n\n\nNP-Completeness\nWang and Jiang proved in 1994 that multiple sequence alignment under the sum-of-pairs (SP) scoring scheme is NP-complete. This means that unless P = NP, no polynomial-time algorithm exists for finding optimal multiple alignments. The proof involves a reduction from the shortest common supersequence problem, itself known to be NP-complete.\nThe NP-completeness of MSA has profound implications. It means that as we add more sequences to our alignment problem, the computational difficulty doesn’t just increase—it increases exponentially. This theoretical barrier has shaped the entire field of multiple alignment, driving researchers to develop increasingly sophisticated heuristics rather than pursuing exact solutions.\nEven seemingly simple variations of the problem remain computationally hard. The star alignment problem (aligning all sequences to a central sequence), the tree alignment problem (aligning sequences according to a phylogenetic tree), and the consensus alignment problem all belong to the class of NP-hard problems. This universality of computational difficulty suggests that the complexity is inherent to the biological problem itself, not merely an artifact of our mathematical formulation.\n\n\nScoring Functions and Objective Complexity\nThe choice of scoring function adds another layer of complexity to the MSA problem. The most common approaches include:\nSum-of-Pairs (SP) Score: The total score is the sum of scores for all pairwise alignments within the multiple alignment. For k sequences, this involves k(k-1)/2 pairwise comparisons. While intuitive, the SP score treats all sequence pairs equally, ignoring evolutionary relationships. It also suffers from the problem of overcounting: a single evolutionary event (like an insertion) gets penalized multiple times in all affected pairwise comparisons.\nTree-Based Scores: These scores weight sequence comparisons according to a phylogenetic tree, giving more importance to distant evolutionary relationships. While biologically more realistic, tree-based scoring requires knowing or estimating the phylogenetic tree, adding another computational challenge.\nConsensus Scores: The alignment is scored against a consensus sequence representing the “average” or most likely ancestral sequence. This approach can be more efficient computationally but may not capture all evolutionary information.\nInformation-Theoretic Scores: These measure the information content or entropy of alignment columns, favoring alignments that create highly conserved columns. While elegant theoretically, these scores can be difficult to optimize and may not always correspond to biological reality.\nEach scoring scheme embodies different assumptions about sequence evolution and conservation, and no single approach is optimal for all situations. This multiplicity of objective functions further complicates the already challenging optimization problem.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/multiple_alignment.html#progressive-alignment-the-dominant-paradigm",
    "href": "chapters/bioinformatics/multiple_alignment.html#progressive-alignment-the-dominant-paradigm",
    "title": "38  Multiple Alignment",
    "section": "Progressive Alignment: The Dominant Paradigm",
    "text": "Progressive Alignment: The Dominant Paradigm\n\nThe Progressive Strategy\nProgressive alignment, introduced by Feng and Doolittle in 1987 and popularized by programs like ClustalW, has become the dominant approach for multiple sequence alignment due to its reasonable balance between speed and accuracy. The strategy builds a multiple alignment incrementally by first aligning the most similar sequences and progressively adding more distant sequences or groups of sequences.\nThe progressive approach typically follows these steps:\n\nCalculate pairwise distances: Compute similarity scores between all pairs of sequences, usually using rapid pairwise alignment or k-mer based methods.\nConstruct a guide tree: Build a phylogenetic tree (usually using UPGMA or neighbor-joining) that represents the evolutionary relationships among sequences.\nProgressive alignment: Starting from the leaves of the tree, align sequences or groups following the branching order, working toward the root.\n\nThe key insight is that closely related sequences are easier to align accurately, and these initial alignments can guide the incorporation of more distant sequences. By following the evolutionary hierarchy, progressive alignment attempts to preserve the most reliable homology relationships first.\n\n\nMathematical Framework\nThe progressive alignment score can be formulated recursively. Let A(X) represent an alignment of sequence set X, and S(A₁, A₂) represent the score of aligning two alignments. For a tree T with children nodes c₁ and c₂ of node v:\n\\[Score(v) = Score(c_1) + Score(c_2) + S(A(c_1), A(c_2))\\]\nThe alignment at each internal node combines the alignments of its children using a profile-to-profile alignment algorithm. A profile represents the frequency of each residue (or gap) at each position in an alignment:\n\\[P_{ij} = \\frac{count(residue_j \\text{ at position } i)}{number \\text{ of sequences}}\\]\nProfile alignment extends pairwise alignment by scoring position pairs using the weighted sum of all possible residue pair scores:\n\\[S(P_1[i], P_2[j]) = \\sum_{a \\in \\Sigma} \\sum_{b \\in \\Sigma} P_1[i][a] \\cdot P_2[j][b] \\cdot s(a,b)\\]\nwhere s(a,b) is the substitution score between residues a and b.\n\n\nAdvantages of Progressive Alignment\nProgressive alignment offers several compelling advantages that explain its widespread adoption:\nComputational Efficiency: The time complexity is approximately O(k²n²) for k sequences of length n—polynomial rather than exponential. This makes it feasible to align hundreds or even thousands of sequences on standard computers.\nBiological Intuition: The approach mirrors the evolutionary process, first grouping closely related sequences that share recent common ancestry. This biological grounding often leads to meaningful alignments even when the mathematical optimum might be biologically incorrect.\nScalability: Progressive methods can handle large datasets that would be impossible for exact algorithms. Modern implementations can align thousands of sequences in reasonable time.\nFlexibility: The framework accommodates various scoring schemes, gap penalties, and substitution matrices. It can be adapted for DNA, protein, or even RNA structural alignments.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/multiple_alignment.html#the-greedy-nature-and-its-shortcomings",
    "href": "chapters/bioinformatics/multiple_alignment.html#the-greedy-nature-and-its-shortcomings",
    "title": "38  Multiple Alignment",
    "section": "The Greedy Nature and Its Shortcomings",
    "text": "The Greedy Nature and Its Shortcomings\n\nThe Fundamental Limitation\nProgressive alignment is inherently a greedy algorithm—it makes locally optimal choices at each step without the ability to reconsider previous decisions. Once two sequences or groups are aligned, their relative positioning is fixed for all subsequent steps. This “once a gap, always a gap” principle means that errors introduced early in the process propagate and potentially amplify as more sequences are added.\nConsider a simple example: suppose sequences A and B are aligned first, introducing a gap in sequence A at position 10. When sequence C is added later, it might align better if the gap in A were at position 11, but this adjustment is impossible within the progressive framework. The alignment of A and B is frozen, forcing C to accommodate their potentially suboptimal arrangement.\n\n\nError Propagation\nThe progressive strategy’s greatest weakness is its susceptibility to error propagation. Mistakes made in early alignment stages cannot be corrected later, and these errors tend to accumulate and magnify as the alignment grows. Several factors contribute to this problem:\nIncorrect Guide Tree: The initial phylogenetic tree is based on rough pairwise distances, often computed using fast but approximate methods. If this tree incorrectly groups sequences, the entire alignment follows this flawed evolutionary hypothesis. For instance, if two divergent sequences happen to share a rare similarity and are incorrectly grouped as close relatives, their alignment will be fixed early and force all other sequences to accommodate this error.\nLocal Minima: The greedy approach can easily become trapped in local minima—alignments that cannot be improved by small modifications but are far from the global optimum. Unlike optimization algorithms that can escape local minima through techniques like simulated annealing or genetic algorithms, progressive alignment has no mechanism for backtracking or exploring alternative alignment paths.\nDistance Effects: Sequences added later in the progressive process must align to increasingly large and rigid profiles. Distant sequences, added near the root of the tree, have the least flexibility in their placement, even though these are precisely the sequences where alignment uncertainty is highest.\n\n\nThe Indel Problem\nOne of the most serious issues with progressive alignment is its tendency to accumulate and retain insertions and deletions (indels). This problem manifests in several ways:\nGap Attraction: Once a gap is introduced in an alignment, it tends to attract additional gaps in subsequently added sequences. This occurs because aligning a residue with an existing gap column often scores better than creating a new gap elsewhere. The result is an artificial clustering of gaps that may not reflect the true evolutionary history of insertions and deletions.\nGap Persistence: Gaps introduced early in the progressive process cannot be removed or repositioned, even if later sequences suggest a different gap placement would be more parsimonious. This rigidity leads to alignments with more gaps than necessary, reducing both accuracy and interpretability.\nInconsistent Gap Penalties: When aligning profiles, the effective gap penalty differs from that used in the initial pairwise alignments. A gap in a profile represents gaps in multiple sequences, but profile alignment methods typically don’t fully account for this multiplicity. This inconsistency can lead to either excessive gap introduction or inappropriate gap extension.\nTerminal Gap Ambiguity: Progressive methods often struggle with terminal gaps (gaps at sequence ends). Different treatment of terminal gaps during pairwise alignment versus profile alignment can create inconsistent gap patterns that don’t reflect biological reality.\n\n\nMathematical Analysis of Error Accumulation\nThe error accumulation in progressive alignment can be modeled mathematically. Let ε be the probability of making an alignment error at each node of the guide tree. For a balanced binary tree with k sequences, the tree height is log₂(k). The probability of at least one error in the path from leaf to root is:\n\\[P(error) = 1 - (1 - \\varepsilon)^{\\log_2(k)}\\]\nFor small ε, this approximates to:\n\\[P(error) \\approx \\varepsilon \\cdot \\log_2(k)\\]\nThis logarithmic growth of error probability with sequence number might seem modest, but it represents a lower bound. In practice, errors compound because each misalignment affects the profile used for subsequent alignments. If we consider error propagation where each error increases the probability of future errors by a factor α &gt; 1, the error growth becomes:\n\\[P(error) \\approx 1 - e^{-\\varepsilon \\cdot \\alpha^{\\log_2(k)}}\\]\nThis shows potential for rapid error accumulation, especially when aligning divergent sequences where ε is substantial.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/multiple_alignment.html#popular-heuristic-multiple-alignment-tools",
    "href": "chapters/bioinformatics/multiple_alignment.html#popular-heuristic-multiple-alignment-tools",
    "title": "38  Multiple Alignment",
    "section": "Popular Heuristic Multiple Alignment Tools",
    "text": "Popular Heuristic Multiple Alignment Tools\n\nClustalW and Clustal Omega\nClustalW, published by Thompson, Higgins, and Gibson in 1994, became the most widely used multiple alignment program in bioinformatics history. Its success stems from several key innovations:\nPosition-Specific Gap Penalties: ClustalW adjusts gap penalties based on local sequence characteristics. Gaps are discouraged in hydrophobic regions (likely to be buried in protein cores) and within secondary structures, while being more permissive in hydrophilic loops. The gap opening penalty at position i is modified by:\n\\[GOP_i = GOP \\times (1 + |log(N_i/N_{avg})|) \\times SS_i \\times HF_i\\]\nwhere N_i is the number of sequences with a gap at position i, SS_i is a secondary structure factor, and HF_i is a hydrophobicity factor.\nSequence Weighting: To correct for uneven sampling of sequence space, ClustalW weights sequences based on their uniqueness. Sequences from densely sampled clades receive lower weights, preventing them from dominating the alignment. The weight for sequence i is:\n\\[W_i = \\sum_{j \\in path\\_to\\_root} \\frac{1}{n_j}\\]\nwhere n_j is the number of branches at node j.\nDynamic Selection of Substitution Matrix: The program selects different substitution matrices based on sequence similarity. For closely related sequences, it uses matrices optimized for high identity (like BLOSUM80), while switching to matrices for distant relationships (like BLOSUM45) for divergent sequences.\nClustal Omega, released in 2011, represents a complete reimplementation designed for the genomic era. It replaces the O(k²) guide tree construction with an O(k log k) algorithm using mBed embeddings, enabling alignment of hundreds of thousands of sequences. The program also uses HMM-based profile-profile alignment for improved accuracy.\n\n\nMUSCLE\nMUSCLE (Multiple Sequence Comparison by Log-Expectation), developed by Robert Edgar in 2004, introduced an iterative refinement approach that addresses some limitations of purely progressive methods:\nThree-Stage Strategy: 1. Draft progressive alignment using k-mer distances 2. Improved progressive alignment using distances from the draft alignment 3. Iterative refinement by subdividing and realigning\nTree-Dependent Partitioning: MUSCLE cuts the guide tree into two subtrees, realigns the resulting profiles, and accepts the new alignment if it improves the score. This process iterates through different tree edges:\n\\[Score_{new} = \\sum_{i,j} w_{ij} \\cdot SP(seq_i, seq_j)_{new}\\]\nwhere w_ij weights pairs based on their tree distance.\nLog-Expectation Scores: MUSCLE uses log-expectation scores that account for evolutionary distance:\n\\[LE = log(\\sum_{a,b} p_a \\cdot p_b \\cdot e^{s(a,b)/\\lambda})\\]\nThis scoring better handles distantly related sequences compared to traditional sum-of-pairs scores.\n\n\nMAFFT\nMAFFT (Multiple Alignment using Fast Fourier Transform) by Katoh et al. (2002) offers multiple alignment strategies with different speed-accuracy trade-offs:\nFFT for Rapid Homology Detection: MAFFT converts amino acid sequences to numeric vectors based on volume and polarity, then uses FFT to rapidly identify homologous regions:\n\\[correlation(i) = FFT^{-1}[FFT(v_1) \\times \\overline{FFT(v_2)}]\\]\nThis approach is approximately 100 times faster than dynamic programming for identifying similar segments.\nConsistency-Based Refinement: The G-INS-i and L-INS-i methods incorporate consistency information from all pairwise alignments. If residue a from sequence x aligns with residue b from sequence y, and b aligns with residue c from sequence z, then a and c should be aligned. This is formulated as:\n\\[C(a,c) = \\sum_{b} P(a \\sim b) \\cdot P(b \\sim c)\\]\nwhere P denotes alignment probability.\nStructural Information Integration: MAFFT can incorporate structural alignments when available, using them to anchor the sequence alignment and improve accuracy in structurally conserved regions.\n\n\nT-Coffee and Consistency-Based Methods\nT-Coffee (Tree-based Consistency Objective Function for alignment Evaluation) by Notredame et al. (2000) pioneered the consistency-based approach to multiple alignment:\nPrimary Library Construction: T-Coffee builds a library of pairwise alignments using both global (ClustalW) and local (LALIGN) methods. Each aligned residue pair receives a weight based on sequence identity:\n\\[w(a_i, b_j) = \\frac{identity(A,B)}{100} \\times 100\\]\nLibrary Extension: The crucial innovation is extending the library through consistency:\n\\[w'(a_i, b_j) = w(a_i, b_j) + \\sum_{k \\neq i,j} min(w(a_i, c_k), w(c_k, b_j))\\]\nThis triplet approach ensures that alignments consistent across multiple sequences receive higher weights.\nProgressive Alignment with Extended Library: The final alignment uses progressive alignment but scores positions using the extended library weights rather than a substitution matrix.\n\n\nPRANK and Phylogeny-Aware Methods\nPRANK (Probabilistic Alignment Kit) by Löytynoja and Goldman (2005) explicitly models insertions and deletions in an evolutionary framework:\nPhylogenetic Placement of Indels: PRANK distinguishes between insertions and deletions by considering the phylogenetic tree. A gap pattern is evaluated for its evolutionary parsimony:\n\\[P(gap\\_pattern | tree) = \\prod_{branches} P(indel\\_events\\_on\\_branch)\\]\nAvoiding False Homology: Traditional aligners tend to align non-homologous sequence by inserting gaps elsewhere. PRANK maintains permanent gaps for true deletions, preventing incorrect alignment of inserted sequences with non-homologous regions.\nCharacter Distinction: The algorithm treats insertions and deletions as distinct evolutionary events, not just generic gaps. This distinction is crucial for accurate phylogenetic inference and evolutionary rate estimation.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/multiple_alignment.html#advanced-methodologies-and-algorithmic-innovations",
    "href": "chapters/bioinformatics/multiple_alignment.html#advanced-methodologies-and-algorithmic-innovations",
    "title": "38  Multiple Alignment",
    "section": "Advanced Methodologies and Algorithmic Innovations",
    "text": "Advanced Methodologies and Algorithmic Innovations\n\nIterative Refinement Strategies\nIterative refinement attempts to escape the local minima inherent to progressive alignment. These methods repeatedly modify an existing alignment to improve its score:\nBarton-Sternberg Algorithm: One of the earliest refinement methods, it removes each sequence from the alignment and realigns it to the profile of remaining sequences. If the new alignment scores better, it’s accepted:\n\\[\\Delta Score = Score(align(seq_i, profile_{-i})) - Score(original)\\]\nGenetic Algorithms: Some tools use evolutionary computation to explore alignment space. SAGA (Sequence Alignment by Genetic Algorithm) represents alignments as individuals in a population, using crossover and mutation operators: - Crossover: Exchange aligned blocks between alignments - Mutation: Shift gaps, merge adjacent gaps, or split gap regions - Selection: Retain alignments with better scores\nThe population evolves toward better alignments, potentially escaping local minima that trap greedy methods.\nSimulated Annealing: This approach accepts worse alignments with probability:\n\\[P(accept) = e^{-\\Delta Score / T}\\]\nwhere T is a temperature parameter that decreases over time. Early iterations explore broadly, while later iterations converge to a local optimum.\n\n\nHidden Markov Models and Statistical Approaches\nProfile Hidden Markov Models (HMMs) provide a probabilistic framework for multiple alignment:\nModel Structure: A profile HMM consists of: - Match states (M): Emit residues according to position-specific distributions - Insert states (I): Model insertions relative to the consensus - Delete states (D): Silent states representing deletions\nParameter Estimation: Given unaligned sequences, the Baum-Welch algorithm estimates:\n\\[\\theta^{(t+1)} = \\arg\\max_\\theta \\sum_i P(seq_i | \\theta^{(t)}) \\log P(seq_i | \\theta)\\]\nAlignment via Viterbi: Once trained, the Viterbi algorithm finds the most probable alignment:\n\\[\\pi^* = \\arg\\max_\\pi P(\\pi | seq, \\theta)\\]\nHMM-based methods like HMMER naturally handle position-specific scoring and provide probabilistic measures of alignment confidence.\n\n\nConsensus and Meta-Methods\nMeta-methods combine multiple alignments to leverage the strengths of different approaches:\nM-Coffee: Extends T-Coffee by combining alignments from multiple programs:\n\\[w_{combined}(a_i, b_j) = \\sum_{method} \\alpha_{method} \\cdot w_{method}(a_i, b_j)\\]\nwhere α weights reflect method reliability.\nMergeAlign: Uses a graph-based approach where each alignment contributes edges to a consistency graph. The final alignment maximizes edge weights while maintaining transitivity.\nColumn Score Comparison: Some methods identify reliably aligned columns by comparing alignments:\n\\[Confidence(column_i) = \\frac{|\\{alignments\\_with\\_same\\_column\\}|}{total\\_alignments}\\]\nOnly high-confidence columns are retained in the consensus.\n\n\nStructure-Based and Domain-Aware Alignment\nWhen structural information is available, it can dramatically improve alignment accuracy:\n3D-Coffee: Incorporates structural alignments using programs like SAP or MUSTANG. Structure-based scores override sequence-based scores in structurally conserved regions:\n\\[Score_{combined} = \\beta \\cdot Score_{structure} + (1-\\beta) \\cdot Score_{sequence}\\]\nwhere β reflects confidence in structural alignment.\nDomain Architecture: Programs like PRALINE recognize domain boundaries and align domains independently: 1. Identify domains using databases like Pfam 2. Align domains separately 3. Assemble full alignment respecting domain boundaries\nThis approach prevents incorrect alignment across domain boundaries, a common source of errors in traditional methods.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/multiple_alignment.html#computational-optimizations-and-scalability",
    "href": "chapters/bioinformatics/multiple_alignment.html#computational-optimizations-and-scalability",
    "title": "38  Multiple Alignment",
    "section": "Computational Optimizations and Scalability",
    "text": "Computational Optimizations and Scalability\n\nAlgorithmic Improvements\nModern alignment programs employ numerous optimizations to handle large-scale data:\nBanded Dynamic Programming: For similar sequences, limit computation to a diagonal band:\n\\[|i - j| \\leq bandwidth\\]\nThis reduces complexity from O(n²) to O(n × bandwidth).\nAnchor Points: Identify highly conserved regions as anchors, then align between anchors: 1. Find exact matches or high-scoring segment pairs 2. Chain anchors consistently 3. Align regions between anchors\nThis divide-and-conquer approach significantly reduces computation for long sequences.\nSparse Dynamic Programming: Store only cells that might contribute to the optimal path, reducing memory from O(n²) to O(n) in favorable cases.\n\n\nParallelization Strategies\nMultiple alignment is amenable to various parallelization approaches:\nPairwise Distance Calculation: Computing k(k-1)/2 pairwise distances is embarrassingly parallel:\nparallel_for each pair (i,j):\n    distance[i][j] = compute_distance(seq[i], seq[j])\nGuide Tree Parallelization: Different subtrees can be aligned independently:\nparallel_for each subtree:\n    align_subtree(node)\nwait_for_children()\nmerge_alignments()\nProfile-Profile Alignment: Dynamic programming cells along anti-diagonals can be computed in parallel:\n\\[cell[i][j] = f(cell[i-1][j], cell[i][j-1], cell[i-1][j-1])\\]\nModern tools like Clustal Omega and MAFFT exploit multi-core architectures and SIMD instructions for substantial speedups.\n\n\nMemory-Efficient Implementations\nLarge-scale alignments require careful memory management:\nLinear Space Alignment: Using Hirschberg’s algorithm, compute alignments in O(n) space by recursively dividing the problem: 1. Find midpoint of optimal path 2. Recursively align two subproblems 3. Concatenate results\nCompressed Profiles: Store profiles using reduced alphabets or clustering similar residues:\n\\[P_{compressed}[i] = cluster(P_{original}[i])\\]\nExternal Memory Algorithms: For massive datasets, use disk-based storage with careful I/O optimization: - B-tree indices for sequence access - Memory-mapped files for profiles - Compressed sequence representation",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/multiple_alignment.html#validation-quality-assessment-and-best-practices",
    "href": "chapters/bioinformatics/multiple_alignment.html#validation-quality-assessment-and-best-practices",
    "title": "38  Multiple Alignment",
    "section": "Validation, Quality Assessment, and Best Practices",
    "text": "Validation, Quality Assessment, and Best Practices\n\nAlignment Quality Metrics\nAssessing alignment quality without knowing the true alignment is challenging but crucial:\nColumn Confidence Scores: Programs like TCS (Transitive Consistency Score) measure how consistently a column is aligned across different methods:\n\\[TCS(column) = \\frac{\\sum_{triplets} consistency(triplet)}{total\\_triplets}\\]\nSum-of-Pairs Score: The total score of all pairwise alignments within the MSA:\n\\[SP = \\sum_{i&lt;j} score(seq_i, seq_j)\\]\nWhile easy to compute, SP scores can be misleading for divergent sequences.\nInformation Content: Measures conservation using Shannon entropy:\n\\[IC(column) = 2 - H(column) = 2 + \\sum_{a} p_a \\log_2(p_a)\\]\nHigh information content indicates strong conservation.\nNormed Discrepancy: Compares an alignment to a reference or expected distribution:\n\\[ND = \\frac{|observed - expected|}{expected}\\]\n\n\nBenchmark Datasets and Validation\nSeveral benchmark databases enable objective comparison of alignment methods:\nBAliBASE: Manually curated structural alignments covering various alignment challenges: - RV11: Equidistant sequences with &lt;20% identity - RV12: Families with &gt;35% identity - RV20: Families with large insertions - RV30: Subgroups with &lt;25% between-group identity - RV40: Sequences with large terminal extensions - RV50: Sequences with internal insertions\nPREFAB: Large-scale benchmark with 1,932 alignments based on structural pairs.\nOXBench: Focuses on challenging cases with low sequence identity.\nPerformance metrics include: - Sum-of-pairs score (SPS): Fraction of correctly aligned residue pairs - Column score (CS): Fraction of correctly aligned columns - Total column score (TCS): Considers partial column correctness\n\n\nBest Practices and Recommendations\nSequence Selection: - Remove redundant sequences (&gt;90% identity) to avoid bias - Include sequences across the full taxonomic range - Ensure sufficient sequence diversity for meaningful patterns - Check for frameshifts and sequencing errors\nMethod Selection: - For &lt;50 sequences with high similarity: ClustalW or MUSCLE - For large datasets (&gt;1000 sequences): Clustal Omega or MAFFT (FFT-NS-2) - For high accuracy with &lt;200 sequences: MAFFT (L-INS-i) or T-Coffee - For phylogeny-aware alignment: PRANK or PAGAN - When structure is available: 3D-Coffee or Expresso\nParameter Optimization: - Test different gap penalties for your sequence family - Consider secondary structure and solvent accessibility - Adjust substitution matrices based on sequence divergence - Use iterative refinement for improved accuracy\nQuality Control: - Visually inspect alignments for obvious errors - Check for excessive gaps or fragmented alignments - Compare results from multiple methods - Use consistency-based confidence scores - Validate against structural data when available",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/multiple_alignment.html#future-directions-and-emerging-challenges",
    "href": "chapters/bioinformatics/multiple_alignment.html#future-directions-and-emerging-challenges",
    "title": "38  Multiple Alignment",
    "section": "Future Directions and Emerging Challenges",
    "text": "Future Directions and Emerging Challenges\n\nMachine Learning and Deep Learning Approaches\nRecent advances in deep learning are beginning to impact multiple sequence alignment:\nNeural Network Scoring Functions: Deep networks can learn complex scoring functions from large databases of verified alignments:\n\\[Score_{NN} = f_{neural}(sequence\\_features, structural\\_features)\\]\nThese learned functions can capture patterns that escape traditional substitution matrices.\nAttention Mechanisms: Transformer architectures, successful in language modeling, show promise for capturing long-range dependencies in sequences:\n\\[Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\\]\nThis could better model coevolution and functional constraints.\nReinforcement Learning: Treating alignment as a sequential decision problem, RL agents can learn optimal alignment strategies:\n\\[\\pi^*(s) = \\arg\\max_a Q^*(s,a)\\]\nwhere states represent partial alignments and actions are alignment operations.\n\n\nIntegration with Structural Prediction\nThe AlphaFold revolution in protein structure prediction creates new opportunities:\nStructure-Guided Alignment: Use predicted structures to inform sequence alignment: 1. Predict structures for all sequences 2. Perform structural superposition 3. Derive sequence alignment from structural alignment\nCoevolution and Contact Prediction: MSAs reveal coevolving positions that are often in physical contact:\n\\[MI(i,j) = \\sum_{a,b} P(a_i, b_j) \\log\\frac{P(a_i, b_j)}{P(a_i)P(b_j)}\\]\nThese signals can both improve alignment and predict structure.\n\n\nHandling Big Data and Genomic Scales\nThe explosion of sequencing data presents new challenges:\nStreaming Algorithms: Process sequences as they arrive without storing entire datasets:\nfor each new_sequence:\n    find_best_position(new_sequence, current_alignment)\n    update_alignment_incrementally()\nApproximate Methods: Trade exact solutions for scalability: - Locality-sensitive hashing for similar sequence detection - Sketching algorithms for distance estimation - Sampling-based approaches for large families\nCloud and Distributed Computing: Leverage cloud infrastructure for massive alignments: - MapReduce for pairwise distance calculation - Distributed graph algorithms for guide tree construction - Federated learning for privacy-preserving alignment\n\n\nSpecialized Applications\nMultiple alignment continues to evolve for specific biological problems:\nViral Quasispecies: Align highly similar sequences with rare variations: - Distinguish true variants from sequencing errors - Track evolution within hosts - Identify drug resistance mutations\nMetagenomics: Align sequences from multiple organisms: - Handle varying abundance levels - Separate strain-level variations - Account for horizontal gene transfer\nLong-Read Alignment: Adapt methods for third-generation sequencing: - Handle higher error rates - Exploit long-range information - Integrate with genome assembly\nRNA Structure: Incorporate secondary structure into alignment: - Simultaneous alignment and folding - Covariation analysis for structure prediction - Pseudoknot-aware alignment",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/multiple_alignment.html#conclusion",
    "href": "chapters/bioinformatics/multiple_alignment.html#conclusion",
    "title": "38  Multiple Alignment",
    "section": "Conclusion",
    "text": "Conclusion\nMultiple sequence alignment remains a central challenge in computational biology, embodying the tension between biological complexity and computational tractability. The NP-completeness of the problem ensures that exact solutions remain out of reach for all but the smallest datasets, making heuristic approaches not just practical alternatives but essential tools.\nThe dominance of progressive alignment, despite its well-understood limitations, reflects a pragmatic balance between speed, accuracy, and biological intuition. The greedy nature of these algorithms, while theoretically suboptimal, often produces biologically meaningful results by following evolutionary relationships. However, the tendency to accumulate and retain errors, particularly in gap placement, remains a significant challenge that no current method fully resolves.\nThe proliferation of alignment methods—from the venerable ClustalW to modern tools like MAFFT and T-Coffee—reflects the diversity of biological problems and the impossibility of a one-size-fits-all solution. Each method embodies different trade-offs between speed and accuracy, different assumptions about sequence evolution, and different strategies for handling the fundamental challenges of multiple alignment.\nLooking forward, the field stands at an exciting juncture. The integration of machine learning approaches promises to capture complex patterns that escape current methods. The availability of predicted structures for most proteins opens new avenues for structure-guided alignment. The continued explosion of sequence data demands ever more efficient algorithms while maintaining or improving accuracy.\nYet fundamental challenges remain. The treatment of insertions and deletions, the handling of repetitive and low-complexity regions, the alignment of highly divergent sequences, and the integration of various types of biological information all present ongoing research opportunities. The multiple sequence alignment problem, far from being solved, continues to evolve with our understanding of molecular evolution and our computational capabilities.\nThe story of multiple sequence alignment is ultimately one of creative compromise—between optimal and feasible, between general and specific, between automated and manual. As we continue to push the boundaries of what’s computationally possible and biologically meaningful, multiple sequence alignment will remain at the heart of comparative genomics, structural biology, and our understanding of molecular evolution. The next generation of methods will need to balance ever-increasing data scales with the need for accurate, interpretable alignments that capture the complex evolutionary relationships among sequences.\nThe continuing citation success of tools like ClustalW, decades after their publication, testament not just to their utility but to the fundamental importance of multiple sequence alignment in modern biology. As we enter an era of population-scale genomics and routine structural prediction, the ancient problem of arranging sequences to reveal their evolutionary relationships remains as relevant and challenging as ever.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple Alignment</span>"
    ]
  },
  {
    "objectID": "chapters/project/alignment_project/index.html",
    "href": "chapters/project/alignment_project/index.html",
    "title": "39  Pairwise alignment",
    "section": "",
    "text": "Filling in the dynamic programming matrix\nThis chapter is about global pairwise alignment, and you will implement your own Needleman-Wunch algorithm.\nYour task is to find an optimal alignment of two sequences. If two such sequences are roughly 140 bases long, there are as many different ways to align them as there are atoms in the visible universe. Finding an optimal alignment among those \\(10^{80}\\) possibilities is a hard problem, but implementing the Needleman-Wunch algorithm will let you do it.\nThis project is meant to train your coding abilities and consolidate your understanding of the Needleman-Wunch algorithm. The better you understand the algorithm before you begin, the easier and more rewarding the project will be. So, re-read the book chapter about pairwise alignment and browse the lecture slides.\nDownload the files you need for this project:\nhttps://munch-group.org/bioinformatics/supplementary/project_files - alignmentproject.py is the file where you must write your code. It already contains a function I wrote for you. - test_alignmentproject.py is the test program that lets you test the code you write in alignmentproject.py.\nPut the files in a folder dedicated to this project. On most computers, you can right-click on the link and choose “Save the file as…” or “Download linked file.”\nThe project is split into two parts:\nTo help you along, the alignmentproject.py file already contains a function I wrote for you, so you can print your dynamic programming (DP) matrix as you gradually fill it in. You are not expected to understand how this function works. I made it as condensed as possible so it does not take up so much space in your file.\nThe function is named print_dp_matrix and takes two arguments:\nThe function returns:",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Pairwise alignment</span>"
    ]
  },
  {
    "objectID": "chapters/project/alignment_project/index.html#filling-in-the-dynamic-programming-matrix",
    "href": "chapters/project/alignment_project/index.html#filling-in-the-dynamic-programming-matrix",
    "title": "39  Pairwise alignment",
    "section": "",
    "text": "Make a matrix\nWe start by making a list of lists (a matrix) with the right shape but only holds None values. We use the None values as placeholders, which you can later replace with scores. You can think of it as an empty matrix into which you can fill scores, just as we did in the lectures. If you want to align two sequences like AT and GAT, you want a matrix with three rows and four columns. Note that the matrix must have one more row than the number of bases in sequence one and one more column than the number of bases in sequence two.\nWrite a function, empty_matrix, that takes two arguments\n\nAn integer (which represents the length of sequence one + 1).\nAn integer (which represents the length of sequence two + 1).\n\nThe function must return:\n\nA list of lists. The number of sub-lists must be equal to the first integer argument. Each sublist must contain a number of None values equal to the second integer argument.\n\nExample usage:\nempty_matrix(3, 4)\nreturns a list with three lists each of length four:\n[[None, None, None, None], [None, None, None, None], [None, None, None, None]]\nEven though this is a list of lists, we can think of it as a three-by-four matrix:\n[[None, None, None, None], \n [None, None, None, None], \n [None, None, None, None]]\nIf you want to print the matrix in a way that looks like the slides I showed you at the lecture, you can use the print_dp_matrix function (again, None represents empty cells):\n          G    A    T\n  None None None None\nA None None None None\nT None None None None\nImportant: You can implement empty_matrix in a way that superficially looks ok but will cause you all kinds of grief when you start filling it in. When you create the list of lists (e.g., three as above), you must generate and add three separate lists. If you add the same list three times, you do not have three separate rows in your matrix. Instead, you have three references to the same row. You can test if you did it right this way – by changing the value of one cell to see what happens:\nempty = empty_matrix(3, 4)\nempty[0][0] = 'Mogens'\nprint(empty)\nIf this only changed one value like below, you are ok:\n[['Mogens', None, None, None], [None, None, None, None], [None, None, None, None]]\nIf it changed the first value in all the lists, it means that all your lists are the same (which is not what you want).\n[['Mogens', None, None, None], ['Mogens', None, None, None], ['Mogens', None, None, None]]\n\n\nFill the top row and left column\nNow that you can make a matrix with the correct dimensions, you need to write a function that fills in the top row and the left column with multiples of the gap score. E.g., if the gap score is -2, you want the matrix to look something like this when you print it with print_dp_matrix:\n          G    A    T\n     0   -2   -4   -6\nA   -2 None None None\nT   -4 None None None\nWrite a function, prepare_matrix, which takes three arguments:\n\nAn integer (which represents the length of sequence one plus one)\nAn integer (which represents the length of sequence two plus one)\nAn integer, which represents the gap_score used for alignment.\n\nThe function must return:\n\nA list of lists. The number of sub-lists must be equal to the first integer argument. The values in the first sub-list must be multiples of the gap score given as the third argument. The first elements of the remaining sub-lists must be multiples of the gap score. All remaining elements of sub-lists must be None.\n\nExample usage:\nprepare_matrix(3, 4, -2)\nmust return:\n[[0, -2, -4, -6], [-2, None, None, None], [-4, None, None, None]]\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\nYou should call empty_matrix inside prepare_matrix to get a matrix filled with None.\n\n\n\nNow, all you need to do is replace the right None values with multiples of the gap score. For example, the third element in the first sub-list is matrix[2][0], for which you would need to assign the value 2 times the gap score. In the same way matrix[3][0] should be 3 times the gap score. So, you need to figure out which elements you should replace and which pairs of indexes you need to access those elements. Then, use range to generate those indexes and for-loops to loop them over.\n\n\nFill the entire matrix\nNow that we can fill the top row and left column, we can start thinking about how to fill the whole matrix.\nFor that, we need a score matrix of match scores. In Python, that is most easily represented as a dictionary of dictionaries like this:\nscore_matrix = {'A': {'A': 2, 'T': 0, 'G': 0, 'C': 0},\n                'T': {'A': 0, 'T': 2, 'G': 0, 'C': 0},\n                'G': {'A': 0, 'T': 0, 'G': 2, 'C': 0},\n                'C': {'A': 0, 'T': 0, 'G': 0, 'C': 2}}\nThat lets you get the score for matching an A with a T like this: score_matrix['A']['T']. Note that the match scores are only for uppercase letters (A, T, G, C).\nWrite a function, fill_matrix, which takes four arguments:\n\nA string, which represents the first sequence.\nA string, which represents the second sequence.\nA dictionary of dictionaries like the one shown above, which represents match scores.\nAn integer, which represents the gap score.\n\nThe function must return:\n\nA list of lists of integers, which represents a correctly filled dynamic programming matrix given the two sequences, the match scores, and the gap score.\n\nExample usage: If score_matrix is defined as above, then\nfill_matrix('AT', 'GAT', score_matrix, -2)\nmust return:\n[[0, -2, -4, -6], [-2, 0, 0, -2], [-4, -2, 0, 2]]\nIf you print that matrix using print_dp_matrix it should look like this:\n      G  A  T\n   0 -2 -4 -6\nA -2  0  0 -2\nT -4 -2  0  2\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\nYou should call prepare_matrix inside fill_matrix to get a matrix with the top row and left column filled. Assuming seq1 is sequence one and seq2 is sequence two, then you can do it like this:\nmatrix = prepare_matrix(len(seq1)+1, len(seq2)+1, gap_score)\n\n\n\nNow, you only need to fill out the rest. You need two nested for-loops to produce the indexes of the elements in the list of lists that you need to assign values to.\nfor i in range(1, len(seq1)+1):\n    for j in range(1, len(seq2)+1):\n        print(i, j) # just to see what i and j are\nExamine this code and make sure you understand why we give those arguments to range. Each combination of i and j lets you access an element matrix[i][j] in matrix (list of lists) that you can assign a value to. The value to assign to matrix[i][j] (green cell on the slides) is the maximum of three values (the yellow cells on the slide):\n\nThe value of the cell to the left (matrix[i][j-1]) plus the gap score.\nThe cell above (matrix[i-1][j]) plus the gap score.\nThe diagonal cell (matrix[i-1][j-1]) plus the match score for base number i (index i-1) of sequence one and base number j (index j-1) of sequence two.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Pairwise alignment</span>"
    ]
  },
  {
    "objectID": "chapters/project/alignment_project/index.html#reconstructing-the-optimal-alignment",
    "href": "chapters/project/alignment_project/index.html#reconstructing-the-optimal-alignment",
    "title": "39  Pairwise alignment",
    "section": "Reconstructing the optimal alignment",
    "text": "Reconstructing the optimal alignment\nThis is the most challenging part, so I will hold your hand here. Below is first a function that identifies which of three cells (the yellow cells on the slides) some cell (green cell on the slides) is derived from. On the slides, this is the cell pointed to by the red arrow.\ndef get_traceback_arrow(matrix, row, col, match_score, gap_score):\n\n    # yellow cells:\n    score_diagonal = matrix[row-1][col-1]\n    score_left = matrix[row][col-1]\n    score_up = matrix[row-1][col]\n\n    # gree cell:\n    score_current = matrix[row][col]\n\n    if score_current == score_diagonal + match_score:\n        return 'diagonal'\n    elif score_current == score_left + gap_score:\n        return 'left'\n    elif score_current == score_up + gap_score:\n        return 'up'\nWrite (do not copy and paste) this into your file, and make sure that it works and that you understand exactly how it works before you proceed.\nHere is a function that uses get_traceback_arrow to do the traceback. It reconstructs the alignment starting from the last column, adding columns in front as the traceback proceeds. It is big, so it breaks across three pages.\ndef trace_back(seq1, seq2, matrix, score_matrix, gap_score):\n\n    # Strings to store the growing alignment strings:\n    aligned1 = ''\n    aligned2 = ''\n\n    # The row and col index of the bottom right cell:\n    row = len(seq1)\n    col = len(seq2)\n\n    # Keep stepping backwards through the matrix untill\n    # we get to the top row or the left col:\n    while row &gt; 0 and col &gt; 0:\n\n        # The two bases we available to match:\n        base1 = seq1[row-1]\n        base2 = seq2[col-1]\n\n        # The score for mathing those two bases:\n        match_score = score_matrix[base1][base2]\n\n        # Find out which cell the score in the current cell was derived from:\n        traceback_arrow = get_traceback_arrow(matrix, row, col, match_score, gap_score)\n\n        if traceback_arrow == 'diagonal':\n                # last column of the sub alignment is base1 over base2:\n            aligned1 = base1 + aligned1\n            aligned2 = base2 + aligned2\n            # next cell is the diagonal cell:\n            row -= 1\n            col -= 1\n        elif traceback_arrow == 'up':\n                # last column in the sub alignment is base1 over a gap:\n            aligned1 = base1 + aligned1\n            aligned2 = '-' + aligned2\n            # next cell is the cell above:\n            row -= 1\n        elif traceback_arrow == 'left':\n                # last column in the sub alignment is a gap over base2:\n            aligned1 = '-' + aligned1\n            aligned2 = base2 + aligned2\n            # next cell is the cell to the left:\n            col -= 1\n\n    # If row is not zero, step along the top row to the top left cell:\n    while row &gt; 0:\n        base1 = seq1[row-1]\n        aligned1 = base1 + aligned1\n        aligned2 = '-' + aligned2\n        row -= 1\n\n    # If col is not zero, step upwards in the left col to the top left cell:\n    while col &gt; 0:\n        base2 = seq2[col-1]\n        aligned1 = '-' + aligned1\n        aligned2 = base2 + aligned2\n        col -= 1\n\n    return [aligned1, aligned2]\nOnce you have written it into your file, make sure you understand the correspondence to the sequences of events on the lecture slides.\nNow you can write a function that performs the alignment. You get to do that yourself. It just calls fill_matrix and then trace_back to get the optimal alignment\nWrite a function, align, that takes four arguments:\n\nA string, which represents sequence one.\nA string, which represents sequence two.\nA dictionary of dictionaries, which represents the match scores (as described above).\nAn integer, which represents the gap score.\n\nThe function must return:\n\nA list of length two. The first element of that list must be a string representing the aligned sequence one. The second element must be a string, representing the aligned sequence two.\n\nExample usage:\nalign('ATAT', 'GATGAT', score_matrix, -2)\nmust return:\n['-AT-AT', 'GATGAT']\nOnce you have written that function, you can print your alignment like this:\nalignment = align('ATAT', 'GATGAT', score_matrix, -2)\nfor s in alignment:\n    print(s)",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Pairwise alignment</span>"
    ]
  },
  {
    "objectID": "chapters/web/ccr5_pwalign/index.html",
    "href": "chapters/web/ccr5_pwalign/index.html",
    "title": "40  CCR5-delta32",
    "section": "",
    "text": "Human Immunodeficiency Virus targets immune cells\nHIV (Human Immunodeficiency Virus) infects immune cells by specifically targeting and binding to certain receptors on the surface of these cells. The primary immune cells that HIV infects are CD4+ T cells (commonly known as T-helper cells), macrophages, and dendritic cells. Here’s a simplified step-by-step explanation of how HIV infects immune cells: The first step in the infection process involves the attachment of HIV to immune cells. The virus carries a glycoprotein on its surface called gp120, which binds to the CD4 receptor on the surface of CD4+ T cells, macrophages, and dendritic cells. This interaction is the initial binding event between the virus and the host cell. After binding to CD4, HIV also requires a co-receptor to gain entry into the host cell. Two common co-receptors used by the virus are CCR5 and CXCR4. Depending on the viral strain, it can use one or both of these co-receptors. This interaction between gp120 and the co-receptor triggers a conformational change in the virus, allowing it to fuse with the host cell membrane. The conformational change in the virus membrane allows it to fuse with the host cell membrane. This fusion event enables the virus to release its genetic material into the interior of the host cell. Once inside the host cell, HIV carries an enzyme called reverse transcriptase, which converts its single-stranded RNA genome into double-stranded DNA. This process is known as reverse transcription.\nThe newly formed viral DNA is transported into the cell’s nucleus, where it is integrated into the host cell’s DNA. The enzyme integrase plays a critical role in this step. Once integrated, the viral DNA becomes a permanent part of the host cell’s genetic material. The integrated viral DNA is transcribed into messenger RNA (mRNA), which is then translated by the host cell’s machinery to produce viral proteins and RNA. This leads to the assembly of new viral particles. New viral particles are assembled in the host cell and are released from the cell’s surface in a process known as budding. The new virus particles can go on to infect other immune cells and continue the cycle of infection. As HIV continues to infect and replicate within CD4+ T cells, macrophages, and dendritic cells, the immune system’s response is compromised. Over time, the gradual loss of CD4+ T cells, which are crucial for coordinating the immune response, weakens the immune system’s ability to defend the body against various infections. This progressive immune system decline eventually leads to the clinical symptoms associated with AIDS (Acquired Immunodeficiency Syndrome).",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>CCR5-delta32</span>"
    ]
  },
  {
    "objectID": "chapters/web/ccr5_pwalign/index.html#can-specific-gene-variants-provide-resistance-to-hiv",
    "href": "chapters/web/ccr5_pwalign/index.html#can-specific-gene-variants-provide-resistance-to-hiv",
    "title": "40  CCR5-delta32",
    "section": "Can specific gene variants provide resistance to HIV?",
    "text": "Can specific gene variants provide resistance to HIV?\nThe CCR5 gene and its protective variant, CCR5-delta32, thus play a pivotal role in the complex interplay between human genetics and HIV. The CCR5 gene, specifically its CCR5-delta32 mutation, has garnered significant attention in the field of HIV research due to its remarkable ability to confer a degree of natural resistance against the virus. The CCR5 gene, short for “C-C chemokine receptor type 5,” encodes a protein receptor found on the surface of certain immune cells, including T-cells and macrophages. This receptor acts as a gateway for HIV to enter these cells, a critical step in the virus’s infection cycle. However, a naturally occurring genetic variant of CCR5 known as CCR5-delta32 possesses a mutation that renders the receptor non-functional. Individuals who inherit two copies of this mutation are notably resistant to HIV infection, as the virus struggles to enter and infect their immune cells. This extraordinary genetic resistance has raised considerable interest in the scientific and medical communities, as it offers insights into potential HIV treatment strategies and the development of innovative therapies.\nThe frequency of the CCR5-delta32 variant varies significantly across different populations worldwide. It is important to note that historical factors, such as population migrations, genetic bottlenecks, and the prevalence of diseases like HIV, can influence the presence and distribution of this protective mutation. Here’s an overview of the frequency of the CCR5-delta32 mutation in different regions: The CCR5-delta32 mutation is most common in populations of Northern European descent, particularly in Scandinavia. In some regions of Northern Europe, such as Sweden and Finland, the mutation can be found in approximately 10-15% of the population. This high prevalence is thought to have resulted from strong selection pressures due to past epidemics, like the bubonic plague. As you move southward in Europe, the frequency of the CCR5-delta32 mutation decreases. In Southern European countries like Spain and Italy, the prevalence drops to about 5% or even lower. The CCR5-delta32 mutation is relatively rare in African, Asian, and Indigenous populations. In most cases, it occurs in less than 1% of these populations. The low frequency can be attributed to the historical lack of selective pressure from diseases like HIV in these regions.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>CCR5-delta32</span>"
    ]
  },
  {
    "objectID": "chapters/web/ccr5_pwalign/index.html#aim",
    "href": "chapters/web/ccr5_pwalign/index.html#aim",
    "title": "40  CCR5-delta32",
    "section": "Aim",
    "text": "Aim\nYour goal is to use pairwise alignment to find the position of the delta32 mutation and establish if it is a deletion, insertion, or nucleotide substitution. This way, you, or other researchers, will be able to learn how the mutation changes the gene’s protein-coding sequence.\nThe learning goals of this exercise are:\n\nAcquire practical experience using online alignemnt tools.\nUnderstanding how parameters in the Needleman-Wunch algorithms affects the alignment.\nUnderstand the different uses of DNA and protein alignment.\nAcquire practical experience using online Blast to search an NCBI sequence database.\nGetting acquainted with different file formats for storing sequence information.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>CCR5-delta32</span>"
    ]
  },
  {
    "objectID": "chapters/web/ccr5_pwalign/index.html#preparation",
    "href": "chapters/web/ccr5_pwalign/index.html#preparation",
    "title": "40  CCR5-delta32",
    "section": "Preparation",
    "text": "Preparation\n\nFasta sequence format\nFiles containing DNA or protein sequences are just text files formatted in a particular way so that each sequence can be associated with additional information. The most simple format is called FASTA format, and files with sequences in this format are called FASTA files and are usually given either a “.fa” or “.fasta” suffix. A FASTA has two elements for each sequence: a header line with a leading ” &gt;” character followed by one or more lines with the DNA or protein sequence (the sequence may be broken over several lines). The content of a FASTA file with two (short) sequences in it could look like this:\n&gt;7423344 some additional description\nAGTCCCTTGCA\nTTATTGCAATAT\n&gt;2342134 some additional description\nGGTCCAATTGC\nAAATTGGAATA\nThe first word after the “&gt;” in the header line is usually a sequence identifier; the rest is an additional description or information.\n\n\nAligning DNA sequences\nFor this this exercise, you will have four fasta files each with one sequence in it:\n\nThe full mRNA of the normal CCR5 gene: CCR5_mRNA.fa\nThe full mRNA of the CCR5-delta32 variant: CCR5_mRNA_delta32.fa\nThe coding sequence (CDS) of CCR5: CCR5_CDS.fa\nThe coding sequence (CDS) of CCR5-delta32: CCR5_CDS_delta32.fa\n\nYou can download the files on Brightspace.\n\nExercise 40-1\nBefore we head into the actual investigation, lets begin by looking at the exon/intron structure of CCR5 by aligning the mRNAs of CCR5 to its CDS using the Needleman-Wunsch algorithm. Go to the EBI website for pairwise alignment. Under “Sequence type” you need to select “DNA” to enable alignment parameters suitable for DNA alignment. Now align the two sequences by copy/pasting the mRNA into the top input field and the CDS into the bottom one (you must include the header line with the “&gt;” character). Leave the rest of the options with their default values.\n\nGuided Reflections\n\nWhat are the other default values? Click “more options” to see what they are.\nWhich score matrix gap penalties are used?\nDoes the program use linear or affine gap scoring?\nAre gaps at the ends of the alignment scored differently than the internal gaps?\n\n\n\n\nExercise 40-2\nNow click “Submit”, to get your job queued on the server. Yol, and the bottom part shows parau have to wait a bit. When the job is completed and the output is shown, the lines beginning with “#” provide additional information about the alignment. The top part list the options used for running the “needle” program in the terminameter values and alignment statistics.\n\nGuided Reflections\n\nWhat gap scoring was used?\nWhich substitution matrix was used?\nWhere do you find the result of the alignment?\nThe alignment shows differences and similarities between the two sequences. Which differences do you see between the mRNA and the CDS? Why is there a difference?\n\n\n\n\nExercise 40-3\nNow, on to the actual investigation. Begin by aligning the mRNAs of CCR5 and CCR5-delta32 to each other using the Needleman-Wunsch algorithm using the default values of parameters. Look at the alignment results.\n\nGuided Reflections\n\nDescribe the similarities between the two sequences. How many bases are the same and where are they located?\nDescribe the differences between the two sequences. Where are the differences located on the mRNA and how many bases are involved? (Hint: 5’-UTR, 3’-UTR, Coding region)",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>CCR5-delta32</span>"
    ]
  },
  {
    "objectID": "chapters/web/ccr5_pwalign/index.html#aligning-protein-sequences",
    "href": "chapters/web/ccr5_pwalign/index.html#aligning-protein-sequences",
    "title": "40  CCR5-delta32",
    "section": "Aligning protein sequences",
    "text": "Aligning protein sequences\n\nExercise 40-4\nNow, you will translate the coding sequence of CCR5 and CCR5-delta32 to see which effect the mutation has on the protein level. Go to the Expasy translate website and set the “Output format” to “verbose”. Then, translate the two coding sequences, one at a time, by pasting the FASTA entries into the field (including the header line).\n\nGuided Reflections\n\nWhat are the likely reading frames of each sequence? (number and direction)\nWhich differences do you observe based on the translated sequence?\n\n\n\n\nExercise 40-5\nNow go back to the EMBOSS Needle website and align the two protein sequences. Look at the results of the alignment.\n\nGuided Reflections\n\nWhere are the similarities between the two protein sequences?\nWhere are the differences between the two protein sequences?\nHow does the mutation affect the protein? (Hints: deletion, insertion, missense, nonsense, frameshift)",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>CCR5-delta32</span>"
    ]
  },
  {
    "objectID": "chapters/web/ccr5_pwalign/index.html#searching-a-database-using-blast",
    "href": "chapters/web/ccr5_pwalign/index.html#searching-a-database-using-blast",
    "title": "40  CCR5-delta32",
    "section": "Searching a database using Blast",
    "text": "Searching a database using Blast\nSooty mangabay (Cercocebus atys) is a species of primates. Sooty mangabays also have common variants in their CCR5 gene that seem to protect them against SIV (simian immunodeficiency virus). We want to investigate how the Sooty mangabay CCR5 variants are similar or different to the gene variants found in the human CCR5.\n\nExercise 40-6\nYou can use Blast to find and investigate these sequences too. Go to the Blast website at NCBI. Clicking “Nucleotide BLAST” will take you to the search interface for the versions of Blast optimized for DNA and RNA searches. Leave the settings to their default values, but take a look around to see what options are available. “Program Selection” lists the version of Blast. We are using megablast for this.\n\nGuided Reflections\n\nClick the “?” icon to learn a bit about each one. How do you think megablast and blastn may differ in the kmer sizes and thresholds they use?\n\n\n\n\nExercise 40-7\nBefore you start the database search, you want to limit its scope to only sequences from sooty mangabey. Set organism to sooty mangabey (“sooty mangabey (taxid:9531)”). Once you do this, it should look like Figure 40.1. Now paste in the CCR5 CDS and click the blue “BLAST” button at the bottom of the page to queue your search request (you may have to wait a bit).\n\n\n\n\n\n\nFigure 40.1: BLAST search interface\n\n\n\nOnce the search results appear, they are sorted by E-value. The light blue header row lets you sort them by other criteria. “Query cover” sorts the database hits by how much of your search sequence (query) they cover.\n\nGuided Reflections\n\nWhat does the E-value mean?\nWhat types of sequences does the BLAST search return?\n\n\n\n\nExercise 40-8\nSince you are looking for search hits to a CDS in the other species, look for “C-C motif chemokine receptor 5 (CCR5), complete CDS” in the descriptions of search hits and pick the one with the highest percent identity. Clicking the corresponding link in the description column takes you to a local alignment of the part of your query sequence that aligns with the database sequence. In this case, it covers your entire CCR5 CDS.\n\nGuided Reflections\n\nDescribe the differences shown by the alignment. How many bases are different? How many gaps are there?",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>CCR5-delta32</span>"
    ]
  },
  {
    "objectID": "chapters/web/ccr5_pwalign/index.html#genbank-sequence-format",
    "href": "chapters/web/ccr5_pwalign/index.html#genbank-sequence-format",
    "title": "40  CCR5-delta32",
    "section": "GenBank sequence format",
    "text": "GenBank sequence format\n\nExercise 40-9\nNow go back to the list of search hits by clicking the “Descriptions” link in the light blue bar above the alignment you were just looking at. See if you can find any search hits with names including “delta”. There should be a delta delta-24 allele among them. Pick the one with sequence ID AF079473.1. Inspect the alignment to the human CCR5.\n\nGuided Reflections\n\nWhat characterizes these variants? What does the “delta” in the variant name refer to?\nGiven what you now know about the human and mangabey CCR5 genes, do you expect that the delta32 mutation to have arisen before or after the common ancestor of humans and mangabeys?\n\n\n\n\nExercise 40-10\nGo back to the descriptions list by clicking the “Descriptions” link in the light blue bar. On the left are tick boxes for each search hit. Mark only the two you have been looking at (IDs AF051905.1 and AF079473.1). In the “Download” dropdown list pick “GenBank” to download the two sequences in GenBank format. GenBank is a sequence entry format like Fasta, including much more information about each sequence. Open the downloaded file in VScode and see what it looks like.\n\nGuided Reflections\n\nWhen were the sequences determined/published?\nWhat sequencing method was used?\nFor each sequence, note another piece of information provided in the GenBank entry format.\n\n\n\n\nExercise 40-11\nIn the GenBank file, you can find the translated sequences of each CDS. Make a new text file in VScode and paste the two translated sequences in to create a Fasta file like this:\n&gt;normal\nMDYQVSSPTYDIDYYTSEPCQKINVKQIAARLLPPLYSLVFIFG\n… rest of the sequence …\n\n&gt;delta24\nMDYQVSSPTYDIDYYTSEPCQKINVKQIAARLLPPLYSLVFIFG\n… rest of the sequence …    \nNow, head back to the EBI needle website and align the two protein sequences. Make sure it is set to protein this time.\n\nGuided Reflections\n\nWhere is the deletion located?\nDoes it just delete amino acids, and if so, how many, or does it change the reading frame or induce stop codons?\nHow does the mangabey CCR5 mutation compare to that in humans?",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>CCR5-delta32</span>"
    ]
  },
  {
    "objectID": "chapters/web/ccr5_pwalign/index.html#putting-it-all-together",
    "href": "chapters/web/ccr5_pwalign/index.html#putting-it-all-together",
    "title": "40  CCR5-delta32",
    "section": "Putting it all together",
    "text": "Putting it all together\n\nExercise 40-12\nHave a look at this scientific paper: “A Novel CCR5 Mutation Common in Sooty Mangabeys Reveals SIVsmm Infection of CCR5-Null Natural Hosts and Efficient Alternative Coreceptor Use In Vivo” and look at Figure 1 and its figure legend.\n\nGuided Reflections\n\nFigure 1A shows the alignment of 6 different gene variants. What species are they from and what variants are shown?\nFigure 1B shows the alignment of the predicted protein sequence. What do the grey boxes mean? What do the arrows mean? What does the highlighted sequence mean?\nWe have now looked at the sequences of different mutations. Suggest an experimental method/setup, that might show something about the functional importance of the mutations.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>CCR5-delta32</span>"
    ]
  },
  {
    "objectID": "chapters/web/ccr5_pwalign/index.html#project-files",
    "href": "chapters/web/ccr5_pwalign/index.html#project-files",
    "title": "40  CCR5-delta32",
    "section": "Project files",
    "text": "Project files\nDownload the files you need for this project:\n \nhttps://munch-group.org/bioinformatics/supplementary/project_files",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>CCR5-delta32</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/protein_substitution_matrices.html",
    "href": "chapters/bioinformatics/protein_substitution_matrices.html",
    "title": "41  Scoring Matrices",
    "section": "",
    "text": "Introduction to Protein Sequence Scoring\nHere’s the thing about protein sequence comparison—it’s vastly more complex than DNA alignment, and that complexity is actually what makes it interesting. When you’re comparing DNA sequences, you’re working with just four nucleotides: \\(A\\), \\(C\\), \\(G\\), and \\(T\\). Pretty straightforward, right? But proteins? They’re built from twenty different amino acids, and here’s where it gets fascinating: each of those amino acids has its own unique chemical personality.\nG\n\n\n\nHydrophobic\n \nHydrophobic:\n \n\n\n\nPolar uncharged\n \nPolar uncharged:\n \n\n\n\n\nAla\n\nAla\n\n\n\n\nPositively charged\n \nPositively charged:\n \n\n\n\n\nSer\n\nSer\n\n\n\n\nNegatively charged\n \nNegatively charged:\n \n\n\n\n\nLys\n\nLys\n\n\n\n\nSpecial\n \nSpecial:\n \n\n\n\n\nAsp\n\nAsp\n\n\n\n\nGly\n\nGly\n\n\n\n\nVal\n\nVal\n\n\n\n\n\nIle\n\nIle\n\n\n\n\nThr\n\nThr\n\n\n\n\nLeu\n\nLeu\n\n\n\n\nCys\n\nCys\n\n\n\n\nMet\n\nMet\n\n\n\n\nAsn\n\nAsn\n\n\n\n\nPhe\n\nPhe\n\n\n\n\nGln\n\nGln\n\n\n\n\nTrp\n\nTrp\n\n\n\n\nTyr\n\nTyr\n\n\n\n\nPro\n\nPro\n\n\n\n\n\n\n\nArg\n\nArg\n\n\n\n\n\nHis\n\nHis\n\n\n\n\n\n\n\n\n\nGlu\n\nGlu\n\n\n\n\n\n\n\n\n\nFigure 41.1: Chemical groupings of the 20 standard amino acids. Amino acids with similar chemical properties tend to substitute for each other more readily during evolution.\nSome are hydrophobic and want to hide from water, some are charged and love interacting with their surroundings, some are tiny and flexible, others are bulky and rigid. These chemical properties aren’t just biochemical trivia—they fundamentally shape how proteins evolve. When a mutation changes one amino acid to another during evolution, whether that change gets accepted or rejected depends heavily on whether the new amino acid can do the same job as the old one. A mutation that swaps one hydrophobic amino acid for another similar hydrophobic amino acid? Usually fine. A mutation that puts a charged amino acid where a hydrophobic one used to be? That’s probably going to break something. This is why comparing protein sequences requires sophisticated scoring schemes that capture these subtle patterns of amino acid replacement. We can’t just score all matches the same and all mismatches the same—we need scoring systems that understand that leucine to isoleucine is a much more reasonable substitution than tryptophan to aspartic acid. The whole challenge of protein sequence alignment boils down to quantifying these substitution patterns in a way that reflects what actually happens during evolution.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Scoring Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/protein_substitution_matrices.html#introduction-to-protein-sequence-scoring",
    "href": "chapters/bioinformatics/protein_substitution_matrices.html#introduction-to-protein-sequence-scoring",
    "title": "41  Scoring Matrices",
    "section": "",
    "text": "Evolution as Nature’s Experiment in Protein Engineering\nThink of evolution as a massive, billion-year experiment in protein engineering. Every time a mutation occurs in a protein-coding gene, nature tests whether that change works. If the new amino acid does the job well enough, the organism survives and reproduces, passing that mutation along. If the change breaks something critical, natural selection eliminates it. This process has been running for billions of years, generating an enormous dataset of which amino acid substitutions work and which don’t. When you compare homologous proteins across different species—say, hemoglobin in humans versus hemoglobin in mice—you’re essentially reading the results of this experiment. The substitutions you observe are the ones that passed nature’s filter. The substitutions you don’t see? Those are the ones that failed. What’s remarkable is that these patterns aren’t random. Conservative substitutions—those swapping one amino acid for another with similar chemical properties—show up all the time because they rarely break anything important. Leucine to valine? Both are hydrophobic, both are roughly similar in size, so this substitution usually preserves protein structure and function. Serine to threonine? Both are small, both are polar, both can form similar hydrogen bonds—again, usually fine. But radical substitutions between chemically different amino acids are rare in the evolutionary record. Try putting an aspartic acid (charged, hydrophilic, small) where a tryptophan (uncharged, hydrophobic, bulky) used to be, and you’ll probably destroy the protein’s ability to fold correctly or carry out its function. Natural selection acts as a filter, keeping the mutations that maintain or improve function while ruthlessly eliminating the ones that cause problems. The protein sequences we observe today represent the survivors—the accumulated substitutions that were functionally acceptable over evolutionary time.\n\n\nQuantifying Substitution Patterns\nSo we’ve established that evolution generates useful data about amino acid substitutions. But how do we turn those observations into actual numbers we can use for sequence alignment? This is where things get mathematically interesting, because the process isn’t as simple as just counting how often each substitution appears. We need to address several subtle challenges. First challenge: distinguishing real evolutionary relationships from coincidence. If you see two sequences both have leucine at the same position, is that because they share a common ancestor (homology), or is it just random chance? Leucine is pretty common, so you’d expect to see it align with itself sometimes even in completely unrelated sequences. Second challenge: accounting for the fact that different amino acids have different background frequencies. Tryptophan is rare in proteins generally—it makes up maybe \\(1\\%\\) of all amino acids. Leucine, on the other hand, is abundant at around \\(10\\%\\). So you’ll naturally see more leucine-leucine alignments than tryptophan-tryptophan alignments even in random sequences, just because there’s more leucine to go around. We need to correct for these frequency differences if we want to identify substitutions that are genuinely meaningful rather than just common. Third challenge: as evolutionary distance increases, you get multiple substitutions happening at the same position. What started as an alanine might have mutated to glycine, then later to serine. When we compare these distant sequences, we only see the beginning and end states, not the intermediate steps. We need statistical methods that account for this “saturation” effect. The standard approach to all these challenges involves transforming raw substitution counts into log-odds scores. Here’s the key equation:\n\\[S(a,b) = \\log\\left(\\frac{q_{ab}}{p_a \\cdot p_b}\\right)\\]\nwhere \\(q_{ab}\\) represents the observed frequency of amino acids \\(a\\) and \\(b\\) being aligned in homologous sequences, while \\(p_a\\) and \\(p_b\\) represent the background frequencies of amino acids \\(a\\) and \\(b\\) respectively. The numerator \\(q_{ab}\\) is what we actually observe in related proteins. The denominator \\(p_a \\cdot p_b\\) is what we’d expect to see if sequences were just random strings of amino acids with no evolutionary relationship. If \\(q_{ab}\\) is larger than \\(p_a \\cdot p_b\\), the substitution occurs more often than chance would predict, so the log gives us a positive score—evidence of an evolutionary relationship. If \\(q_{ab}\\) is smaller than expected, we get a negative score—this substitution is rare and penalized. This formulation elegantly captures the core insight: substitutions that occur frequently in evolutionarily related sequences, beyond what random chance would produce, are the ones that should get positive scores in our alignment algorithm.\n\n\nThe Need for Multiple Scoring Matrices\nHere’s something that might surprise you: there’s no such thing as a single “best” substitution matrix. The substitution patterns you observe between closely related proteins look completely different from those you see between distant relatives, and that means we need different scoring matrices optimized for different evolutionary distances. Let me explain why this matters. Imagine comparing human and chimpanzee hemoglobin—these proteins are extremely similar because humans and chimps diverged recently. In closely related sequences like these, most positions are still identical, and the few substitutions you do see are almost always conservative: leucine to isoleucine, serine to threonine, that kind of thing. These are the “easy” substitutions that happen readily. Now imagine comparing human hemoglobin to sea urchin hemoglobin. These organisms diverged hundreds of millions of years ago, so their sequences have accumulated many more changes. You still see some conservative substitutions, but you also see more radical changes, multiple substitutions at the same position, and the whole picture becomes much more complicated. If you use a scoring matrix optimized for closely related sequences (like BLOSUM80 or PAM40) to compare distant sequences, you’ll fail to detect the relationship because you’re being too picky—you’re penalizing substitutions that are perfectly normal at that evolutionary distance. Conversely, if you use a matrix optimized for distant sequences (like BLOSUM45 or PAM250) to compare closely related sequences, you’ll be too permissive, accepting substitutions and gaps where you should demand near-perfect matches. This trade-off between sensitivity and specificity is fundamental to sequence analysis. Matrices for close relationships are stringent—they strongly penalize most substitutions and reward only highly conservative changes. Great for distinguishing orthologs from paralogs in closely related species, terrible for detecting remote homologs. Matrices for distant relationships are permissive—they accept a much broader range of substitutions. Great for finding distant relatives, but they’ll generate false positives if you use them on closely related sequences. The practical upshot is that choosing the right matrix depends on knowing or estimating the evolutionary distance between your sequences, and having a whole toolkit of matrices optimized for different distances gives you the flexibility to handle whatever comparison you’re doing.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Scoring Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/protein_substitution_matrices.html#pam-matrices-modeling-evolution-through-time",
    "href": "chapters/bioinformatics/protein_substitution_matrices.html#pam-matrices-modeling-evolution-through-time",
    "title": "41  Scoring Matrices",
    "section": "PAM Matrices: Modeling Evolution Through Time",
    "text": "PAM Matrices: Modeling Evolution Through Time\n\nThe Evolutionary Model Behind PAM\nThe Point Accepted Mutation (PAM) matrices represent one of the foundational achievements in computational biology, and they come with a fascinating origin story. Back in the 1970s, Margaret Dayhoff—often called the founding mother of bioinformatics—had a visionary idea: what if we could capture the patterns of protein evolution mathematically and use those patterns to improve sequence analysis? This was revolutionary thinking at a time when most people were still figuring out how to sequence proteins at all. Dayhoff didn’t just develop PAM matrices; she built the first comprehensive protein sequence database (the Atlas of Protein Sequence and Structure), established fundamental methods for sequence comparison, and basically laid the groundwork for the entire field of bioinformatics. Working with punch cards and early computers, she meticulously collected and analyzed protein sequences to extract the evolutionary signal hidden in substitution patterns. Her insight was that evolution leaves quantifiable traces, and if you could measure those traces carefully enough, you could build mathematical models that would let you detect evolutionary relationships that might not be obvious from casual inspection. The PAM model itself rests on a clear conceptual foundation: protein evolution happens through point mutations (changes in individual nucleotides that alter amino acids), and some of these mutations get “accepted” by natural selection while others get rejected. The key word here is “accepted”—we’re not counting all mutations that occur, just the ones that persist in populations because they don’t break the protein badly enough to be lethal. This distinction matters because it means PAM matrices capture functional constraints, not just mutational processes. The basic unit of measurement is the PAM unit itself: one PAM unit corresponds to an evolutionary distance where, on average, one accepted point mutation occurs per 100 amino acid positions. So if two sequences are separated by 1 PAM unit of evolution, you’d expect about \\(1\\%\\) of their amino acid positions to differ. Notice that this measures evolutionary time in terms of observed changes, not absolute time—\\(1\\%\\) divergence might take a million years in a slowly evolving protein or just thousands of years in a rapidly evolving one.\nThe mathematical machinery of PAM matrices starts with a mutation probability matrix \\(M\\). Each element \\(M_{ij}\\) represents the probability that amino acid \\(i\\) mutates to amino acid \\(j\\) over one PAM unit of evolutionary time. The diagonal elements \\(M_{ii}\\) tell you the probability of no change—the chance that amino acid \\(i\\) stays as \\(i\\). The off-diagonal elements capture substitution probabilities. Since these are probabilities and something has to happen (either change or no change), they must sum to 1:\n\\[\n\\sum_{j=1}^{20} M_{ij} = 1\n\\]\nfor each amino acid \\(i\\). This matrix \\(M\\) is what we call a stochastic matrix, and it encodes the fundamental evolutionary process—the transition probabilities from one amino acid to another over a small time step.\n\n\nConstructing PAM Matrices from Data\nBuilding a PAM matrix starts with collecting the right kind of data—and this is where Dayhoff’s work was particularly heroic. Remember, this was the 1970s. No automated DNA sequencers, no massive sequence databases, no computational infrastructure. Dayhoff and her team manually collected and aligned protein sequences, working with punch cards and early computers that had less computing power than your phone. The key methodological decision was to focus on very closely related protein sequences—specifically, proteins sharing more than \\(85\\%\\) sequence identity. Why so stringent? Because when sequences are that similar, you can be confident that each difference you observe represents a single substitution event. If you used more divergent sequences, you’d risk observing positions that had mutated multiple times, and that would mess up your statistics—you’d be looking at the endpoint of a complex chain of substitutions rather than individual mutation events. This high-similarity threshold was critical for getting accurate estimates of the basic substitution rates. Dayhoff’s insistence on biological accuracy over computational convenience set standards for rigor that still guide bioinformatics research today. Once you have your aligned sequences, the construction process involves some straightforward counting followed by some clever mathematics. Let \\(A_{ij}\\) represent the number of times you observe amino acid \\(i\\) mutate to amino acid \\(j\\) in your dataset. From these counts, you calculate the relative mutability of each amino acid—essentially, how mutation-prone each amino acid is:\n\\[m_i = \\frac{\\sum_{j \\neq i} A_{ij}}{n_i \\cdot c}\\]\nwhere \\(n_i\\) is the total occurrence of amino acid \\(i\\) in your dataset, and \\(c\\) is a scaling constant chosen so that the average probability of mutation equals \\(0.01\\) (which is what defines 1 PAM unit). Some amino acids are inherently more mutable than others—maybe because their codons are more susceptible to certain mutations, or because changes to them are more often tolerated functionally. This formula captures that variation. Once you have the mutabilities, you construct the actual mutation probability matrix \\(M\\):\n\\[\nM_{ij} = \\begin{cases}\n\\frac{A_{ij}}{\\sum_{k} A_{ik}} \\cdot m_i & \\text{if } i \\neq j \\\\\n1 - m_i & \\text{if } i = j\n\\end{cases}\n\\]\nThe off-diagonal elements tell you: given that amino acid \\(i\\) is going to mutate, what’s the probability it mutates to \\(j\\) specifically? The diagonal elements tell you the probability of no change. This \\(M\\) matrix represents evolution over a tiny time step—just one PAM unit. To model longer evolutionary distances, we’ll use a clever mathematical trick based on the Markov property. \n\n\n\n\n\n\n\n\n\nso\n\n\n\n1\n\nCollect closely related\nprotein sequences\n\n\n\n2\n\nCount observed\nsubstitutions\n\n\n\n1-&gt;2\n\n\n\n\n\n3\n\nCalculate relative\nmutabilities\n\n\n\n2-&gt;3\n\n\n\n\n\n4\n\nConstruct\nPAM1 matrix\n\n\n\n3-&gt;4\n\n\n\n\n\n5\n\nExponentiate\nPAM1 matrix\n\n\n\n5-&gt;4\n\n\n\n\n\n6\n\nCompute PAM-n\nmatrix\n\n\n\n6-&gt;5\n\n\n\n\n\n7\n\nConvert to log-odds\nscores\n\n\n\n6-&gt;7\n\n\n\n\n\n8\n\nPAM-n scoring matrix\n(e.g., PAM250)\n\n\n\n7-&gt;8\n\n\n\n\n\n\n\n\nFigure 41.2: PAM matrix construction workflow. Starting from closely related sequences (&gt;85% identity), substitution counts are collected and normalized to build the PAM1 mutation probability matrix. Matrix exponentiation extrapolates to longer evolutionary distances.\n\n\n\n\n\n\n{\n    const d3 = await require(\"d3@7\");\n    window.d3 = d3; // Make d3 globally available\n    await import(\"https://unpkg.com/d3-graphviz@5.0.2/build/d3-graphviz.js\");\n    const div = html`&lt;div&gt;&lt;/div&gt;`;\n    d3.select(div).graphviz()\n      .renderDot(`\n\n## BEGIN GRAPH #####################\n\ndigraph so \n{\n    margin=\"0.1,0.2\";\n    node[ shape = box fontsize=\"8pt\" ];\n\n    1 [fixedsize=shape width=1.2 height=0.6 label=\"Collect closely related\\nprotein sequences\\n(&gt;85% identity)\"];\n    2 [fixedsize=shape width=1.2 height=0.6 label=\"Count observed\\nsubstitutions\\nA_ij\"];\n    3 [fixedsize=shape width=1.2 height=0.6 label=\"Calculate relative\\nmutabilities\\nm_i\"];\n    4 [fixedsize=shape width=1.2 height=0.6 label=\"Construct PAM1\\nmutation matrix\\nM\"];\n    5 [fixedsize=shape width=1.2 height=0.6 label=\"Matrix exponentiation\\nM^n = M × M × ... × M\"];\n    6 [fixedsize=shape width=1.2 height=0.6 label=\"PAM-n mutation\\nprobability matrix\\nM^(n)\"];\n    7 [fixedsize=shape width=1.2 height=0.6 label=\"Convert to log-odds\\nscoring matrix\\nS^(n)\"];\n    8 [fixedsize=shape width=1.2 height=0.6 label=\"PAM-n scoring matrix\\n(e.g., PAM250)\"];\n\n    { rank = same; 1 2 3 }\n    { rank = same; 6 5 4 }\n    { rank = same; 7 8 }\n\n    1 -&gt; 2 -&gt; 3;\n    3 -&gt; 4;\n    6 -&gt; 5 -&gt; 4 [ dir = back ];\n    6 -&gt; 7;\n    7 -&gt; 8;\n}\n\n## END GRAPH #####################\n\n    `);\n    return div;\n}\n\n\n\n\n\n\n\n\nFigure 41.3: PAM matrix construction workflow. Starting from closely related sequences (&gt;85% identity), substitution counts are collected and normalized to build the PAM1 mutation probability matrix. Matrix exponentiation extrapolates to longer evolutionary distances.\n\n\n\n\n\n\nExtrapolating to Higher PAM Distances\nHere’s where the PAM framework becomes really powerful: we can take our PAM1 matrix (built from very closely related sequences) and extrapolate it to model much longer evolutionary distances. The trick is beautifully simple. Remember that \\(M\\) is a matrix of transition probabilities over one PAM unit. What if you want to know what happens over \\(n\\) PAM units? You just multiply the matrix by itself \\(n\\) times:\n\\[\nM^{(n)} = M^n\n\\]\nThat’s it. Matrix exponentiation. This works because of the Markov property—the probability of being in state \\(j\\) after \\(n\\) steps is the sum over all possible intermediate paths, and matrix multiplication automatically computes exactly that. So \\(M^{(250)}\\) tells you the transition probabilities after 250 PAM units of evolution. This is PAM250, one of the most commonly used matrices, and it corresponds to sequences where approximately \\(80\\%\\) of positions have experienced at least one accepted mutation. At that evolutionary distance, you’re comparing quite distantly related proteins—think human versus yeast versions of the same protein. Once you have \\(M^{(n)}\\), you convert it into actual alignment scores using the log-odds formula:\n\\[\nS^{(n)}_{ij} = 10 \\cdot \\log_{10}\\left(\\frac{M^{(n)}_{ij}}{f_j}\\right)\n\\]\nwhere \\(f_j\\) is the background frequency of amino acid \\(j\\). The factor of 10 and the use of base-10 logarithms are historical quirks from the pre-computer era—they yield integer scores that were convenient for manual calculations. There’s nothing fundamental about them; some modern matrices use different scaling factors or base-2 logarithms. What matters is the ratio inside the log: how much more likely is this substitution in related proteins compared to random chance? One consequence of this model is that sequence identity decreases exponentially with PAM distance:\n\\[\n\\text{Identity} = \\sum_{i=1}^{20} f_i \\cdot M^{(n)}_{ii}\n\\]\nThe diagonal elements \\(M^{(n)}_{ii}\\) represent the probability that amino acid \\(i\\) is still \\(i\\) after \\(n\\) PAM units, weighted by how common that amino acid is. As \\(n\\) increases, these probabilities decrease, and sequence identity drops. At PAM250, you’re down to about \\(20\\%\\) identity—close to what we call the “twilight zone” of sequence comparison, where it becomes very hard to distinguish real homology from random similarity.\n\n\n\n\n\n\n\n\nFigure 41.4: Relationship between PAM distance and expected sequence identity. As evolutionary distance increases, sequence identity decays exponentially. PAM250 corresponds to approximately 20% identity, near the twilight zone of sequence comparison.\n\n\n\n\n\n\n\nWorked Example: Constructing a PAM1 Matrix\nTo illustrate the construction of PAM matrices, let’s work through a simplified example using a subset of amino acids. Consider a dataset of closely related sequences (&gt;85% identity) where we observe the following substitution counts between four amino acids (A, G, V, L):\nStep 1: Count observed substitutions\nFrom our aligned sequences, we observe: - A → G: 30 substitutions - A → V: 10 substitutions - A → L: 5 substitutions - G → V: 8 substitutions - G → L: 4 substitutions - V → L: 25 substitutions\nTotal occurrences in dataset: - A: 1000 positions - G: 800 positions - V: 600 positions - L: 900 positions\nStep 2: Calculate relative mutabilities\nThe relative mutability \\(m_i\\) for each amino acid represents its propensity to mutate:\n\\[\nm_A = \\frac{30 + 10 + 5}{1000 \\cdot c} = \\frac{45}{1000c}\n\\]\n\\[\nm_G = \\frac{30 + 8 + 4}{800 \\cdot c} = \\frac{42}{800c}\n\\]\n\\[\nm_V = \\frac{10 + 8 + 25}{600 \\cdot c} = \\frac{43}{600c}\n\\]\n\\[\nm_L = \\frac{5 + 4 + 25}{900 \\cdot c} = \\frac{34}{900c}\n\\]\nWe choose \\(c\\) such that the average mutability equals 0.01 (1 PAM unit). The average mutability weighted by amino acid frequency is:\n\\[\n\\bar{m} = \\frac{1000 \\cdot m_A + 800 \\cdot m_G + 600 \\cdot m_V + 900 \\cdot m_L}{3300} = 0.01\n\\]\nSolving for \\(c\\): \\(c = 4.8\\)\nThis gives us: - \\(m_A = 0.0094\\) - \\(m_G = 0.0109\\) - \\(m_V = 0.0149\\) - \\(m_L = 0.0079\\)\nStep 3: Construct the PAM1 mutation probability matrix\nThe mutation probability matrix \\(M\\) has diagonal elements representing no change and off-diagonal elements representing substitutions:\n\\[\nM_{AA} = 1 - m_A = 0.9906\n\\]\n\\[\nM_{AG} = \\frac{30}{45} \\cdot m_A = 0.0063\n\\]\n\\[\nM_{AV} = \\frac{10}{45} \\cdot m_A = 0.0021\n\\]\n\\[\nM_{AL} = \\frac{5}{45} \\cdot m_A = 0.0010\n\\]\nFollowing this pattern for all amino acids:\n\\[\nM = \\begin{pmatrix}\n0.9906 & 0.0063 & 0.0021 & 0.0010 \\\\\n0.0078 & 0.9891 & 0.0021 & 0.0010 \\\\\n0.0029 & 0.0031 & 0.9851 & 0.0089 \\\\\n0.0013 & 0.0011 & 0.0070 & 0.9921\n\\end{pmatrix}\n\\]\nStep 4: Calculate PAM250 by matrix exponentiation\nTo obtain PAM250, we calculate \\(M^{250}\\):\n\\[\nM^{250} = \\begin{pmatrix}\n0.189 & 0.174 & 0.275 & 0.362 \\\\\n0.217 & 0.221 & 0.269 & 0.293 \\\\\n0.297 & 0.242 & 0.224 & 0.237 \\\\\n0.323 & 0.211 & 0.199 & 0.267\n\\end{pmatrix}\n\\]\nStep 5: Convert to log-odds scoring matrix\nUsing background frequencies \\(f_A = 0.303\\), \\(f_G = 0.242\\), \\(f_V = 0.182\\), \\(f_L = 0.273\\):\n\\[\nS_{AA} = 10 \\log_{10}\\left(\\frac{0.189}{0.303}\\right) = -2.0\n\\]\n\\[\nS_{AG} = 10 \\log_{10}\\left(\\frac{0.174}{0.242}\\right) = -1.4\n\\]\n\\[\nS_{AV} = 10 \\log_{10}\\left(\\frac{0.275}{0.182}\\right) = 1.8\n\\]\n\\[\nS_{AL} = 10 \\log_{10}\\left(\\frac{0.362}{0.273}\\right) = 1.2\n\\]\nThe complete PAM250 scoring matrix for our simplified example:\n\\[\nS^{(250)} = \\begin{pmatrix}\n-2 & -1 & 2 & 1 \\\\\n-1 & -1 & 2 & 0 \\\\\n2 & 1 & 1 & -1 \\\\\n1 & -1 & -1 & 0\n\\end{pmatrix}\n\\]\nThis simplified example demonstrates how evolutionary observations are transformed into practical scoring matrices. The positive scores (e.g., A-V: 2) indicate substitutions occurring more frequently than expected by chance, while negative scores indicate rare substitutions.\n\n\nPython Implementation: PAM Matrix from Pairwise Alignment\nTo make the PAM construction process more concrete, let’s implement it in Python using a single long pairwise alignment. This example shows how to extract substitution counts from an alignment and construct a PAM1 matrix.\nimport numpy as np\nfrom collections import Counter, defaultdict\n\ndef extract_substitutions(seq1, seq2):\n    \"\"\"\n    Extract substitution counts from a pairwise alignment.\n    Assumes sequences are aligned and of equal length.\n    \"\"\"\n    substitutions = defaultdict(int)\n    aa_counts = Counter()\n\n    for a1, a2 in zip(seq1, seq2):\n        if a1 != '-' and a2 != '-':  # Skip gap positions\n            aa_counts[a1] += 1\n            if a1 != a2:\n                # Store substitutions symmetrically\n                pair = tuple(sorted([a1, a2]))\n                substitutions[pair] += 1\n\n    return substitutions, aa_counts\n\ndef compute_pam1_matrix(seq1, seq2):\n    \"\"\"\n    Compute a PAM1 scoring matrix from a pairwise alignment.\n    \"\"\"\n    # Example sequences (representing &gt;85% identity alignment)\n    # In practice, you would use multiple alignments\n\n    # Extract substitutions and counts\n    substitutions, aa_counts = extract_substitutions(seq1, seq2)\n\n    # Get unique amino acids\n    amino_acids = sorted(set(aa_counts.keys()))\n    n_aa = len(amino_acids)\n    aa_to_idx = {aa: i for i, aa in enumerate(amino_acids)}\n\n    # Calculate total positions and substitution matrix A\n    A = np.zeros((n_aa, n_aa))\n    for (aa1, aa2), count in substitutions.items():\n        i, j = aa_to_idx[aa1], aa_to_idx[aa2]\n        A[i, j] = count / 2  # Divide by 2 because we count each pair once\n        A[j, i] = count / 2  # Make symmetric\n\n    # Calculate relative mutabilities\n    total_positions = sum(aa_counts.values())\n    mutabilities = np.zeros(n_aa)\n\n    for i, aa in enumerate(amino_acids):\n        if aa_counts[aa] &gt; 0:\n            mutabilities[i] = np.sum(A[i, :]) / aa_counts[aa]\n\n    # Scale to 1 PAM unit (1% accepted mutations)\n    scaling_factor = 0.01 / np.mean(mutabilities[mutabilities &gt; 0])\n    mutabilities *= scaling_factor\n\n    # Construct mutation probability matrix M\n    M = np.zeros((n_aa, n_aa))\n    for i in range(n_aa):\n        if mutabilities[i] &gt; 0:\n            # Off-diagonal elements: mutation probabilities\n            for j in range(n_aa):\n                if i != j and np.sum(A[i, :]) &gt; 0:\n                    M[i, j] = (A[i, j] / np.sum(A[i, :])) * mutabilities[i]\n            # Diagonal element: probability of no change\n            M[i, i] = 1 - mutabilities[i]\n        else:\n            M[i, i] = 1.0  # No mutations observed\n\n    # Calculate background frequencies\n    total_aa = sum(aa_counts.values())\n    frequencies = np.array([aa_counts[aa] / total_aa for aa in amino_acids])\n\n    # Convert to log-odds scoring matrix\n    # Using PAM convention: S = 10 * log10(M[i,j] / f[j])\n    S = np.zeros((n_aa, n_aa))\n    for i in range(n_aa):\n        for j in range(n_aa):\n            if M[i, j] &gt; 0 and frequencies[j] &gt; 0:\n                S[i, j] = 10 * np.log10(M[i, j] / frequencies[j])\n            else:\n                S[i, j] = -10  # Large negative score for impossible substitutions\n\n    # Round to integers\n    S = np.round(S).astype(int)\n\n    return S, amino_acids, M, frequencies\n\n# Example usage with a long pairwise alignment\nseq1 = \"ARNDCQEGHILKMFPSTWYVARCDEGHKLMNPQRSTVWYARNDCEGHILKMFPSTWYV\"\nseq2 = \"AKNDCQEGHVLKMFPSTWYVARCDEGHRLMNPQRSTVWYAKNDCEGHILKMFASTWYV\"\n#       *R-&gt;K     *I-&gt;V           *K-&gt;R     *R-&gt;K       *P-&gt;A\n\n# Compute PAM1 matrix\nS_pam1, amino_acids, M_prob, freqs = compute_pam1_matrix(seq1, seq2)\n\n# Display the scoring matrix\nprint(\"PAM1 Scoring Matrix (subset):\")\nprint(\"   \", \"  \".join(f\"{aa:&gt;3}\" for aa in amino_acids[:8]))\nfor i in range(min(8, len(amino_acids))):\n    row = [f\"{S_pam1[i,j]:3d}\" for j in range(min(8, len(amino_acids)))]\n    print(f\"{amino_acids[i]:&gt;3}\", \" \".join(row))\n\n# Show mutation probability matrix for verification\nprint(\"\\nMutation Probability Matrix M (first 4x4):\")\nfor i in range(min(4, len(amino_acids))):\n    row = [f\"{M_prob[i,j]:.4f}\" for j in range(min(4, len(amino_acids)))]\n    print(f\"{amino_acids[i]:&gt;3}\", \" \".join(row))\n\n# Calculate PAM250 by matrix exponentiation\ndef compute_pam_n(M, n, frequencies):\n    \"\"\"Compute PAM-n matrix by raising M to the nth power\"\"\"\n    M_n = np.linalg.matrix_power(M, n)\n\n    # Convert to scoring matrix\n    n_aa = len(M)\n    S_n = np.zeros((n_aa, n_aa))\n    for i in range(n_aa):\n        for j in range(n_aa):\n            if M_n[i, j] &gt; 0 and frequencies[j] &gt; 0:\n                S_n[i, j] = 10 * np.log10(M_n[i, j] / frequencies[j])\n            else:\n                S_n[i, j] = -10\n\n    return np.round(S_n).astype(int)\n\n# Compute PAM250\nS_pam250 = compute_pam_n(M_prob, 250, freqs)\n\nprint(\"\\nPAM250 Scoring Matrix (subset):\")\nprint(\"   \", \"  \".join(f\"{aa:&gt;3}\" for aa in amino_acids[:8]))\nfor i in range(min(8, len(amino_acids))):\n    row = [f\"{S_pam250[i,j]:3d}\" for j in range(min(8, len(amino_acids)))]\n    print(f\"{amino_acids[i]:&gt;3}\", \" \".join(row))\n\n# Analyze substitution patterns\nprint(\"\\nObserved substitutions in alignment:\")\nsubs, _ = extract_substitutions(seq1, seq2)\nfor (aa1, aa2), count in sorted(subs.items(), key=lambda x: -x[1])[:5]:\n    print(f\"  {aa1} &lt;-&gt; {aa2}: {count} times\")\nThis implementation demonstrates several key concepts:\n\nExtracting substitutions from alignments: The code counts how often each amino acid pair is observed in aligned positions, which forms the empirical basis for the PAM matrix.\nCalculating relative mutabilities: Each amino acid’s tendency to mutate is computed from the observed substitution frequencies, then scaled to represent 1 PAM unit (1% accepted mutations).\nBuilding the mutation probability matrix: The matrix M encodes the probability of each amino acid mutating to every other amino acid over one PAM unit of evolutionary time.\nConverting to log-odds scores: The mutation probabilities are transformed into scores suitable for sequence alignment using the log-odds ratio formula.\nExtrapolating to longer evolutionary distances: Matrix exponentiation (M^n) models the accumulation of mutations over n PAM units, allowing construction of PAM250 and other matrices.\n\nThe example alignment shows typical patterns: conservative substitutions (R→K, both basic amino acids) occur more frequently than radical changes. The resulting PAM1 matrix will assign positive scores to these common substitutions and negative scores to rare ones.\n\n\nLimitations and Assumptions of PAM Matrices\nLike any model, PAM matrices rest on assumptions, and it’s important to understand where those assumptions might break down. First big assumption: substitution rates stay constant over evolutionary time. In reality, selection pressures change—a protein might be under strong purifying selection for millions of years, then suddenly face relaxed selection after a gene duplication event, then experience positive selection as it adapts to a new function. The PAM model ignores all this temporal variation and assumes evolution proceeds at a steady rate. Second assumption: all positions in a protein evolve at the same rate. This is clearly wrong—active site residues that are critical for catalysis evolve much more slowly than surface loop residues that don’t do much. A mutation in the active site might be lethal; a mutation in a surface loop might be completely neutral. PAM matrices average over all these differences, which means they’re missing important position-specific information. Third assumption: mutations at different positions are independent. In real proteins, mutations are often correlated—you might need a compensatory mutation at position \\(B\\) to tolerate a mutation at position \\(A\\), because they interact structurally. PAM matrices can’t capture these epistatic effects. Finally, there’s an implicit reversibility assumption: the model assumes that the probability of \\(a\\) substituting to \\(b\\) equals the probability of \\(b\\) substituting to \\(a\\), when you account for amino acid frequencies. This might not hold if there are directional trends in evolution—for instance, if proteins are generally evolving toward higher or lower GC content in their genes. Despite these limitations, PAM matrices work surprisingly well in practice, which tells you that these assumptions, while not perfect, are reasonable first approximations for many proteins.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Scoring Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/protein_substitution_matrices.html#blosum-matrices-an-empirical-approach",
    "href": "chapters/bioinformatics/protein_substitution_matrices.html#blosum-matrices-an-empirical-approach",
    "title": "41  Scoring Matrices",
    "section": "BLOSUM Matrices: An Empirical Approach",
    "text": "BLOSUM Matrices: An Empirical Approach\n\nThe BLOCKS Database Foundation\nIn the 1990s, Steven and Jorja Henikoff developed BLOSUM (BLOcks SUbstitution Matrix) matrices using a fundamentally different philosophy from PAM. Instead of building a model from closely related sequences and extrapolating, they asked: why not just directly observe substitution patterns at the evolutionary distance you care about? BLOSUM matrices are built from the BLOCKS database, which contains multiply aligned conserved regions from protein families. Think of these “blocks” as the parts of proteins that are so functionally important that they stay recognizable even across huge evolutionary distances. These are your active sites, your binding pockets, your critical structural motifs—the regions where evolution is conservative because mutations tend to break things. Each block is an ungapped local alignment, meaning there are no insertions or deletions within the block, just substitutions. This simplifies the statistics considerably because you don’t have to worry about gap penalties or how to count gaps in your substitution frequencies. The brilliant innovation in BLOSUM was the clustering approach. Here’s the problem they solved: sequence databases are biased. If you have 100 sequences in a protein family, but 95 of them are from closely related bacterial species and only 5 are from diverse organisms, your substitution statistics will be dominated by those nearly-identical bacterial sequences. You’ll essentially be counting the same substitutions over and over, which gives you a skewed view of substitution patterns. The Henikoffs’ solution: cluster sequences within each block based on percent identity, then weight each cluster’s contribution to avoid over-representing closely related sequences. If sequences share more than a specified threshold of identity (say, \\(62\\%\\) for BLOSUM62), they get clustered together and treated as a single representative sequence. This prevents any particular evolutionary lineage from dominating the statistics and gives you a more balanced view of substitution patterns across diverse proteins.\n\n\n\n\n\n\n\n\nFigure 41.5: BLOSUM clustering concept. Sequences exceeding the identity threshold are grouped into clusters and downweighted to prevent over-representation. BLOSUM62 uses a 62% identity threshold, while BLOSUM45 uses 45% and BLOSUM80 uses 80%.\n\n\n\n\n\n\n\nComputing BLOSUM Matrices\nOnce you have your blocks with sequences appropriately clustered, the construction process is conceptually straightforward: count amino acid pairs and convert the counts into log-odds scores. But the details matter, especially the weighting scheme. For each column in a block containing \\(n\\) sequences, you could potentially extract \\(\\binom{n}{2} = \\frac{n(n-1)}{2}\\) amino acid pairs by comparing every sequence to every other sequence. But remember, we’ve clustered sequences to avoid redundancy, so we need to weight each pair’s contribution based on which clusters they come from. Let \\(c_i\\) represent the size of the cluster containing sequence \\(i\\). The weight of a pairwise comparison between sequences \\(i\\) and \\(j\\) is:\n\\[w_{ij} = \\frac{1}{c_i \\cdot c_j}\\]\nThink about what this achieves. If both sequences are in large clusters (lots of nearly identical sequences), the weight is small—we’re downweighting this comparison because it’s not giving us much new information. If one or both sequences are in small clusters (unique or rare sequences), the weight is larger. This elegantly solves the redundancy problem. Now you count amino acid pairs across all column positions in all blocks, weighting each observation:\n\\[q_{ab} = \\frac{\\sum_{\\text{pairs}} w_{ij} \\cdot \\delta_{ab}(i,j)}{\\sum_{\\text{all pairs}} w_{ij}}\\]\nwhere \\(\\delta_{ab}(i,j)\\) is an indicator that equals 1 if sequence \\(i\\) has amino acid \\(a\\) and sequence \\(j\\) has amino acid \\(b\\) at the position you’re examining (or vice versa when \\(a \\neq b\\)), and 0 otherwise. This gives you \\(q_{ab}\\), the observed frequency of the \\((a,b)\\) pair in your blocks. To convert this into a log-odds score, you need the expected frequency—what you’d see if amino acids were pairing up randomly with no evolutionary constraint:\n\\[e_{ab} = \\begin{cases}\np_a \\cdot p_b \\cdot 2 & \\text{if } a \\neq b \\\\\np_a^2 & \\text{if } a = b\n\\end{cases}\\]\nwhere \\(p_a = q_{aa} + \\frac{1}{2}\\sum_{b \\neq a} q_{ab}\\) is the marginal frequency—how often amino acid \\(a\\) appears overall. The factor of 2 when \\(a \\neq b\\) accounts for the fact that the pair \\((a,b)\\) is the same as \\((b,a)\\) —the pair is unordered.\n\n\nThe Log-Odds Scoring System\nWith observed and expected frequencies in hand, computing the actual BLOSUM score is straightforward:\n\\[S_{ab} = \\lambda \\cdot \\log_2\\left(\\frac{q_{ab}}{e_{ab}}\\right)\\]\nwhere \\(\\lambda\\) is a scaling factor chosen to yield convenient integer scores—typically 2 or thereabouts. Notice the use of base-2 logarithms, which gives the scores a nice interpretation: a score of \\(+1\\) means the substitution is twice as likely as expected by chance, while a score of \\(-1\\) means it’s half as likely. A score of \\(+2\\) means four times as likely, \\(-2\\) means one-quarter as likely, and so on. This is more intuitive than the base-10 logs used in PAM matrices. Positive scores indicate favorable substitutions—ones that show up more often in real proteins than random chance would predict. Negative scores penalize rare substitutions. The magnitude tells you how strong the signal is. One useful property of BLOSUM matrices is that you can quantify their information content using relative entropy:\n\\[H = \\sum_{a,b} q_{ab} \\cdot \\log_2\\left(\\frac{q_{ab}}{e_{ab}}\\right)\\]\nThis is expressed in bits and measures how much information you gain about evolutionary relationships from observing a particular amino acid pair versus what you’d expect from random associations. Higher entropy means more specific, informative substitution patterns. Generally, matrices derived from more similar sequences (like BLOSUM80) have higher entropy than those from more divergent sequences (like BLOSUM45), because closely related sequences have more constrained, predictable substitution patterns that deviate more strongly from random expectation.\n\n\nThe BLOSUM Series and Clustering Thresholds\nThe number in a BLOSUM matrix name tells you the clustering threshold used during construction, and understanding this is key to choosing the right matrix. BLOSUM62 clusters sequences at \\(62\\%\\) identity—if two sequences in a block share \\(\\geq 62\\%\\) identity, they’re treated as a single cluster. BLOSUM80 uses an \\(80\\%\\) threshold, so only very similar sequences get clustered together. Here’s the counterintuitive part: lower numbers mean matrices suitable for more divergent sequences. Why? Because with a low clustering threshold like \\(45\\%\\), you’re lumping together fairly diverse sequences into clusters, which means your blocks contain a broader range of evolutionary distances. The substitution patterns you extract reflect what happens over longer evolutionary time. With a high threshold like \\(80\\%\\), you’re mostly keeping sequences separate unless they’re very similar, so your substitution patterns reflect shorter evolutionary distances. The matrix properties follow predictable patterns based on this threshold. Low-threshold matrices like BLOSUM45 have lower relative entropy and are permissive—they accept a wider range of substitutions, making them good for detecting remote homologs where the sequence similarity is weak. High-threshold matrices like BLOSUM80 have higher entropy and are stringent—they penalize most substitutions heavily and reward only very conservative changes, making them appropriate for distinguishing closely related sequences. BLOSUM62 sits in the middle and has become the de facto standard because it performs well across a broad range of evolutionary distances, making it a robust default choice when you don’t know in advance how similar your sequences will be. One interesting mathematical detail: you can recover the target frequencies implicit in each BLOSUM matrix:\n\\[q_{ab} = p_a \\cdot p_b \\cdot e^{\\lambda S_{ab}}\\]\nThis reveals that BLOSUM matrices implicitly assume an exponential distribution of evolutionary distances in the training data—a different assumption from the time-structured model of PAM matrices.\n\n\nWorked Example: Constructing a BLOSUM Matrix\nLet’s construct a simplified BLOSUM matrix from a small block of aligned sequences. Consider the following ungapped alignment block containing five sequences:\nSequence 1: A V L G\nSequence 2: A V M G\nSequence 3: G I L A\nSequence 4: G V L A\nSequence 5: A I M G\nStep 1: Apply clustering threshold\nSuppose we use a 60% identity threshold (similar to BLOSUM62). Comparing sequences pairwise: - Sequences 1 and 2: 75% identity (3/4 matches) → cluster together - Sequences 3 and 4: 50% identity (2/4 matches) → remain separate - Sequence 5: &lt;60% identity with all others → remains separate\nThis gives us 4 clusters: - Cluster 1: {Seq1, Seq2} with size \\(c_1 = 2\\) - Cluster 2: {Seq3} with size \\(c_2 = 1\\) - Cluster 3: {Seq4} with size \\(c_3 = 1\\) - Cluster 4: {Seq5} with size \\(c_4 = 1\\)\nStep 2: Calculate weighted pair frequencies\nFor each column, we count amino acid pairs with weights. For column 1: - A-A pairs: (1,2) with weight \\(\\frac{1}{2 \\times 2} = 0.25\\); (1,5) with weight \\(\\frac{1}{2 \\times 1} = 0.5\\); (2,5) with weight \\(\\frac{1}{2 \\times 1} = 0.5\\) - A-G pairs: (1,3) with weight \\(\\frac{1}{2 \\times 1} = 0.5\\); (1,4) with weight \\(\\frac{1}{2 \\times 1} = 0.5\\); (2,3) with weight \\(\\frac{1}{2 \\times 1} = 0.5\\); (2,4) with weight \\(\\frac{1}{2 \\times 1} = 0.5\\) - G-G pairs: (3,4) with weight \\(\\frac{1}{1 \\times 1} = 1.0\\)\nTotal weight for column 1: \\(0.25 + 0.5 + 0.5 + 0.5 + 0.5 + 0.5 + 0.5 + 1.0 = 4.25\\)\nContinuing for all columns and summing:\n\n\n\nPair\nColumn 1\nColumn 2\nColumn 3\nColumn 4\nTotal\n\n\n\n\nA-A\n1.25\n0\n0\n1.0\n2.25\n\n\nA-G\n2.0\n0\n0\n2.0\n4.0\n\n\nG-G\n1.0\n0\n0\n1.25\n2.25\n\n\nV-V\n0\n1.25\n0\n0\n1.25\n\n\nV-I\n0\n2.0\n0\n0\n2.0\n\n\nI-I\n0\n1.0\n0\n0\n1.0\n\n\nL-L\n0\n0\n2.25\n0\n2.25\n\n\nL-M\n0\n0\n2.0\n0\n2.0\n\n\nM-M\n0\n0\n1.0\n0\n1.0\n\n\n\nTotal weight across all pairs: 18.0\nStep 3: Calculate observed frequencies\n\\[q_{AA} = \\frac{2.25}{18.0} = 0.125\\] \\[q_{AG} = \\frac{4.0}{18.0} = 0.222\\] \\[q_{GG} = \\frac{2.25}{18.0} = 0.125\\]\nAnd similarly for other pairs.\nStep 4: Calculate marginal frequencies\n\\[p_A = q_{AA} + \\frac{1}{2}(q_{AG} + q_{AL} + q_{AM} + ...) = 0.278\\] \\[p_G = q_{GG} + \\frac{1}{2}(q_{AG} + q_{GL} + q_{GM} + ...) = 0.222\\] \\[p_V = 0.181\\] \\[p_I = 0.139\\] \\[p_L = 0.125\\] \\[p_M = 0.055\\]\nStep 5: Calculate expected frequencies and log-odds scores\nFor A-A: \\[e_{AA} = p_A^2 = 0.278^2 = 0.077\\] \\[S_{AA} = 2 \\times \\log_2\\left(\\frac{0.125}{0.077}\\right) = 2 \\times 0.70 = 1.4 \\approx 1\\]\nFor A-G: \\[e_{AG} = 2 \\times p_A \\times p_G = 2 \\times 0.278 \\times 0.222 = 0.123\\] \\[S_{AG} = 2 \\times \\log_2\\left(\\frac{0.222}{0.123}\\right) = 2 \\times 0.85 = 1.7 \\approx 2\\]\nFor L-M: \\[e_{LM} = 2 \\times p_L \\times p_M = 2 \\times 0.125 \\times 0.055 = 0.014\\] \\[S_{LM} = 2 \\times \\log_2\\left(\\frac{0.111}{0.014}\\right) = 2 \\times 3.0 = 6\\]\nStep 6: Construct the final BLOSUM matrix\nAfter calculating all scores and rounding to integers:\n\\[\\text{BLOSUM} = \\begin{array}{c|cccccc}\n  & A & G & V & I & L & M \\\\\n\\hline\nA & 1 & 2 & -2 & -2 & -1 & -2 \\\\\nG & 2 & 1 & -3 & -3 & -2 & -3 \\\\\nV & -2 & -3 & 2 & 3 & 1 & 1 \\\\\nI & -2 & -3 & 3 & 2 & 1 & 1 \\\\\nL & -1 & -2 & 1 & 1 & 2 & 6 \\\\\nM & -2 & -3 & 1 & 1 & 6 & 3 \\\\\n\\end{array}\\]\nThis example demonstrates how BLOSUM matrices capture actual substitution patterns observed in conserved regions. The high score for L-M (6) reflects the frequent substitution between these hydrophobic residues in our block, while negative scores indicate rare substitutions.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Scoring Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/protein_substitution_matrices.html#comparative-analysis-of-pam-and-blosum",
    "href": "chapters/bioinformatics/protein_substitution_matrices.html#comparative-analysis-of-pam-and-blosum",
    "title": "41  Scoring Matrices",
    "section": "Comparative Analysis of PAM and BLOSUM",
    "text": "Comparative Analysis of PAM and BLOSUM\n\nTheoretical Versus Empirical Approaches\nPAM and BLOSUM represent two fundamentally different philosophies for tackling the same problem, and understanding the contrast helps you appreciate the strengths and limitations of each. PAM matrices are model-driven. They start with a theoretical model of how evolution works—mutations accumulate over time following a Markov process—and they build substitution matrices by estimating the model parameters from closely related sequences, then extrapolating to longer time scales through matrix exponentiation. This gives you a coherent mathematical framework with clear interpretability: PAM250 means 250 point accepted mutations per 100 positions. The downside? The whole approach rests on assumptions—constant substitution rates over time, position independence, reversibility—and if those assumptions don’t hold for your sequences, the extrapolation might go wrong. BLOSUM matrices, in contrast, are data-driven and empirical. They skip the modeling step entirely and just directly observe substitution patterns at whatever evolutionary distance is represented in the BLOCKS database. No extrapolation, no evolutionary model, just “here’s what we actually see in real conserved protein regions.” This avoids the assumptions inherent in PAM’s model, but it also means you lose the theoretical elegance and the direct interpretability—BLOSUM62 doesn’t have the same clear evolutionary time interpretation that PAM250 does. In practice, these philosophical differences lead to different performance characteristics. PAM matrices tend to excel when you’re comparing sequences at specific evolutionary distances that match their PAM number. Use PAM250 for sequences around \\(20\\%\\) identity, PAM120 for sequences around \\(40\\%\\) identity, and so on. They’re tuned for particular distances because of the explicit evolutionary model. BLOSUM matrices are more versatile across a range of distances. BLOSUM62 works well for sequences anywhere from \\(30\\%\\) to \\(70\\%\\) identity, which is why it’s become the default choice in most sequence alignment tools—you don’t have to know the evolutionary distance in advance to get reasonable results. This broader applicability comes from the diversity of evolutionary distances already present in the blocks used for construction.\n\n\n\n\n\n\n\n\nFigure 41.6: Comparison of PAM250 and BLOSUM62 substitution matrices for a subset of amino acids. While both matrices capture similar biological patterns (high scores for conservative substitutions), they differ in magnitude and specific values due to their different construction methods.\n\n\n\n\n\n\n\nInformation Content and Entropy Considerations\nInformation content—measured by relative entropy—tells you how much discriminatory power a substitution matrix has, and it behaves differently for PAM versus BLOSUM matrices. For PAM matrices, information content follows a predictable pattern as you increase the PAM distance:\n\\[H_{\\text{PAM}n} \\approx H_{\\text{PAM}1} \\cdot \\left(1 - e^{-\\alpha n}\\right)\\]\nwhere \\(\\alpha\\) is a decay constant. What this means: information content starts low at PAM1 (because there are almost no substitutions—most positions are identical), increases as you go to PAM50, PAM100, etc. (substitution patterns become more diverse and informative), then plateaus at high PAM distances like PAM250 (saturation—substitutions have occurred so many times that the patterns become less specific). There’s a sweet spot in the middle where information content is maximized. BLOSUM matrices show a different pattern. Information content is inversely related to the clustering threshold. BLOSUM80 has high entropy because you’re looking at closely related sequences with constrained, specific substitution patterns. BLOSUM45 has lower entropy because you’re pooling together more diverse sequences, so the substitution patterns are less specific and closer to random expectation. This creates a fundamental trade-off: high entropy means good specificity (few false positives) but potentially lower sensitivity (might miss true homologs). Low entropy means better sensitivity (can detect weak similarities) but more false positives. Your choice of matrix depends on which error you care more about avoiding.\n\n\nStatistical Significance and E-values\nYour choice of substitution matrix doesn’t just affect which alignments score highest—it fundamentally changes how you assess statistical significance. When you do a database search, you want to know: is this alignment score good enough to believe it represents a real evolutionary relationship, or could it have arisen by chance? The answer depends on the score distribution, which depends on the matrix. For ungapped alignments, scores follow an extreme value distribution:\n\\[P(S \\geq x) \\approx 1 - e^{-Kmn e^{-\\lambda x}}\\]\nwhere \\(m\\) and \\(n\\) are the lengths of your sequences, and \\(K\\) and \\(\\lambda\\) are parameters specific to the substitution matrix and amino acid composition. Different matrices have different \\(K\\) and \\(\\lambda\\) values, which means an alignment score of, say, \\(+50\\) might be highly significant with one matrix but marginal with another. This affects E-value calculations—the expected number of alignments with that score or better that you’d see by chance—and consequently affects which sequences you identify as homologs. To enable fair comparison across different matrices, we use bit scores:\n\\[S' = \\frac{\\lambda S - \\ln K}{\\ln 2}\\]\nBit scores normalize the raw scores using the matrix-specific parameters, giving you a matrix-independent measure of alignment quality. A bit score of 50 means the same thing regardless of whether you used BLOSUM62 or PAM250. This normalization is crucial when you’re comparing results from different search tools or different matrices to select the best approach for your sequences.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Scoring Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/protein_substitution_matrices.html#specialized-matrices-and-modern-developments",
    "href": "chapters/bioinformatics/protein_substitution_matrices.html#specialized-matrices-and-modern-developments",
    "title": "41  Scoring Matrices",
    "section": "Specialized Matrices and Modern Developments",
    "text": "Specialized Matrices and Modern Developments\n\nPosition-Specific Scoring Matrices (PSSMs)\nGeneral substitution matrices like BLOSUM62 treat all positions in a protein the same way—they ask “how likely is alanine to substitute for valine?” without regard to where in the protein that substitution occurs. But we know this is an oversimplification. Some positions are highly conserved (active site residues, structural hinge points) where almost any substitution is deleterious. Other positions are variable (surface loops, linker regions) where many substitutions are tolerated. Position-specific scoring matrices (PSSMs) capture this position-by-position variation. Instead of a single matrix for all positions, you build a scoring profile from a multiple sequence alignment where each column (each position) gets its own substitution scores based on what’s observed at that specific position:\n\\[M_{i,a} = \\log\\left(\\frac{f_{i,a} + \\beta p_a}{(1 + \\beta)p_a}\\right)\\]\nwhere \\(f_{i,a}\\) is the observed frequency of amino acid \\(a\\) at position \\(i\\) in your alignment, and \\(\\beta\\) is a pseudocount parameter that prevents you from assigning zero probability to amino acids that just haven’t been observed yet. This formulation smoothly interpolates between the observed frequencies (when you have lots of data) and background frequencies (when data is scarce). The magic happens when you use PSSMs iteratively, as in PSI-BLAST. You start with a query sequence and a general matrix like BLOSUM62, search a database, and identify homologs. Then you align those homologs and build a PSSM that captures the substitution patterns specific to your protein family. In the next iteration, you search again using this PSSM instead of the general matrix. The PSSM is more sensitive to the specific evolutionary constraints of your family, so you detect more distant homologs. You iterate, refining the PSSM each time, progressively extending your reach into more remote sequence space. This iterative approach has revolutionized remote homology detection, finding relationships that would be invisible to a single-pass search with a general matrix.\n\n\nStructure-Based Substitution Matrices\nA major limitation of general substitution matrices is that they ignore structural context. An alanine in an alpha-helix deep in the protein core faces very different evolutionary constraints than an alanine in a surface loop exposed to solvent. Structure-based substitution matrices address this by partitioning amino acids according to their structural environment—buried versus exposed, helix versus sheet versus loop—and building separate substitution matrices for each environment:\n\\[S_{ab}^{(e)} = \\log\\left(\\frac{q_{ab}^{(e)}}{p_a^{(e)} \\cdot p_b^{(e)}}\\right)\\]\nwhere the superscript \\((e)\\) denotes a specific structural environment. These matrices reveal fascinating patterns: buried positions strongly favor hydrophobic substitutions and heavily penalize hydrophilic ones (because burying a charged residue disrupts the hydrophobic core). Exposed positions are more permissive, accepting a wider range of substitutions. Helical positions favor helix-forming amino acids like alanine and leucine, while beta-sheet positions favor residues like valine and isoleucine. By using the appropriate environment-specific matrix for each position (if you have structural information or predictions), you can improve alignment accuracy, especially for distantly related proteins where structural constraints are often better conserved than sequence similarity.\n\n\nCompositionally Adjusted Matrices\nSome proteins have weird amino acid compositions that violate the assumptions built into standard matrices. Transmembrane proteins are packed with hydrophobic residues. Prion proteins are loaded with glutamine and asparagine. Proteins from thermophilic organisms have elevated levels of charged residues for stability. If you use a standard matrix with standard background frequencies to align these proteins, you’ll get biased results because the matrix assumes a typical amino acid composition. Compositional adjustment solves this by modifying the matrix based on the actual composition of the sequences you’re comparing:\n\\[S'_{ab} = S_{ab} + \\lambda \\log\\left(\\frac{p'_a p'_b}{p_a p_b}\\right)\\]\nwhere \\(p'_a\\) and \\(p'_b\\) are the observed frequencies in your specific sequences, while \\(p_a\\) and \\(p_b\\) are the background frequencies the matrix was built with. This adjustment effectively recalibrates the matrix to account for compositional bias while preserving the relative substitution preferences—you’re still rewarding biochemically sensible substitutions, but you’re not artificially penalizing amino acids just because they happen to be unusually common or rare in your sequences.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Scoring Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/protein_substitution_matrices.html#practical-considerations-in-matrix-selection",
    "href": "chapters/bioinformatics/protein_substitution_matrices.html#practical-considerations-in-matrix-selection",
    "title": "41  Scoring Matrices",
    "section": "Practical Considerations in Matrix Selection",
    "text": "Practical Considerations in Matrix Selection\n\nChoosing Matrices for Specific Applications\nChoosing the right substitution matrix isn’t just an academic exercise—it can make the difference between finding what you’re looking for and missing it entirely. The choice depends on your biological question and what you know about your sequences. If you’re identifying orthologs between closely related species—say, comparing mouse and rat proteins—you want high specificity. Use stringent matrices like BLOSUM80 or PAM120 that penalize most substitutions heavily. These matrices help you distinguish true orthologs (same gene in different species) from paralogs (different genes from an ancient duplication) by demanding high sequence similarity. False positives are costly here because you might mis-assign function, so you’re willing to be picky. On the other hand, if you’re searching databases for remote homologs—trying to find a distant evolutionary relative of your query protein that might share function or structure despite low sequence identity—you need sensitivity. Use permissive matrices like BLOSUM45 or PAM250 that are more forgiving of substitutions. These matrices won’t penalize chemically dissimilar amino acids as harshly, which lets you detect relationships that have diverged substantially over evolutionary time. The trade-off? More false positives, which means you need to carefully evaluate your hits with statistical tests (E-values) and look for corroborating evidence. A clever strategy employed by some alignment programs is to use different matrices at different stages. During initial pairwise comparisons when you’re casting a wide net, use a sensitive matrix like BLOSUM45 to avoid missing potential homologs. Then during refinement, when you’re trying to get the best possible alignment of sequences you’ve already identified as related, switch to BLOSUM62 or BLOSUM80 for more accurate positioning. This staged approach balances the competing demands of sensitivity and accuracy.\n\n\n\n\n\n\n\n\nFigure 41.7: Optimal matrix selection based on sequence identity. Different substitution matrices perform best at different evolutionary distances. BLOSUM62 offers broad applicability across a wide range of identities, making it a robust default choice.\n\n\n\n\n\n\n\nGap Penalties and Matrix Scaling\nSubstitution matrices don’t work in isolation—they operate in concert with gap penalties to define your complete scoring scheme, and the balance between substitution scores and gap penalties critically affects what alignments you get. Think of it as a trade-off: would you rather accept a mismatch at a position, or would you rather open a gap (insertion/deletion)? If your gap penalties are too low relative to substitution penalties, the algorithm will insert gaps everywhere, fragmenting your alignment into tiny matching pieces separated by gaps. If gap penalties are too high, the algorithm will force mismatches even at positions where an insertion or deletion would make much more biological sense. The optimal gap penalties depend on which substitution matrix you’re using because different matrices have different score ranges. Empirical benchmarking has established good default values: for BLOSUM62, gap opening penalties of 10-12 and gap extension penalties of 1-2 work well for most proteins. These values strike a balance where gaps get introduced when they genuinely improve the alignment, but not gratuitously. Matrix scaling is another detail that matters more than you might think. All the substitution scores in a matrix can be multiplied by a constant without changing the relative relationships—a scaled matrix produces alignments with the same structure, just different raw scores. But the absolute scale affects numerical precision in alignment algorithms and influences statistical significance calculations. Most modern tools use integer-scaled matrices for computational efficiency (integer arithmetic is faster than floating-point), but the scaling is chosen to preserve the mathematical relationships and dynamic range of the original log-odds scores, so you’re not losing information.\n\n\nPerformance Evaluation and Benchmarking\nHow do we know which substitution matrix actually works best? You need carefully curated benchmark datasets where the evolutionary relationships are known—typically from structural data, since proteins with similar structures are reliably homologous even when sequence similarity is low. Databases like HOMSTRAD (Homologous Structure Alignment Database) provide gold-standard alignments based on structural superposition. You can test a substitution matrix by seeing how well it reproduces these known-correct alignments. The key metrics are sensitivity (can you detect true homologs?), specificity (can you avoid false positives?), and alignment accuracy (when you align two sequences, do you get the residue pairings right?). A powerful tool for evaluation is the ROC (Receiver Operating Characteristic) curve, which plots true positive rate versus false positive rate as you vary the score threshold for calling sequences homologous. The area under the ROC curve gives you a single number summarizing matrix performance: values near \\(1.0\\) mean excellent discrimination, values near \\(0.5\\) mean you’re doing no better than random guessing. Extensive benchmarking using ROC analysis has validated BLOSUM62 as a robust general-purpose matrix, while also identifying specific scenarios where other matrices perform better—BLOSUM45 for very remote homologs, BLOSUM80 for close relatives, PAM matrices when you have good estimates of evolutionary distance. Another useful metric for database searching is “coverage versus errors per query”: as you relax your score threshold, you detect more true homologs (increasing coverage), but you also accumulate false positives (errors). The optimal matrix minimizes errors while maximizing coverage, though the exact balance you want depends on your application—is it worse to miss a true homolog or to chase a false lead?",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Scoring Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/protein_substitution_matrices.html#future-directions-and-emerging-approaches",
    "href": "chapters/bioinformatics/protein_substitution_matrices.html#future-directions-and-emerging-approaches",
    "title": "41  Scoring Matrices",
    "section": "Future Directions and Emerging Approaches",
    "text": "Future Directions and Emerging Approaches\n\nMachine Learning and Deep Learning Approaches\nRecent advances in machine learning have opened new avenues for developing substitution scoring schemes. Deep learning models trained on large sequence databases can learn complex substitution patterns that escape traditional statistical approaches. These models capture higher-order dependencies between positions and can adapt to specific protein families or functional classes.\nNeural network architectures like transformers, which have revolutionized natural language processing, show promise for learning context-dependent substitution patterns. These models can potentially capture long-range interactions and correlated mutations that traditional position-independent matrices miss. The challenge lies in interpreting these complex models and extracting biological insights from their learned representations.\n\n\nIntegration with Structural Prediction\nThe recent breakthrough in protein structure prediction by AlphaFold and similar systems creates opportunities for structure-informed substitution matrices. As structural data becomes available for most proteins, substitution patterns can be analyzed in their full three-dimensional context. This integration promises matrices that capture the subtle interplay between sequence and structure evolution.\nContact-dependent substitution matrices represent one promising direction, where substitution scores depend not just on the amino acids being compared but also on their structural neighbors. Such matrices could capture the compensatory mutations that maintain protein stability and the correlated changes that preserve protein-protein interfaces.\n\n\nPhylogenetic Context and Lineage-Specific Matrices\nThe recognition that substitution patterns vary across evolutionary lineages motivates the development of clade-specific matrices. Matrices optimized for vertebrate proteins may not perform optimally for bacterial sequences, reflecting different evolutionary pressures and constraints. The construction of taxonomically focused matrices requires balancing specificity gains against reduced training data.\nTime-heterogeneous models that allow substitution patterns to vary across evolutionary time represent another frontier. These models could capture the acceleration of evolution following gene duplication or the different selective pressures operating at different evolutionary epochs. The mathematical and computational challenges of such models are substantial, but they promise more accurate evolutionary inference.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Scoring Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/protein_substitution_matrices.html#conclusion",
    "href": "chapters/bioinformatics/protein_substitution_matrices.html#conclusion",
    "title": "41  Scoring Matrices",
    "section": "Conclusion",
    "text": "Conclusion\nProtein substitution matrices represent one of those foundational achievements in computational biology that’s easy to take for granted because they work so reliably. These matrices transformed the fuzzy, qualitative notion of “these sequences look kinda similar” into precise quantitative frameworks that let you rigorously test evolutionary hypotheses, detect remote homologs, and predict protein function. From Dayhoff’s pioneering work on PAM matrices in the 1970s to the Henikoffs’ empirical BLOSUM approach in the 1990s, substitution matrices have enabled countless discoveries in molecular evolution, structural biology, and functional genomics. The field has progressed from simple statistical models to sophisticated approaches incorporating position-specificity, structural context, and compositional biases. Each advance addresses limitations of previous methods while preserving core insights about what makes certain amino acid substitutions more likely than others. Looking forward, we’re entering an exciting era where massive sequence databases, routine structure prediction, and powerful machine learning methods are opening new possibilities. Deep learning models can potentially capture complex substitution patterns that escape traditional statistical approaches—context dependencies, long-range interactions, and subtle correlations that position-independent matrices miss. Structure-informed matrices can leverage the protein structure prediction revolution to account for three-dimensional constraints. Lineage-specific matrices can capture the fact that substitution patterns differ between prokaryotes, plants, and animals. Despite these advances, the fundamental principles remain constant: we’re trying to capture evolutionary signal, distinguish real homology from chance similarity, and quantify sequence relationships in biologically meaningful ways. Future methods, whether based on transformers or physics-based simulations, will build on the foundation that PAM and BLOSUM established. And the practical impact extends far beyond sequence alignment—substitution matrices inform protein engineering, guide drug design, and underpin phylogenetic inference. Understanding how these matrices work, why they’re constructed the way they are, and how to choose the right one for your problem remains essential knowledge for anyone working with protein sequences. As methods evolve, the core insights about protein evolution encoded in substitution matrices will continue to guide the field.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Scoring Matrices</span>"
    ]
  },
  {
    "objectID": "chapters/web/mrsa_blast_multalign/index.html",
    "href": "chapters/web/mrsa_blast_multalign/index.html",
    "title": "43  MRSA",
    "section": "",
    "text": "Sequence Retrieval\nMethicillin-resistant Staphylococcus aureus (MRSA) is a strain of the staph bacteria that’s become resistant to the antibiotics typically used to treat ordinary staph infections. Originally, the Staphylococcus aureus bacterium was a common cause of skin infections, pneumonia, and other medical conditions. However, its methicillin-resistant counterpart, MRSA, emerged as a strain that resists many of the conventional antibiotics. This resistance is not limited to just methicillin but extends to other antibiotics, often rendering standard treatments ineffective.\nIndividuals infected with MRSA can experience a range of ailments, from skin and wound infections to more severe conditions like pneumonia, bloodstream infections, and sepsis. Particularly concerning is its ability to cause life-threatening complications in older or immunocompromised people or people with chronic illnesses.\nA critical factor that has amplified MRSA’s resilience and adaptability is the phenomenon of horizontal gene transfer (HGT). Unlike the usual vertical transfer of genes from parent to offspring, HGT facilitates the direct exchange of genetic material between different bacterial species. This form of gene transfer has been pivotal in the rapid acquisition and spread of antibiotic resistance genes within microbial communities, including those of Staphylococcus aureus.\nThe most common mechanism of HGT involves direct cell-to-cell contact, where a donor bacterium transfers genetic material, like the SCCmec carrying the mecA gene, to a recipient bacterium. Less commonly, viruses that infect bacteria, known as bacteriophages, can mistakenly package bacterial DNA, including resistance genes, and transfer them to another bacterium upon subsequent infection.\nMRSA’s resilience stems from various genetic mutations but is prominently due to the acquisition of the mecA gene. This gene provides the bacteria with the ability to produce a unique protein that alters its cell wall, reducing the efficacy of even last-resort antibiotics, such as methicillin. mecA is believed to have been acquired through HGT and often locates to a mobile genetic element called the staphylococcal chromosomal cassette (SCCmec), allowing for its potential transfer between different strains or even species of bacteria.\nGiven the role of HGT in the rapid spread of resistance, it’s evident that MRSA’s evolution isn’t merely a product of its internal genetic changes. It’s deeply intertwined with a broader microbial ecosystem, where genes, particularly those conferring survival advantages like antibiotic resistance, can be exchanged, adopted, and propagated. This understanding underscores the challenge that antibiotic resistance presents.\nIinvestigating the origins of the mecA gene in MRSA providesinsights into the potential sources and the evolutionary trajectory of antibiotic resistance. Your goal is to identify which organism(s) Staphylococcus aureus might have acquired the mecA gene, and what evidence supports this potential horizontal gene transfer event?\nThe structural learning goals of this exercise are:",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>MRSA</span>"
    ]
  },
  {
    "objectID": "chapters/web/mrsa_blast_multalign/index.html#sequence-retrieval",
    "href": "chapters/web/mrsa_blast_multalign/index.html#sequence-retrieval",
    "title": "43  MRSA",
    "section": "",
    "text": "Exercise 43-1\nObtain the genetic sequence for the mecA gene from MRSA. Go to Genbank and find a complete sequence for mecA in Staphylococcus aureus. That is, do a nucleotide search for “Staphylococcus aureus mecA” and select a MecA gene version with “complete cds”. See Figure 43.2. Do not pick the complete genome. Check if MRSA is mentioned in the documentation for the result.\n\n\nExercise 43-2\nFind the FASTA version of the genomic reference assembly. This will give you the genetic sequence of the gene. Keep the header of the sequence (“&gt;… Staph…, complete cds”), as we will need this later.\n\n\nExericse 43-3\nFind the FASTA version of the protein sequence. There might be a link to this in “Related Information.”\n\n\n\n\n\n\nFigure 43.2: Screenshot",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>MRSA</span>"
    ]
  },
  {
    "objectID": "chapters/web/mrsa_blast_multalign/index.html#database-searching-using-blast",
    "href": "chapters/web/mrsa_blast_multalign/index.html#database-searching-using-blast",
    "title": "43  MRSA",
    "section": "Database Searching using BLAST",
    "text": "Database Searching using BLAST\n\nExercise 43-4\nGo to the NCBI BLAST web portal. Use the mecA nucleotide sequence to search for similar sequences in the nucleotide database. Which organisms have regions with high similarity to the mecA gene nucleotide sequence in your query? Tip: Check the “Show results in a new window” to allow searching multiple times and compare results. Further, know that it is normal for blasting to take a few minutes. Examine the following metrics (columns):\n\nE-value (Expect value): The number of hits one can “expect” to see by chance when searching a database of a particular size. Lower E-values indicate a more significant match. Typically, E-values below 0.05 or especially 0.01 suggest a significant match, but the threshold can depend on the content and especially the size of the search. The longer a search term, the less likely it is for random matches.\nPercent Identity: The percentage of identical matches between the query and subject sequences over the aligned region.\n\n\n\nExercise 43-5\nTry adjusting the number of maximum target sequences in “Algorithm Parameters” (e.g., to 1000). You can also exclude “Staphylococcus aureus” from the search results to only see other organisms.\n\nHow did this affect the results?\nCheck the taxonomy of your found organisms.\nCheck the graphic summary of the results.\n\n\n\nExercise 43-6\nTry the other programs (“Program Selection”). They may be slower but can find less similar matches that may uncover the potential sources of the gene.\n\nDid this affect the results? If so, how? (Use the taxonomy and graphic summary in your analysis.)\nWhat are the main differences between the programs (megablast, discontiguous megablast, and blastn).\nWhat do we expect from the different algorithms? It is okay to use Wikipedia or similar resources.\n\n\n\nExercise 43-7\nTry the protein blast algorithms and compare the findings. This requires the protein sequence. Do you find the MecA gene product (i.e., protein) in other types of bacteria (i.e., not Staphylococcus strains)? If so, what may this imply about horizontal gene transfer of this gene? Is there a risk of MRSA “helping” completely different, and perhaps more dangerous, bacteria genera become antibiotics resistant?\n\n\nExercise 43-8\nYou might see multiple Staphylococcus strains when allowing higher maximum target sequences. What are the potential interpretations of this concerning horizontal gene transfer? Can we deduce a single origin of the gene in Staphylococcus aureus from these results? Remember that we see a snapshot of where the genes were present during various studies – this is ever-changing.\n\n\nExericse 43-9\nAccording to Bloemendaal et al. (2010): “Methicillin Resistance Transfer from Staphylococcus epidermidis to Methicillin-Susceptible Staphylococcus aureus in a Patient during Antibiotic Therapy”, mecA may be transferred from Staphylococcus epidermidis. The mecA gene is located on the Staphylococcal Cassette Chromosome mec (SCCmec), and they found that the SCCmec were almost identical in MRSA and in S. epidermidis in a patient.\n\nDoes your results imply this may be true?\nIf this was the case in that one patient, does it mean the MecA gene in MRSA always stems from S. epidermidis?",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>MRSA</span>"
    ]
  },
  {
    "objectID": "chapters/web/mrsa_blast_multalign/index.html#multiple-sequence-alignment-using-clustalw",
    "href": "chapters/web/mrsa_blast_multalign/index.html#multiple-sequence-alignment-using-clustalw",
    "title": "43  MRSA",
    "section": "Multiple Sequence Alignment using ClustalW",
    "text": "Multiple Sequence Alignment using ClustalW\n\nExercise 43-10\nChoose five or more types of bacteria from your BLAST results, such as Staphylococcus epidermidis (include Staphylococcus aureus). You will be comparing the MecA gene’s sequence in each of these to find relations between them.\n\n\nExercise 43-11\nGo to Genbank and find nucleotide sequences for the MecA gene in each of your selected organisms. E.g., search for “Staphylococcus epidermidis MecA”. You should get the (when possible) complete sequence for the gene only. Get the header of the FASTA file as well.\n\n\nExericise 43-12\nGo to ClustalW at https://www.genome.jp/tools-bin/clustalw and input the sequences (with header first) of the mecA sequences from MRSA and the selected organisms. See Figure 43.3 Select DNA as you’re inputting nucleotide sequences. Feel free to try the protein sequences.\nRun the multiple alignments. - What does the output tell us? - How may these alignments be useful to us in determining the origin of the MecA gene?\n\n\nExercise 43-13\nAt the top of the output page, select a method in the tree menu (e.g., FastTree) and press Exec. This will generate a phylogram. A phylogram is a depiction of evolutionary relationships between taxa, in this case, the mecA gene from different types of bacteria. The branch lengths illustrate evolutionary distance and nodes from where the tree branches can be interpreted as common ancestors when we are working with vertical gene transfer. Try to imagine what the nodes could be interpreted as when we are talking about horizontal gene transfer. Hint: maybe the nodes could be interpreted as possible horizontal gene transfer events.\n\nWhat does the generated phylogram tell you?\nHow could we use phylograms to track the potential horizontal gene transfer between the different bacteria?\n\n\n\n\n\n\n\nFigure 43.3: Screenshot",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>MRSA</span>"
    ]
  },
  {
    "objectID": "chapters/web/mrsa_blast_multalign/index.html#discussion",
    "href": "chapters/web/mrsa_blast_multalign/index.html#discussion",
    "title": "43  MRSA",
    "section": "Discussion",
    "text": "Discussion\nBased on the BLAST and ClustalW results, which organisms might have been the potential source(s) for the mecA gene in MRSA?\n\nDiscuss evidence of potential horizontal gene transfer events based on sequence similarity.\nReason about the roles of the various HGT approaches in MRSA (i.e., Conjugation, Transduction, Transformation). What types are most likely between staph bacteria?\nDiscuss the significance of horizontal gene transfer in the rapid emergence of antibiotic resistance.\nWhat are some ways we can use sequence alignments of MRSA to combat them? E.g., treatment, outbreak management (like tracking sources, spread, and evolution), and new drugs such as protein-targeting vaccines.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>MRSA</span>"
    ]
  },
  {
    "objectID": "chapters/web/mrsa_blast_multalign/index.html#additional-tools",
    "href": "chapters/web/mrsa_blast_multalign/index.html#additional-tools",
    "title": "43  MRSA",
    "section": "Additional tools",
    "text": "Additional tools\nDTU (Technical University of Denmark) has developed a set of tools highly relevant to antibiotic resistance and MRSE specifically. These are not part of the assignment, but check them out if you are curious:\n\nIdentification of acquired antibiotic resistance genes\nSCCmecFinder identifies SCCmec elements in sequenced S. aureus isolate\nIdentification of acquired virulence genes\nspaTyper predicts the S. aureus spa type",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>MRSA</span>"
    ]
  },
  {
    "objectID": "chapters/project/hiv_project/index.html",
    "href": "chapters/project/hiv_project/index.html",
    "title": "44  HIV sub-groups",
    "section": "",
    "text": "Compute the similarity of two sequences\nThis chapter is a programming project where you will put your new programming skills to use analyzing an HIV DNA sequences.\nYou have now been introduced to all the programming rules you will see in this course. You now know all the building blocks required to write any program – literally any. The reason why computer geeks are good at what they do is not that they know some incomprehensible secrets. It is because they practiced, a lot. With practice, the simple rules you know now will let you write anything from first-person shooter games over jumbo jet autopilots to scripts for simple problems in bioinformatics. In the last three chapters, we will train your ability to solve bioinformatics problems by putting together all the things you have learned.\nThe programming project in this chapter deals with DNA sequences from HIV viruses. There are two types of HIV: HIV-1, which is by far the most common, and HIV-2, which is mostly found in West Africa. HIV-1 vira are divided into groups M, N, O, and P. The most important group M (for major) is one primarily responsible for the global epidemic. Group M is further divided into subtypes A, B, C, D, F, G, J, K, and CRFs. In this project we will look at sequences from the subtypes A, B, C, and D. You have multiple database sequences for each of these four subtypes and you have one unknown sequence from a patient that you need to assign to either subtype A, B, C or D. To do this you will have to write a program that predicts the subtype of the unknown sequence. How cool is that?\nDownload the files you need for this project:\nhttps://munch-group.org/bioinformatics/supplementary/project_files\nYou also need to download the two project files:\nPut the files in a folder dedicated to this project. On most computers you can right-click on the link and choose “Save file as…” or “Download linked file”.\nNow open each file in VScode and have a look at what is in the data files. (Do not change them in any way and do not save them after viewing. If VScode asks you if you want to save it before closing, say no.) How many sequences are there in each file?\nThe project is divided into the following parts:\nMake sure you read the entire exercise and understand what you are supposed to do before you begin!\nWe need to compare our unknown HIV sequence to all the HIV sequences of known subtypes. That way we can identify the sequence of a known subtype that is most similar to your unknown sequence. We will then assume that our unknown sequence has the same subtype as this sequence. To accomplish this we first need to write some code that compares two sequences so we can compare our HIV sequence to each of the other HIV sequences.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>HIV sub-groups</span>"
    ]
  },
  {
    "objectID": "chapters/project/hiv_project/index.html#compute-the-similarity-of-two-sequences",
    "href": "chapters/project/hiv_project/index.html#compute-the-similarity-of-two-sequences",
    "title": "44  HIV sub-groups",
    "section": "",
    "text": "Compare two sequences\nWrite a function sequence_similarity that takes two arguments:\n\nA string which is a DNA sequence.\nA string of the same length as argument one, which is also a DNA sequence.\n\nThe function must return:\n\nA float, which is the proportion of bases that are the same in two DNA sequences.\n\nExample usage:\nsequence_similarity('AGTC' 'AGTT')\nshould return 0.75.\nStart out defining your function like this:\ndef sequence_similarity(seq1, seq2):\n    # your code here...\nRemember that range(len(seq1)) generates the numbers you can use to index the string seq1. You can use those numbers as indexes to look up positions in both strings. You will need a for-loop in your function and a variable that keeps track of how many similarities you have seen as you iterate through the sequences.\n\n\nCompare aligned sequences\nAll sequences, including the unknown sequence, are from the same multiple alignment. This ensures that sequence positions match up across all sequences but also means that a lot of gap characters ('-') are inserted. To compute similarities between such sequences you need to make function much like seqeuence_similarity that does not consider sequence positions where both bases are a gap ('-') characters. In other words, you must not only count the number of characters that are the same, you also need to count how many alignment columns that are \"-\" for both sequences. E.g. the following mini alignment has five such columns and four columns where the bases are the same. So in the following alignment, the similarity is 0.8 (4/5):\nA-CT-A\nA-CTTA\nWrite a function alignment_similarity that takes two arguments:\n\nA string which is a DNA sequence with gap characters.\nA string of the same length as argument one, which is also a DNA sequence with gap characters.\n\nThe function must return:\n\nA float, which is the proportion of bases that are the same in two DNA sequences.\n\nalignment_similarity('A-CT-A', 'A-CTTA')\nshould return 0.8.\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\nUse an if-statement to test if the two characters at some index are equal to '-' in both sequences. You can use an expression like this:\nseq1[i] == '-' and seq2[i] == '-'\n\n\n\nOnce your function has computed both the number of identical bases and the number of alignment columns that are not both '-', you can have it return the similarity as the ratio of the two.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>HIV sub-groups</span>"
    ]
  },
  {
    "objectID": "chapters/project/hiv_project/index.html#read-the-hiv-sequences-into-your-program",
    "href": "chapters/project/hiv_project/index.html#read-the-hiv-sequences-into-your-program",
    "title": "44  HIV sub-groups",
    "section": "Read the HIV sequences into your program",
    "text": "Read the HIV sequences into your program\nTo use your alignment_similarity function to assess similarity between your unknown sequence and the sequences of known subtype, you need to read the sequences into your program. Here is a function that will read the sequences from one of the files you downloaded into a list:\ndef read_data(file_name):\n    f = open(file_name)\n    sequence_list = list()\n    for line in f:\n        seq = line.strip()\n        sequence_list.append(seq)\n    f.close()\n    return sequence_list\nYou can use that function to read the unknown sequence into your program:\nunknown_list = read_data('unknown_type.txt')\nIn this case, the list only contains the one unknown HIV sequence in unknown_type.txt.\nYou also need to load the typed HIV sequences into your program. Here is a function that returns a dictionary in which the keys are subtypes ('A', 'B', 'C' and 'D') and each value is a lists of sequences with that subtype:\ndef load_typed_sequences():\n    return {'A': read_data('subtypeA.txt'),\n            'B': read_data('subtypeB.txt'),\n            'C': read_data('subtypeC.txt'),\n            'D': read_data('subtypeD.txt') }\nIf you use the function like this:\ntyped_data = load_typed_sequences()\nthen you can access the list of sequences of subtype A like this: typed_data['A'].",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>HIV sub-groups</span>"
    ]
  },
  {
    "objectID": "chapters/project/hiv_project/index.html#compare-your-hiv-sequence-to-hiv-sequences-of-known-subtype",
    "href": "chapters/project/hiv_project/index.html#compare-your-hiv-sequence-to-hiv-sequences-of-known-subtype",
    "title": "44  HIV sub-groups",
    "section": "Compare your HIV sequence to HIV sequences of known subtype",
    "text": "Compare your HIV sequence to HIV sequences of known subtype\nTo type you HIV sequence you must compare your sequence to all the database sequences to see which group has the best matching sequence.\nWrite a function get_similarities that takes two arguments:\n\nA string, which is your unknown HIV sequence.\nA list of strings, each of which is an HIV sequence of known type.\n\nThe function must return:\n\nA list of floats, which should be the similarities between the unknown sequence given as the first argument and the list of sequences given as the second argument.\n\nExample usage:\nget_similarities(unknown_list[0], typed_data['A'])\nshould return:\n[0.8553288474061211, 0.8721742704480066,\n 0.854924397221087, 0.8481709291032696,\n 0.8498330281159108] \nThe function should use the function alignment_similarity to compare your unknown sequence (unknown_list[0]) to each of the sequences of some subtype. Start out like this:\ndef get_similarities(unknown, typed_sequences):\n    # Your code here...\n    \nIn your function you need to define a list that you can append the similarities you compute to:\nsimilarities = []\nThis is the list of results that your function must return. To compute the similarity between you unknown sequence and each of the sequences of known subtype, you can use your alignment_similarity function inside a for-loop.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>HIV sub-groups</span>"
    ]
  },
  {
    "objectID": "chapters/project/hiv_project/index.html#compute-maximum-similarity-to-each-subtype",
    "href": "chapters/project/hiv_project/index.html#compute-maximum-similarity-to-each-subtype",
    "title": "44  HIV sub-groups",
    "section": "Compute maximum similarity to each subtype",
    "text": "Compute maximum similarity to each subtype\nTo predict the subtype of the unknown HIV sequence you need to compare the unknown sequence to all the sequences of each of the different subtypes. The subtype of the sequence with the highest similarity to your unknown sequence is then our predicted subtype (or our best guess).\nWrite a function get_max_similarities that takes two arguments:\n\nA string, which is your unknown HIV sequence.\nA dictionary, like the one returned by load_typed_sequences.\n\nThe function must return:\n\nA dictionary, in which keys are strings representing each subtype ('A', 'B', 'C', and 'D') and values are floats representing the maximum similarity between the unknown sequence and the sequences of a subtype. The dictionary could look like this (it does not, you need to compute the similarities yourself.):\n\n{'A': 0.89, 'B': 0.95, 'C': 0.82, 'D': 0.99}\nTo get the highest number in a list of numbers, you can use the max function in Python. It works like this:\nnumbers = [3, 8, 53, 12, 7]\nprint(max(numbers))  # prints 53\nFor example, to get the highest similarity between the unknown sequence and sequences in typed_data['A']:\nsubtypeA_similarities = get_similarities(unknown_list[0], typed_data['A'])\nsubtypeA_max = max(subtypeA_similarities)",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>HIV sub-groups</span>"
    ]
  },
  {
    "objectID": "chapters/project/hiv_project/index.html#identify-the-hiv-subtype",
    "href": "chapters/project/hiv_project/index.html#identify-the-hiv-subtype",
    "title": "44  HIV sub-groups",
    "section": "Identify the HIV subtype",
    "text": "Identify the HIV subtype\nNow for the grand finale! You ultimately want to be able to write code like this:\nunknown_list = read_data('unknown_type.txt')\ntyped_data = load_typed_sequences()\nsubtype = predict_subtype(unknown_list[0], typed_data)\nprint(\"Patient HIV is subtype {}\".format(subtype))\nSo all you need now is the predict_subtype function.\nWrite a function predict_subtype that takes two arguments:\n\nA string, which is your unknown HIV sequence.\nA dictionary, like the one returned by load_typed_sequences.\n\nThe function must return:\n\nA string of length one (either 'A', 'B', 'C', or 'D') representing the predicted subtype of your unknown HIV sequence.\n\nThe function should use get_max_similarities to compute the dictionary of max similarities and then extract from that dictionary the key with the highest value (similarity). So the function must return 'A' if the unknown sequence is most similar to a sequence of subtype A, 'B' if the unknown sequence is most similar to a sequence of subtype B and so on.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>HIV sub-groups</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/clustering.html",
    "href": "chapters/bioinformatics/clustering.html",
    "title": "46  Clustering of sequences",
    "section": "",
    "text": "47 Princibles of clustering\nIn the realm of bioinformatics, clustering represents one of the most fundamental approaches to understanding the evolutionary relationships and functional similarities between biological sequences. Whether we are examining DNA sequences that encode the blueprints of life or protein sequences that carry out cellular functions, the ability to group related sequences together provides invaluable insights into evolution, function, and biological organization. This lecture note explores the comprehensive landscape of sequence clustering, from its theoretical foundations to practical applications in modern genomics and proteomics.\nThe importance of sequence clustering cannot be overstated in contemporary biological research. As sequencing technologies continue to advance at an unprecedented pace, researchers are confronted with enormous databases containing millions of sequences. The human genome alone contains approximately 3 billion base pairs, and when we consider the diversity of life on Earth, the amount of sequence data becomes truly astronomical. Clustering provides a systematic approach to organize, analyze, and interpret this vast biological information, enabling scientists to identify patterns, predict functions, and trace evolutionary histories.\nAt its core, sequence clustering is about finding meaningful relationships in biological data. When we cluster DNA sequences, we might be looking for genes that share common evolutionary origins, regulatory elements that control similar biological processes, or sequences that have been horizontally transferred between organisms. For protein sequences, clustering can reveal functional domains, identify protein families, and predict the three-dimensional structures and functions of newly discovered proteins. These applications extend far beyond academic curiosity – they directly impact drug discovery, disease diagnosis, agricultural improvements, and our fundamental understanding of life itself.\nThe process of clustering sequences involves multiple interconnected concepts from biology, mathematics, and computer science. We must consider how to measure similarity between sequences, which mathematical models best represent evolutionary processes, and how to efficiently compute relationships among potentially millions of sequences. This interdisciplinary nature makes sequence clustering both challenging and intellectually rewarding, requiring practitioners to bridge multiple fields of knowledge.\nThe fundamental principles underlying sequence clustering rest on the assumption that similarity in sequence reflects similarity in function and evolutionary origin. This assumption, while generally valid, requires careful consideration of the biological context and the specific goals of the analysis. The principles of clustering guide us in making decisions about how to group sequences, what criteria to use for similarity assessment, and how to interpret the resulting clusters in biological terms.\nDistance metrics form the foundation of clustering algorithms. In the context of biological sequences, distance can be measured in various ways, each with its own biological interpretation and computational implications. The simplest approach might count the number of positions where two sequences differ, known as the Hamming distance. However, this approach treats all changes equally, ignoring the biological reality that some mutations are more likely than others. For example, transitions (purine to purine or pyrimidine to pyrimidine changes) occur more frequently than transversions in DNA sequences, and certain amino acid substitutions are more conservative than others in proteins.\nMore sophisticated distance measures incorporate evolutionary models that account for these biological realities. The Jukes-Cantor model, one of the simplest, assumes equal rates of substitution between all nucleotides but corrects for multiple substitutions at the same site. The Kimura two-parameter model distinguishes between transitions and transversions, while more complex models like the General Time Reversible (GTR) model allow for different substitution rates between all pairs of nucleotides. For protein sequences, matrices like PAM (Point Accepted Mutation) and BLOSUM (BLOcks SUbstitution Matrix) encode empirically observed substitution frequencies, providing biologically meaningful distance measures.\nThe choice of clustering algorithm profoundly impacts the results and their interpretation. Hierarchical clustering methods, which build tree-like structures (dendrograms) showing relationships at multiple levels of granularity, are particularly popular in biological applications because they naturally represent evolutionary relationships. These methods can be agglomerative, starting with individual sequences and progressively merging them into larger clusters, or divisive, starting with all sequences in one cluster and recursively splitting them. Non-hierarchical methods, such as k-means clustering, partition sequences into a predetermined number of clusters without imposing a hierarchical structure, which can be useful when the goal is to identify functional groups rather than evolutionary relationships.\nThe concept of homology plays a central role in sequence clustering. Homologous sequences share a common evolutionary ancestor, and identifying homology is often the primary goal of clustering analyses. However, homology is not directly observable – we infer it from sequence similarity. This inference becomes complicated by the fact that sequences can be similar due to convergent evolution (homoplasy) rather than common ancestry. Furthermore, the relationship between sequence similarity and homology is not linear; sequences with less than 20-30% identity can still be homologous, while sequences with higher similarity might have arisen independently.\nAlignment quality significantly affects clustering results. Before sequences can be clustered, they must be aligned to identify corresponding positions. For closely related sequences, this alignment is straightforward, but for distantly related sequences, alignment becomes challenging and uncertain. Multiple sequence alignment algorithms must balance biological accuracy with computational efficiency, particularly when dealing with large datasets. Progressive alignment methods, which align sequences in the order determined by a guide tree, are commonly used but can propagate early errors throughout the alignment. Iterative refinement methods can improve alignment quality but at increased computational cost.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Clustering of sequences</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/clustering.html#which-pair-to-cluster-next",
    "href": "chapters/bioinformatics/clustering.html#which-pair-to-cluster-next",
    "title": "46  Clustering of sequences",
    "section": "Which pair to cluster next",
    "text": "Which pair to cluster next\nThe decision of which sequences or clusters to merge at each step is crucial in hierarchical clustering algorithms. This decision is based on a linkage criterion that defines the distance between clusters based on the distances between their constituent sequences. The choice of linkage criterion can dramatically affect the resulting cluster structure and its biological interpretation.\nSingle linkage clustering, also known as nearest-neighbor clustering, defines the distance between two clusters as the minimum distance between any pair of sequences, one from each cluster. This approach tends to produce elongated clusters and can suffer from “chaining,” where clusters grow by progressively adding sequences that are similar to the most recently added member but potentially quite different from early members. In biological contexts, single linkage can be useful for identifying sequences connected by a series of intermediates, such as in studying protein evolution through gene duplication and divergence.\nComplete linkage clustering, or furthest-neighbor clustering, uses the maximum distance between sequences in different clusters. This approach tends to produce compact, spherical clusters and is less susceptible to chaining than single linkage. However, complete linkage can be overly conservative, failing to group sequences that share clear evolutionary relationships but have diverged significantly in some regions. This method might be appropriate when the goal is to identify tight functional groups where all members maintain high similarity.\nAverage linkage clustering computes the mean distance between all pairs of sequences in different clusters. This approach, which includes variants like UPGMA (Unweighted Pair Group Method with Arithmetic Mean) and WPGMA (Weighted Pair Group Method with Arithmetic Mean), provides a balance between the extremes of single and complete linkage. UPGMA, in particular, has been widely used in phylogenetic analysis, though its assumption of a constant evolutionary rate (molecular clock) can lead to incorrect trees when this assumption is violated.\nWard’s method, another popular linkage criterion, minimizes the within-cluster variance when merging clusters. This approach tends to produce clusters of similar size and is particularly effective when the true clusters in the data have roughly equal variance. In biological applications, Ward’s method can be useful for identifying functional modules in protein families or gene expression data, where we expect relatively homogeneous groups.\nThe choice of linkage criterion should be guided by both the biological question and the characteristics of the data. For evolutionary studies, methods that respect the tree-like nature of descent with modification are preferred. For functional classification, methods that produce compact, well-separated clusters might be more appropriate. It’s often valuable to try multiple linkage criteria and compare the results, as consistent patterns across methods provide stronger evidence for genuine biological relationships.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Clustering of sequences</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/clustering.html#how-to-computes-distances-from-the-new-cluster",
    "href": "chapters/bioinformatics/clustering.html#how-to-computes-distances-from-the-new-cluster",
    "title": "46  Clustering of sequences",
    "section": "How to computes distances from the new cluster",
    "text": "How to computes distances from the new cluster\nOnce two sequences or clusters have been merged, we need a method to compute distances from this new cluster to all remaining sequences and clusters. This computation must be efficient, as it will be performed many times during the clustering process, and it should preserve the biological meaning of the distances.\nIn UPGMA, the distance from a new cluster to any other cluster is computed as the arithmetic mean of all pairwise distances between sequences in the two clusters. If cluster \\(C\\) is formed by merging clusters \\(A\\) and \\(B\\), and we want to compute the distance to cluster \\(D\\), we use the formula:\n\\[d(C,D) = \\frac{n_A \\times d(A,D) + n_B \\times d(B,D)}{n_A + n_B}\\]\nLet’s break down this formula: - \\(n_A\\) = number of sequences in cluster \\(A\\) - \\(n_B\\) = number of sequences in cluster \\(B\\) - \\(d(A,D)\\) = distance between clusters \\(A\\) and \\(D\\) - \\(d(B,D)\\) = distance between clusters \\(B\\) and \\(D\\)\nThe numerator \\(n_A \\times d(A,D) + n_B \\times d(B,D)\\) represents the sum of all pairwise distances between sequences in the merged cluster and cluster \\(D\\). The denominator \\(n_A + n_B\\) is the total number of sequences in the new cluster \\(C\\). This formula ensures that each original sequence contributes equally to the averaged distance, maintaining the unweighted nature of the method.\nFor example, if cluster \\(A\\) contains 3 sequences, cluster \\(B\\) contains 2 sequences, \\(d(A,D) = 0.4\\), and \\(d(B,D) = 0.6\\):\n\\[d(C,D) = \\frac{3 \\times 0.4 + 2 \\times 0.6}{3 + 2} = \\frac{1.2 + 1.2}{5} = \\frac{2.4}{5} = 0.48\\]\nThe WPGMA method modifies this calculation by giving equal weight to the two subclusters being merged, regardless of how many sequences each contains. The formula becomes:\n\\[d(C,D) = \\frac{d(A,D) + d(B,D)}{2}\\]\nUsing the same example values as above: \\[d(C,D) = \\frac{0.4 + 0.6}{2} = \\frac{1.0}{2} = 0.5\\]\nNote how WPGMA gives a different result (0.5) compared to UPGMA (0.48) because it doesn’t account for the different sizes of clusters \\(A\\) and \\(B\\). This can be interpreted as giving more recent speciation events greater influence on the computed distances, which might be appropriate if recent evolutionary changes are of particular interest.\nFor neighbor-joining, a more sophisticated approach is used that doesn’t assume a molecular clock. The algorithm maintains a matrix of distances and iteratively selects the pair of sequences that minimizes the total branch length of the tree. When clusters are merged, new distances are computed using a formula that accounts for the evolutionary distance along each branch, allowing for different rates of evolution in different lineages. The specific formula involves corrections for the average divergence of each taxon from all others, ensuring that the method remains consistent even when evolutionary rates vary.\nThe computational efficiency of distance updates is crucial for practical applications. Naive implementations that recompute all distances from scratch have O(n³) complexity for n sequences, which becomes prohibitive for large datasets. Efficient implementations maintain auxiliary data structures that allow distance updates in O(n²) time, making it feasible to cluster thousands of sequences. For even larger datasets, approximate methods or sampling strategies may be necessary.\nThe Lance-Williams formula provides a general framework for updating distances in hierarchical clustering. This formula expresses the distance from a new cluster to any other cluster as a linear combination of the distances from the constituent clusters, with coefficients that depend on the specific linkage criterion. Understanding this formula helps in implementing efficient clustering algorithms and in developing new linkage criteria tailored to specific biological questions.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Clustering of sequences</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/clustering.html#example-of-wrong-tree-produced-in-absence-of-molecular-clock",
    "href": "chapters/bioinformatics/clustering.html#example-of-wrong-tree-produced-in-absence-of-molecular-clock",
    "title": "46  Clustering of sequences",
    "section": "Example of wrong tree produced in absence of molecular clock",
    "text": "Example of wrong tree produced in absence of molecular clock\nTo understand how violation of the molecular clock assumption can lead to incorrect phylogenetic trees, consider a scenario with four species: A, B, C, and D. Suppose the true evolutionary history is ((A,B),(C,D)), meaning A and B share a more recent common ancestor with each other than with C or D, and likewise for C and D. However, imagine that after the split between the (A,B) and (C,D) lineages, species C experienced a dramatically increased rate of evolution, perhaps due to a change in DNA repair mechanisms or intense selective pressure.\nAs a result of this rate variation, species C accumulates many more mutations than the other species. When we measure pairwise distances, we might observe:\n\n\n\nSpecies Pair\nDistance\n\n\n\n\n\\(d(A,B)\\)\n10\n\n\n\\(d(C,D)\\)\n30\n\n\n\\(d(A,C)\\)\n40\n\n\n\\(d(A,D)\\)\n25\n\n\n\\(d(B,C)\\)\n42\n\n\n\\(d(B,D)\\)\n27\n\n\n\nUsing UPGMA, which assumes all species have evolved for the same amount of time, let’s trace through the clustering process:\nStep 1: Find the minimum distance - Minimum is \\(d(A,B) = 10\\) - Cluster A and B together to form cluster (A,B) - Height in dendrogram = \\(10/2 = 5\\)\nStep 2: Calculate new distances to cluster (A,B) Using the UPGMA formula with \\(n_A = n_B = 1\\):\n\\[d((A,B),C) = \\frac{1 \\times d(A,C) + 1 \\times d(B,C)}{1 + 1} = \\frac{40 + 42}{2} = \\frac{82}{2} = 41\\]\n\\[d((A,B),D) = \\frac{1 \\times d(A,D) + 1 \\times d(B,D)}{1 + 1} = \\frac{25 + 27}{2} = \\frac{52}{2} = 26\\]\nThe remaining distance \\(d(C,D) = 30\\).\nStep 3: Find the next minimum distance - Comparing: \\(d((A,B),C) = 41\\), \\(d((A,B),D) = 26\\), \\(d(C,D) = 30\\) - Minimum is \\(d((A,B),D) = 26\\) - Cluster (A,B) with D to form ((A,B),D) - Height = \\(26/2 = 13\\)\nStep 4: Final clustering - Only C remains to be added - Final tree topology: (((A,B),D),C)\nThis produces the incorrect topology (((A,B),D),C), suggesting that D is more closely related to A and B than to C, when the true topology should be ((A,B),(C,D)).\nThe error arises because UPGMA interprets the large distances to C as indicating ancient divergence, when in fact they reflect rapid evolution in the C lineage. This phenomenon, known as long-branch attraction in a different context, demonstrates how rate variation can mislead clustering algorithms that assume a molecular clock. The correct tree would be recovered by methods that don’t assume a molecular clock, such as neighbor-joining or maximum likelihood methods.\nThis example illustrates a general principle: when evolutionary rates vary significantly among lineages, methods that assume a molecular clock will tend to group slowly-evolving lineages together, regardless of their true evolutionary relationships. This can lead to systematic biases in phylogenetic reconstruction, particularly when studying groups that have experienced different selective pressures or have different biological characteristics affecting their mutation rates.\nReal-world examples of molecular clock violations abound. The evolution of HIV shows extreme rate variation, with some lineages evolving orders of magnitude faster than others due to differences in immune pressure and transmission dynamics. In mammals, the lineage leading to modern rodents shows accelerated evolution compared to primates, affecting everything from phylogenetic reconstruction to estimates of divergence times. Understanding these violations is crucial for choosing appropriate clustering methods and interpreting their results correctly.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Clustering of sequences</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/clustering.html#sec-upgma",
    "href": "chapters/bioinformatics/clustering.html#sec-upgma",
    "title": "46  Clustering of sequences",
    "section": "Neighbor-joining",
    "text": "Neighbor-joining\nNeighbor-joining (NJ), developed by Naruya Saitou and Masatoshi Nei in 1987, represents a major advance in phylogenetic reconstruction methods. Unlike UPGMA, neighbor-joining does not assume a molecular clock, making it more robust to rate variation among lineages. The method is based on the principle of minimum evolution, seeking the tree topology that minimizes the total branch length.\nThe neighbor-joining algorithm operates on a matrix of pairwise distances but uses a modified distance measure that accounts for the average divergence of each sequence from all others. For each pair of sequences \\(i\\) and \\(j\\), it computes a corrected distance:\n\\[Q(i,j) = (n-2) \\times d(i,j) - R_i - R_j\\]\nWhere: - \\(n\\) = the total number of sequences (or taxa) - \\(d(i,j)\\) = the original distance between sequences \\(i\\) and \\(j\\) - \\(R_i = \\sum_{k=1}^{n} d(i,k)\\) = the sum of distances from sequence \\(i\\) to all other sequences - \\(R_j = \\sum_{k=1}^{n} d(j,k)\\) = the sum of distances from sequence \\(j\\) to all other sequences\nThe pair with the minimum \\(Q\\) value is selected for joining. The logic behind this formula is to find the pair that minimizes the total tree length. The term \\((n-2) \\times d(i,j)\\) scales the distance between \\(i\\) and \\(j\\), while subtracting \\(R_i\\) and \\(R_j\\) corrects for the average divergence of each sequence.\nFor example, with 4 sequences and distances: - \\(d(1,2) = 0.3\\), \\(d(1,3) = 0.5\\), \\(d(1,4) = 0.6\\) - \\(d(2,3) = 0.4\\), \\(d(2,4) = 0.7\\) - \\(d(3,4) = 0.8\\)\nFirst, calculate the row sums: - \\(R_1 = 0.3 + 0.5 + 0.6 = 1.4\\) - \\(R_2 = 0.3 + 0.4 + 0.7 = 1.4\\) - \\(R_3 = 0.5 + 0.4 + 0.8 = 1.7\\) - \\(R_4 = 0.6 + 0.7 + 0.8 = 2.1\\)\nThen compute \\(Q\\) values: - \\(Q(1,2) = (4-2) \\times 0.3 - 1.4 - 1.4 = 0.6 - 2.8 = -2.2\\) - \\(Q(1,3) = 2 \\times 0.5 - 1.4 - 1.7 = 1.0 - 3.1 = -2.1\\) - And so on…\nWhen two sequences are joined in neighbor-joining, they are connected to a new internal node, and the branch lengths from this node to each sequence are computed to reflect their individual evolutionary distances. This differs from UPGMA, where both branches from a merge point have equal length. The formulas for branch lengths in NJ are:\n\\[v_i = \\frac{d(i,j)}{2} + \\frac{R_i - R_j}{2(n-2)}\\]\n\\[v_j = d(i,j) - v_i\\]\nWhere \\(v_i\\) and \\(v_j\\) are the branch lengths from the new internal node to sequences \\(i\\) and \\(j\\) respectively.\nThe first formula can be understood as: - \\(\\frac{d(i,j)}{2}\\) = half the distance between \\(i\\) and \\(j\\) (if they evolved at equal rates) - \\(\\frac{R_i - R_j}{2(n-2)}\\) = correction factor based on the difference in average divergence\nThe second formula ensures that \\(v_i + v_j = d(i,j)\\), preserving the original distance between the sequences.\nThe neighbor-joining algorithm has several desirable theoretical properties. It is consistent, meaning that given sufficient data from a tree-like evolutionary process, it will recover the correct tree topology. It is also relatively robust to modest violations of its assumptions, such as mild deviations from additivity in the distance matrix. These properties, combined with its computational efficiency (O(n³) but with a small constant factor), have made neighbor-joining one of the most widely used methods in molecular phylogenetics.\nHowever, neighbor-joining also has limitations. It assumes that the distance matrix is additive or nearly additive, meaning that distances can be perfectly represented as the sum of branch lengths on a tree. When this assumption is violated, such as in the presence of homoplasy or horizontal gene transfer, NJ can produce incorrect topologies. Additionally, NJ produces a single point estimate of the tree without measures of uncertainty, though bootstrap resampling can be used to assess the reliability of different parts of the tree.\nThe implementation of neighbor-joining requires careful attention to numerical precision, particularly when dealing with very similar or very divergent sequences. Round-off errors can accumulate during the iterative process, potentially affecting the final tree topology. Various optimizations have been developed to improve both the speed and accuracy of NJ implementations, including fast neighbor-joining algorithms that reduce the computational complexity for large datasets.\n\nAdditive distances / Non-recombining sequence ancestry\nThe concept of additive distances is fundamental to understanding when distance-based clustering methods will accurately recover evolutionary relationships. A distance matrix is additive if the distances can be exactly represented as path lengths on a tree with non-negative branch lengths. In other words, for any four sequences, the distances satisfy the four-point condition:\nFor any four taxa \\(i\\), \\(j\\), \\(k\\), and \\(l\\), we compute three sums: - \\(S_1 = d(i,j) + d(k,l)\\) - \\(S_2 = d(i,k) + d(j,l)\\) - \\(S_3 = d(i,l) + d(j,k)\\)\nThe four-point condition states that the two largest of these three sums must be equal. Mathematically:\n\\[\\max(S_1, S_2, S_3) = \\text{second largest}(S_1, S_2, S_3)\\]\nThis can also be written as: the two largest sums among \\(\\{d(i,j) + d(k,l), d(i,k) + d(j,l), d(i,l) + d(j,k)\\}\\) are equal.\nFor example, consider four sequences with the following distance matrix:\n    1    2    3    4\n1   0   0.2  0.5  0.6\n2  0.2   0   0.5  0.6\n3  0.5  0.5   0   0.3\n4  0.6  0.6  0.3   0\nLet’s check the four-point condition: - \\(S_1 = d(1,2) + d(3,4) = 0.2 + 0.3 = 0.5\\) - \\(S_2 = d(1,3) + d(2,4) = 0.5 + 0.6 = 1.1\\) - \\(S_3 = d(1,4) + d(2,3) = 0.6 + 0.5 = 1.1\\)\nHere, \\(S_2 = S_3 = 1.1\\) are the two largest values, so the four-point condition is satisfied, indicating these distances can be perfectly represented on a tree.\nIn biological terms, additive distances arise when sequences evolve according to a tree-like process without recombination or horizontal gene transfer. Each mutation occurs on a specific branch of the tree and contributes to the distance between all pairs of sequences separated by that branch. The total distance between any two sequences is then the sum of the lengths of the branches on the path connecting them in the tree.\nReal biological sequences often violate the additivity assumption to some degree. Recombination, which is common in many organisms, can cause different parts of a sequence to have different evolutionary histories. Horizontal gene transfer, particularly prevalent in prokaryotes, can move genetic material between distantly related lineages. Homoplasy, where independent mutations produce the same character state, can cause distances to underestimate the true evolutionary divergence. These violations can lead to distance matrices that cannot be perfectly represented on any tree.\nTesting for additivity involves checking whether the four-point condition holds for all quartets of sequences. In practice, perfect additivity is rare, and methods have been developed to measure the degree of deviation from additivity. The delta score quantifies how well a distance matrix fits a tree topology, with lower scores indicating better fit. Statistical tests can assess whether observed deviations from additivity are greater than expected from sampling error alone.\nWhen distances are approximately additive, methods like neighbor-joining can still recover the correct tree topology with high probability. The robustness of these methods to mild violations of additivity is one reason for their continued popularity. However, when additivity is strongly violated, alternative approaches may be needed, such as network-based methods that can represent conflicting signals in the data or methods that explicitly model recombination and horizontal gene transfer.\n\n\nMinimal evolution principle\nThe principle of minimum evolution, also known as the principle of parsimony at the level of tree length, suggests that among all possible tree topologies, the one requiring the least total amount of evolutionary change is most likely to be correct. This principle underlies many phylogenetic methods, including neighbor-joining, and provides a criterion for choosing among alternative evolutionary hypotheses.\nThe minimum evolution principle can be justified on both philosophical and statistical grounds. Philosophically, it embodies Occam’s razor – the simplest explanation consistent with the data is preferred. Statistically, under certain models of evolution, the tree with minimum length is the maximum likelihood estimate of the true tree. The principle is particularly compelling when evolutionary changes are rare relative to the time scales being considered, as unnecessary postulation of extra changes reduces the probability of the observed data.\nIn the context of distance-based clustering, minimum evolution seeks the tree topology and branch lengths that minimize the sum of all branch lengths while fitting the observed distances as closely as possible. This is typically formulated as a least-squares problem: find the tree T and branch lengths that minimize the sum of squared differences between observed distances and path lengths on the tree. Neighbor-joining can be viewed as a greedy heuristic for this optimization problem, making locally optimal choices that often lead to the globally optimal or near-optimal tree.\nThe minimum evolution criterion can be applied more generally through methods like minimum evolution distance methods and least-squares tree fitting. These methods search more extensively through tree space than neighbor-joining, potentially finding better trees at the cost of increased computation. Some implementations use branch swapping operations to explore local modifications of an initial tree, while others use more sophisticated search strategies.\nIt’s important to note that minimum evolution, like all optimality criteria, can be misled by certain patterns in the data. Long-branch attraction, where rapidly evolving lineages are incorrectly grouped together, can cause minimum evolution methods to prefer incorrect trees. Model misspecification, such as failing to account for rate variation among sites, can also lead to systematic biases. These limitations highlight the importance of using multiple methods and carefully considering the assumptions underlying each approach.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Clustering of sequences</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/clustering.html#statistical-principles-of-bootstrapping",
    "href": "chapters/bioinformatics/clustering.html#statistical-principles-of-bootstrapping",
    "title": "46  Clustering of sequences",
    "section": "Statistical Principles of Bootstrapping",
    "text": "Statistical Principles of Bootstrapping\nThe bootstrap is a resampling technique that estimates the sampling distribution of a statistic by repeatedly resampling with replacement from the original data. In the context of phylogenetic analysis, the “data” consists of aligned sequence positions (sites or columns in the alignment), and the “statistic” is typically the tree topology or specific clades within the tree. The fundamental principle is that if our inference method is robust, it should produce similar results when applied to slightly different datasets that could plausibly have been observed.\nTo understand why bootstrap analysis works, it’s essential to grasp how phylogenetic information is encoded in sequence alignments. Each column in an alignment represents a site that has evolved along the tree, and the pattern of nucleotides or amino acids at that site carries information about the evolutionary history. Consider a simple case with four sequences where a site shows the pattern AAGA. This pattern, where sequences 1, 2, and 4 share state A while sequence 3 has G, provides evidence for grouping sequences 1, 2, and 4 together, separated from sequence 3. The strength of this evidence depends on how many sites show consistent patterns supporting the same grouping.\nDifferent site patterns provide evidence for different branches in the tree. Sites that change along deep branches in the tree create patterns that support the major divisions among sequences, while sites that change along recent branches support the grouping of closely related sequences. The frequency of each site pattern in the alignment directly relates to the length of the corresponding branch in the true tree—longer branches accumulate more changes and thus generate more sites with patterns supporting those branches. This is why phylogenetic methods can estimate both tree topology and branch lengths from the distribution of site patterns.\nWhen an alignment contains many informative sites with strong, consistent signals, resampling these sites will likely reproduce similar site pattern frequencies in each bootstrap replicate. Even though each bootstrap sample contains a different mix of the original sites (with some appearing multiple times and others not at all), the overall distribution of site patterns remains relatively stable. Consequently, the same tree topology will be recovered from most bootstrap replicates, resulting in high bootstrap support values. For instance, if 100 sites strongly support grouping sequences A and B together, and only 10 sites weakly suggest alternative groupings, then most bootstrap replicates will still contain enough A-B supporting sites to recover that clade, even with the random variation introduced by resampling.\nConversely, when an alignment is short or contains few informative sites, the phylogenetic signal is weaker and more susceptible to sampling variation. If only 5 sites support grouping A with B while 3 sites support grouping A with C, bootstrap resampling can easily shift this delicate balance. Some bootstrap replicates might sample more A-B supporting sites, while others might happen to include multiple copies of the A-C supporting sites. This sampling variation leads to different trees being recovered from different bootstrap replicates, resulting in low bootstrap support values. This explains why short sequences or rapidly evolving regions with high noise-to-signal ratios tend to produce low bootstrap values—not necessarily because the relationships are wrong, but because the data provide insufficient evidence to overcome sampling variation.\nMathematically, let \\(X = \\{x_1, x_2, ..., x_n\\}\\) represent our original alignment with \\(n\\) sites. A bootstrap replicate \\(X^*_b\\) is created by randomly sampling \\(n\\) sites from \\(X\\) with replacement:\n\\[X^*_b = \\{x^*_{b,1}, x^*_{b,2}, ..., x^*_{b,n}\\}\\]\nwhere each \\(x^*_{b,i}\\) is randomly drawn from \\(\\{x_1, x_2, ..., x_n\\}\\) with probability \\(1/n\\).\nThe probability that a specific site \\(x_i\\) appears exactly \\(k\\) times in a bootstrap replicate follows a binomial distribution:\n\\[P(x_i \\text{ appears } k \\text{ times}) = \\binom{n}{k} \\left(\\frac{1}{n}\\right)^k \\left(1-\\frac{1}{n}\\right)^{n-k}\\]\nFor large \\(n\\), this approximates a Poisson distribution with parameter \\(\\lambda = 1\\). Consequently, the probability that a site doesn’t appear in a bootstrap replicate is:\n\\[P(x_i \\text{ absent}) = \\left(1-\\frac{1}{n}\\right)^n \\approx e^{-1} \\approx 0.368\\]\nThis means approximately 36.8% of the original sites will be absent from any given bootstrap replicate, while others will appear multiple times. This variation in site composition creates the diversity needed to assess uncertainty.\nThe bootstrap procedure for phylogenetic analysis follows these steps:\n\nGenerate \\(B\\) bootstrap replicates (typically \\(B = 100\\) to \\(1000\\))\nFor each replicate \\(b = 1, ..., B\\):\n\nCreate bootstrap alignment \\(X^*_b\\) by resampling sites\nReconstruct phylogenetic tree \\(T^*_b\\) using the same method applied to original data\n\nCalculate bootstrap support for each clade as:\n\n\\[BS(clade) = \\frac{\\#\\{T^*_b : clade \\in T^*_b\\}}{B} \\times 100\\%\\]\nThe bootstrap support value represents the percentage of bootstrap trees containing the specific clade. For example, if a clade appears in 850 out of 1000 bootstrap trees, it has 85% bootstrap support.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Clustering of sequences</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/clustering.html#bootstrap-assumptions-and-requirements",
    "href": "chapters/bioinformatics/clustering.html#bootstrap-assumptions-and-requirements",
    "title": "46  Clustering of sequences",
    "section": "Bootstrap Assumptions and Requirements",
    "text": "Bootstrap Assumptions and Requirements\nThe validity of bootstrap analysis in phylogenetics rests on several critical assumptions that must be carefully considered when interpreting results. The most fundamental assumption is that sequence sites evolve independently and identically distributed (i.i.d.). This means each site evolves according to the same stochastic process, independent of other sites. Mathematically, if \\(L(D|T,\\theta)\\) is the likelihood of data \\(D\\) given tree \\(T\\) and model parameters \\(\\theta\\):\n\\[L(D|T,\\theta) = \\prod_{i=1}^{n} L(x_i|T,\\theta)\\]\nThis multiplicative decomposition is only valid under the i.i.d. assumption. In reality, this assumption is often violated in several important ways. Linkage and recombination create dependencies where physically linked sites share evolutionary histories rather than evolving independently. Structural constraints in proteins and RNA molecules mean that sites must co-evolve to maintain three-dimensional structure and function, creating strong correlations between positions. The phenomenon of covarion evolution, where the evolutionary rate at one site depends on the states at other sites, further violates independence. Additionally, heterotachy, the variation in evolutionary rates across the tree at the same site, violates the identically distributed assumption.\nWhen these assumptions are violated, bootstrap values may be misleading. For instance, if sites are positively correlated due to structural constraints, bootstrap resampling may underestimate the true variance, leading to inflated bootstrap values.\nAnother crucial assumption is that the alignment is correct and fixed. Bootstrap analysis does not account for alignment uncertainty. If the alignment contains errors, these errors are propagated through all bootstrap replicates, potentially giving high support to incorrect relationships. This is particularly problematic for divergent sequences where alignment ambiguity is high.\nThe bootstrap also assumes that the phylogenetic reconstruction method is consistent—that is, it converges to the true tree given infinite data. If the method is statistically inconsistent due to model misspecification, high bootstrap values may support incorrect clades. This phenomenon, known as “high support for the wrong tree,” can occur under conditions like long-branch attraction with parsimony methods.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Clustering of sequences</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/clustering.html#what-bootstrap-values-quantify",
    "href": "chapters/bioinformatics/clustering.html#what-bootstrap-values-quantify",
    "title": "46  Clustering of sequences",
    "section": "What Bootstrap Values Quantify",
    "text": "What Bootstrap Values Quantify\nA common misconception is that bootstrap values represent the probability that a clade is true. However, bootstrap values actually quantify something more subtle: the repeatability or stability of the inference given data resampling. Specifically, a bootstrap value estimates:\n\\[BS(clade) \\approx P(clade \\in \\hat{T}^* | D)\\]\nwhere \\(\\hat{T}^*\\) is the estimated tree from a hypothetical replicate dataset drawn from the same underlying distribution as the original data.\nTo understand what this means, consider the relationship between bootstrap values and actual accuracy. Simulation studies have shown that the relationship is complex and depends on multiple factors. For well-supported clades, bootstrap values tend to be conservative, underestimating the actual accuracy of the inference. In contrast, for weakly-supported clades, bootstrap values may either overestimate or underestimate accuracy depending on the specific conditions. Most importantly, the relationship is non-linear—a 70% bootstrap value doesn’t translate directly to a 70% probability of the clade being correct.\nHillis and Bull (1993) found through simulations that bootstrap proportions of 70% or higher usually correspond to a 95% or greater probability of recovering true clades, suggesting bootstrap values are generally conservative. However, this relationship varies considerably with several factors. Sequence length plays a crucial role, with longer sequences generally yielding more reliable bootstrap estimates because they contain more information to overcome sampling variation. Substitution rate heterogeneity can distort bootstrap values, as greater heterogeneity violates model assumptions and may lead to systematic biases. The tree reconstruction method also matters significantly, with different methods showing different bootstrap behaviors—maximum likelihood methods often produce higher bootstrap values than parsimony for the same data. Finally, the true tree topology itself affects bootstrap performance, with balanced trees typically showing different patterns than highly imbalanced trees.\nThe bootstrap can be interpreted from a frequentist perspective as assessing sampling variance. If we could repeatedly sample sequences from the same evolutionary process, how often would we recover the same clade? This interpretation makes clear that bootstrap values reflect both the strength of phylogenetic signal and the sensitivity of our inference method to sampling variation.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Clustering of sequences</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/clustering.html#when-bootstrap-analysis-is-appropriate",
    "href": "chapters/bioinformatics/clustering.html#when-bootstrap-analysis-is-appropriate",
    "title": "46  Clustering of sequences",
    "section": "When Bootstrap Analysis is Appropriate",
    "text": "When Bootstrap Analysis is Appropriate\nBootstrap analysis is most appropriate when several key conditions are met. First and foremost, sufficient data must exist for meaningful resampling—with very short sequences containing fewer than 100 sites, bootstrap values are often uninformatively low because the limited data cannot support stable inference under resampling variation. The method works best when sites can be considered approximately independent, which is typically the case for single genes or carefully selected multi-gene datasets where linkage effects are minimal. Bootstrap is particularly valuable when the primary goal is to assess topological uncertainty rather than uncertainty in branch lengths, as branch length estimates are more sensitive to model assumptions. The technique also excels at comparing support across different nodes within the same analysis, providing a consistent framework for evaluating which relationships are well-supported versus questionable.\nBootstrap analysis may be less appropriate or require modification under certain circumstances. When analyzing genome-scale data, the independence assumption is strongly violated because genes on the same chromosome share evolutionary histories through linkage, and different genomic regions may have experienced different selective pressures or recombination events. Sites showing strong correlation, such as paired sites in RNA secondary structures that must co-evolve to maintain base-pairing, violate the independence assumption in ways that standard bootstrap cannot accommodate. High alignment uncertainty presents another challenge, as bootstrap doesn’t account for alignment errors, which become fixed across all bootstrap replicates and can lead to false confidence in incorrect relationships. Finally, bootstrap values aren’t directly comparable across different studies because they depend heavily on the specific dataset characteristics, reconstruction method, and model assumptions used in each analysis.\nFor genome-scale data, alternatives like the multi-scale bootstrap or site-pattern bootstrap may be more appropriate. These methods adjust for the fact that phylogenomic datasets violate traditional bootstrap assumptions.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Clustering of sequences</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/clustering.html#computational-implementation",
    "href": "chapters/bioinformatics/clustering.html#computational-implementation",
    "title": "46  Clustering of sequences",
    "section": "Computational Implementation",
    "text": "Computational Implementation\nThe computational cost of bootstrap analysis can be substantial, as it requires reconstructing many trees. The total computational complexity is:\n\\[O(B \\times C(n,m))\\]\nwhere \\(B\\) is the number of bootstrap replicates, and \\(C(n,m)\\) is the complexity of the tree reconstruction method for \\(n\\) taxa and \\(m\\) sites.\nHere’s a detailed algorithm for implementing bootstrap analysis:\nAlgorithm: Phylogenetic Bootstrap\nInput: Alignment D (n taxa × m sites), method M, replicates B\nOutput: Original tree T with bootstrap support values\n\n1. T ← ReconstructTree(D, M)  // Original tree\n2. Initialize clade_counts ← empty dictionary\n3. For b = 1 to B:\n4.     D* ← BootstrapSample(D, m)  // Resample m sites with replacement\n5.     T* ← ReconstructTree(D*, M)\n6.     For each clade in T*:\n7.         clade_counts[clade] ← clade_counts[clade] + 1\n8. For each clade in T:\n9.     support[clade] ← (clade_counts[clade] / B) × 100\n10. Return T with support values\nEfficient implementation requires careful optimization through several strategies. Parallel processing is particularly effective because bootstrap replicates are completely independent, allowing different replicates to be computed simultaneously on different processors without any communication overhead. Rapid bootstrapping methods, such as RAxML’s rapid bootstrap algorithm, combine fast hill-climbing searches with selective thorough optimization to reduce computation time by an order of magnitude while maintaining accuracy. The transfer bootstrap approach modifies the support calculation by using transfer distance instead of simple clade presence/absence, providing more nuanced support values that better reflect topological similarity. UFBoot (Ultrafast bootstrap) implements an approximation algorithm with statistical correction for bias, enabling bootstrap analysis of very large datasets that would be computationally prohibitive with standard methods.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Clustering of sequences</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/clustering.html#interpreting-bootstrap-results",
    "href": "chapters/bioinformatics/clustering.html#interpreting-bootstrap-results",
    "title": "46  Clustering of sequences",
    "section": "Interpreting Bootstrap Results",
    "text": "Interpreting Bootstrap Results\nBootstrap values should be interpreted carefully in biological context. General guidelines suggest that values above 95% indicate very strong support where the clade is highly reliable and unlikely to change with additional data. Values between 70% and 95% represent moderate to strong support, suggesting the clade is likely correct though some uncertainty remains. Bootstrap values from 50% to 70% indicate weak support where the clade is uncertain and should be treated with caution. Values below 50% provide no meaningful support, and such clades should be considered unreliable for biological inference.\nHowever, these thresholds are somewhat arbitrary and depend heavily on the specific analysis context. Several factors profoundly affect the interpretation of bootstrap values. Dataset size plays a major role, as larger datasets generally yield higher bootstrap values simply because more data provides more consistent signal that survives resampling. The evolutionary rate of the sequences matters significantly—rapidly evolving sequences often show lower bootstrap support because they accumulate more homoplasy and conflicting signals. Taxonomic scope influences values as well, with broader taxonomic sampling potentially reducing bootstrap values by introducing more variation and potential for conflicting signals. The reconstruction method used also affects absolute values, with maximum likelihood methods often producing higher bootstrap values than parsimony for the same dataset due to their more sophisticated modeling of the evolutionary process.\nBootstrap values can also serve as diagnostic tools to identify problematic regions of the tree. Consistently low values across multiple nodes may indicate rapid radiation events where multiple lineages diverged in quick succession, leaving insufficient time for synapomorphies to accumulate along internal branches. Such low values might also reveal conflicting phylogenetic signals arising from processes like incomplete lineage sorting or horizontal gene transfer. In some cases, low bootstrap support simply reflects insufficient data to confidently resolve relationships, suggesting that longer sequences or more genes are needed. Persistent patterns of low support in specific parts of the tree can also indicate systematic biases such as long-branch attraction, where the reconstruction method is being misled by convergent evolution or model misspecification.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Clustering of sequences</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/clustering.html#advanced-bootstrap-methods",
    "href": "chapters/bioinformatics/clustering.html#advanced-bootstrap-methods",
    "title": "46  Clustering of sequences",
    "section": "Advanced Bootstrap Methods",
    "text": "Advanced Bootstrap Methods\nSeveral variations and improvements to the standard bootstrap have been developed:\nParametric Bootstrap: Instead of resampling sites, simulate new datasets under the fitted model: \\[D^*_b \\sim P(D|\\hat{T}, \\hat{\\theta})\\]\nThis approach can be more powerful when the model is correctly specified but requires accurate parameter estimation.\nWeighted Bootstrap: Assign random weights \\(w_i \\sim Exponential(1)\\) to sites instead of resampling, maintaining continuous information.\nMulti-scale Bootstrap: Adjusts for data size by resampling \\(n'\\) sites where \\(n' = n^\\alpha\\) for various \\(\\alpha\\) values, then extrapolates to \\(\\alpha = 1\\).\nPosterior Predictive Bootstrap: Combines Bayesian and bootstrap approaches by resampling from the posterior predictive distribution.\nIn conclusion, bootstrap analysis provides a practical and widely-applicable method for assessing uncertainty in phylogenetic reconstruction. While it has limitations and its values don’t directly translate to probabilities of correctness, it remains one of the most important tools for evaluating the robustness of evolutionary inferences. Understanding its statistical foundations, assumptions, and proper interpretation is essential for anyone conducting phylogenetic analyses or sequence clustering studies.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Clustering of sequences</span>"
    ]
  },
  {
    "objectID": "chapters/project/seqdist_project/index.html",
    "href": "chapters/project/seqdist_project/index.html",
    "title": "47  Sequence trees",
    "section": "",
    "text": "Measuring sequence distance\nThis chapter is about clustering sequence based on the evolutionary distance between them.\nDownload the files you need for this project:\nhttps://munch-group.org/bioinformatics/supplementary/project_files\nPut the files in a folder dedicated to this project. On most computers you can right-click on the link and choose “Save file as…” or “Download linked file”.\nClustering is based on the distances between all pairs of sequences. So before you can build your tree you must compute those distances and fill them into a table like that in the book. Here we break that task into three parts:",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Sequence trees</span>"
    ]
  },
  {
    "objectID": "chapters/project/seqdist_project/index.html#measuring-sequence-distance",
    "href": "chapters/project/seqdist_project/index.html#measuring-sequence-distance",
    "title": "47  Sequence trees",
    "section": "",
    "text": "Compare two sequences\nMake the Jukes-Cantor correction\nGenerate a (lower triangular) distance matrix\n\n\nCompare two sequences\nThe first function you must write is one that finds the proportion of different bases between two sequences:\nWrite a function, sequence_difference, which takes two arguments:\n\nA string, which represents a DNA sequence.\nA string, which represents a DNA sequence of the same length as argument one.\n\nThe function must return:\n\nA float, which represents the proportion of different bases between the two sequences.\n\nExample usage:\nsequence_difference('AAATTAAA', 'AAAAAAAA')\nshould return\n0.25\n\n\nMake the Jukes-Cantor correction\nTo take into account that some substitutions may fall on top of others you must do the Jukes-Cantor correction you read about in the book. The formula is like this:\n\\[ K = -\\frac{3}{4} \\ln(1 - \\frac{4}{3}*D) \\]\nWhere \\(D\\) is the proportion of differences as returned by seqeunce_diff and \\(K\\) is the Jukes-Cantor corrected distance. In the top of seqdistproject.py it already says:\nfrom math import log\nThat line makes the log (logarithm) builtin function from the math python library available to your programme. Try to find its Python documentation to see how you use it.\nWrite a function, jukes_cantor, which takes one argument:\n\nA float, which represents a proportion of different bases between two sequences.\n\nThe function must return:\n\nA float, which represents the Jukes-Cantor corrected distance corresponding the proportion of differences given as argument.\n\nExample usage:\njukes_cantor(0.1)\nshould return\n0.10732563273050497",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Sequence trees</span>"
    ]
  },
  {
    "objectID": "chapters/project/seqdist_project/index.html#lower-triangular-distance-matrices",
    "href": "chapters/project/seqdist_project/index.html#lower-triangular-distance-matrices",
    "title": "47  Sequence trees",
    "section": "Lower triangular distance matrices",
    "text": "Lower triangular distance matrices\nThis project is all about distances between pairs (of sequences), and what would be more natural than to put all the distances in a matrix so you can look up the distance between the sequences with indexes i and j as the matrix element in row i and column j. You already know how matrices can be represented by lists of lists. E.g. a matrix like this:\n0.0  0.1  0.3  0.3  0.1  0.2\n0.1  0.0  0.2  0.4  0.1  0.1\n0.3  0.2  0.0  0.4  0.2  0.3\n0.3  0.4  0.4  0.0  0.2  0.1\n0.1  0.1  0.2  0.2  0.0  0.1\n0.2  0.1   0.3  0.1  0.1 0.0\n\ncan be expressed as a list of lists like this:\n[[0.0, 0.1, 0.3, 0.3, 0.1, 0.2],\n [0.1, 0.0, 0.2, 0.4, 0.1, 0.1],\n [0.3, 0.2, 0.0, 0.4, 0.2, 0.3],\n [0.3, 0.4, 0.4, 0.0, 0.2, 0.1],\n [0.1, 0.1, 0.2, 0.2, 0.0, 0.1],\n [0.2, 0.1, 0.3, 0.1, 0.1, 0.0]]\nNotice how the diagonal is all zeros because these distances represent the distance of a sequence to itself. Also, notice that the part above the diagonal is a mirror the part below the diagonal (in bold). This is all a bit redundant, especially in this project where you will have to reduce the matrix as you group (or cluster) sequences together. We want something nice and lean where we only have the numbers we need – and that is the lower triangular matrix:\n0.1\n0.3 0.2\n0.3 0.4 0.4\n0.1 0.1 0.2 0.2\n0.2 0.1 0.3 0.1 0.1\nIn Python this is still just a list of lists, only, each sublist now has the same length as its index in the big list (E.g. [0.3, 0.2] has index 2 in the list and has length 2):\nmatrix = [[],\n              [0.1],\n              [0.3, 0.2],\n              [0.3, 0.4, 0.4],\n              [0.1, 0.1, 0.2, 0.2],\n              [0.2, 0.1, 0.3, 0.1, 0.1]]\nHere I am just writing it nicely. If you where to print that list of lists it would look like this:\n[[], [0.1], [0.3, 0.2], [0.3, 0.4, 0.4], [0.1, 0.1, 0.2, 0.2], [0.2, 0.1, 0.3, 0.1, 0.1]]\nSay your sequences had names: A, B, C, D, E, and F, then the above data structure represents distances between each pair like this:\nA \nB 0.1\nC 0.3 0.2\nD 0.3 0.4 0.4\nE 0.1 0.1 0.2 0.2\nF 0.2 0.1 0.3 0.1 0.1\n   A   B   C   D   E   F\nThere is only one drawback with this reduced representation of the full square matrix: In the full matrix you can get the distance between the sequences with indexes i and j as both matrix[i][j] and matrix[j][i] because the part above and below the diagonal are the same. Using the lower triangular matrix, you must always use the largest index first. Using the smaller one first will give you an IndexError. So if you want the distance between sequences with index 2 and 4, you must use the bigger index first (as the row index): matrix[4][2].",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Sequence trees</span>"
    ]
  },
  {
    "objectID": "chapters/project/seqdist_project/index.html#generate-a-distance-matrix",
    "href": "chapters/project/seqdist_project/index.html#generate-a-distance-matrix",
    "title": "47  Sequence trees",
    "section": "Generate a distance matrix",
    "text": "Generate a distance matrix\nWrite a function, lower_trian_matrix, which takes one argument:\n\nA list of strings. All strings have equal length and represent DNA sequences.\n\nThe function must return:\n\nA list of lists of floats, which represent the lower triangular matrix of Jukes-Cantor distances between DNA sequences given as argument.\n\nExample usage:\nsequences = ['TAAAAAAAAAAA', \n             'TTAAAAAAAAAA', \n             'AAAAAAAAAAGG', \n             'AAAAAAAAGGGG']\nlower_trian_matrix(sequences)\nhere lower_trian_matrix should return:\n[[], \n [0.08833727674228764], \n [0.30409883108112323, 0.4408399986765892], \n [0.6081976621622466, 0.8239592165010822, 0.18848582121067953]]\nYou should use sequence_difference to compute the proportion of differences between each pair of sequences and jukes_cantor to produce the corrected distance to fill into the matrix.\nStart by figuring out what pairs of indexes you need and then figure out how you can make two nested for-loops generate them. Remember that the length of each sublist is equal to its index in the big list.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Sequence trees</span>"
    ]
  },
  {
    "objectID": "chapters/project/seqdist_project/index.html#clustering",
    "href": "chapters/project/seqdist_project/index.html#clustering",
    "title": "47  Sequence trees",
    "section": "Clustering",
    "text": "Clustering\nNow that you have the distance matrix you are ready for the actual clustering. There are three steps to that:\n\nFind the pair you want to join\nCompute the distances between the joined pair and all other elements (linkage)\nKeep going until you only have one left\n\nDepending on how you choose which pair to join and how you compute the new distances for the joined pair determines what kind of clustering you do. Here we will try a centroid-like linkage called WPGMA. It does not work as well as UPGMA but is a bit easier to implement (you can look up WPGMA on wikipedia).\n\nFind the pair to join\nHere you want to be able to find the pair with the smallest distance. To do that we identify the cell in the matrix with the smallest value:\nWrite a function, find_lowest_cell, which takes one argument:\n\nA list of lists, which represents a lower triangular distance matrix as returned by lower_trian_matrix.\n\nThe function must return:\n\nA list of two integers, which represent the row and column index of the cell with the smallest value in the matrix.\n\nRemember that the row index is always smaller than the column index. The two indexes tell you which two elements to join next.\nExample usage: Assuming that matrix is the lower triangular matrix returned by lower_trian_matrix in the previous example, then\nfind_lowest_cell(matrix)\nShould return\n[1, 0]\n\n\nDecide on a linkage method\nYou also need a function that computes a new distance from two original ones using the the centroid-like linkage we have decided to use.\nWrite a function, link, that takes two arguments:\n\nA float, which represents a matrix element.\nA float, which represents another matrix element.\n\nThe function must return:\n\nA float, which is the average of the two arguments\n\nExample usage:\nlink(0.4, 0.2)\nShould return:\n0.3",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Sequence trees</span>"
    ]
  },
  {
    "objectID": "chapters/project/seqdist_project/index.html#perform-the-clustering",
    "href": "chapters/project/seqdist_project/index.html#perform-the-clustering",
    "title": "47  Sequence trees",
    "section": "Perform the clustering",
    "text": "Perform the clustering\nThe three functions that do the actual clustering are complicated but you should be able to follow what they do. The first one updates the table to reflect that you join a pair. The second updates the list of sequence names (labels) to reflect that you joined a pair. The last one uses the two other functions to cluster pair until there is only one cluster left.\nYour task is to carefully type the code for each function and to understand what every line of code does.\n\nUpdating labels\nThe function update_labels takes three arguments:\n\nA list of strings representing sequence names.\nAn integer representing the index of a sequence name.\nAn integer representing the index of another sequence name.\n\nThe function does not return anything, but it updates the list of names to reflect that you joined a pair. If your list looks like this before you call the function:\nlabels = ['A', 'B', 'C', 'D']\nThen after you call the function like this update_labels(labels, 1, 0), the list will look like this:\n['(A,B)', 'C', 'D']\nHere is the function:\ndef update_labels(labels, i, j):\n\n    # turn the label at first index into a combination of both labels\n    labels[j] = \"({},{})\".format(labels[j], labels[i])\n\n    # Remove the (now redundant) label in the first index\n    del labels[i]\n\n\nUpdating the matrix\nThe function update_table takes three arguments:\n\nA list of lists, which represents a lower triangular distance matrix.\nAn integer representing the index of one of the elements to join.\nAn integer representing the index of the other element to join.\n\nThe way this function is implemented, it is assumed that the second argument is always larger than the third argument. I.e. the second argument is a row index and the third argument is a column index.\nThe function does not return anything, but it updates the matrix to reflect that a pair has been joined. If your matrix looks like this before you call the function:\nm = [[], [0.1], [0.3, 0.4], [0.6, 0.8, 0.2]]\nThen after you call the function like this update_table(m, 1, 0), the matrix will look like this:\n[[], [0.35], [0.7, 0.2]]\nHere is the function:\ndef update_table(table, a, b):\n\n    # For the lower index, reconstruct the entire row (ORANGE)\n    for i in range(0, b):\n        table[b][i] = link(table[b][i], table[a][i])\n\n    # Link cells to update the column above the min cell (BLUE)\n    for i in range(b+1, a):\n        table[i][b] = link(table[i][b], table[a][i])\n        \n    # Link cells to update the column below the min cell (RED)\n    for i in range(a+1, len(table)):\n        table[i][b] = link(table[i][b], table[i][a])\n\n    # Delete cells we no longer need (lighter colors)\n    for i in range(a+1, len(table)):\n        # Remove the (now redundant) first index column entry\n        del table[i][a]\n    # Remove the (now redundant) first index row\n    del table[a] \nThe colors refer to cell colors on the slide you I showed you at the lecture.\n\n\nDo the clustering\nNow onto the real task, the actual clustering. The function cluster takes two arguments:\n\nA list of strings representing DNA sequences of equal length.\nA list of strings representing sequence names.\n\nThe function returns:\n\nA string representing the generated clustering.\n\nHere is the function:\ndef cluster(sequences, names):\n\n    table = lower_trian_matrix(sequences)\n    labels = names[:]\n\n    # Until all labels have been joined...\n    while len(labels) &gt; 1:\n        # Locate lowest cell in the table\n        i, j = find_lowest_cell(table)\n\n        # Join the table on the cell co-ordinates\n        update_table(table, i, j)\n\n        # Update the labels accordingly\n        update_labels(labels, i, j)\n\n    # Return the final label\n    return labels[0]\nHere is a simple example of how you can use your new clustering code:\nnames = ['A', 'B', 'C', 'D']\nsequences = ['TAAAAAAAAAAA', \n             'TTAAAAAAAAAA', \n             'AAAAAAAAAAGG', \n             'AAAAAAAAGGGG']\n\ntree = cluster(sequences, names)\nprint(tree)\n\n\nOn your own\nFrom here on you are on your own. If you find a FASTA file with aligned (ungapped) homologous sequences, you can use the function below to read it into your program and try your code out on real-world sequences. I will leave it to you to figure out how it works.\ndef read_fasta(filename):\n    f = open(filename, 'r')\n    record_list = []\n    header = \"\"\n    sequence = \"\"\n    for line in f:\n        line = line.strip() ## get rid of whitespace and newline\n        if line.startswith(\"&gt;\"):\n            if header != \"\": ## if it is the first header\n                record_list.append([header, sequence])\n                sequence = \"\"\n            header = line[1:]\n        else:\n            sequence += line\n    record_list.append([header, sequence])\n    return record_list",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Sequence trees</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/models_of_dna_evolution.html",
    "href": "chapters/bioinformatics/models_of_dna_evolution.html",
    "title": "48  Models of DNA Evolution",
    "section": "",
    "text": "Introduction: Why Model DNA Evolution?\nThe study of molecular evolution requires mathematical frameworks to understand how DNA sequences change over time. When we observe two DNA sequences from different species, we see differences that have accumulated through evolutionary history. However, the observed differences represent only the net result of a complex process involving multiple substitutions, back-mutations, and parallel changes occurring over millions of years. Models of DNA evolution provide the mathematical machinery to infer evolutionary relationships, estimate divergence times, and understand the mechanisms driving genetic change.\nConsider two homologous DNA sequences that diverged from a common ancestor. The simple count of differences between them underestimates the true number of substitutions that occurred because some positions may have experienced multiple changes. A position that appears identical in both sequences might have undergone substitution followed by reversion to the original state. Similarly, convergent evolution might cause independent substitutions to the same nucleotide. Models of DNA evolution allow us to account for these hidden substitutions and provide more accurate estimates of evolutionary distances.\nThe importance of evolutionary models extends far beyond theoretical interest. These models form the foundation for phylogenetic reconstruction, enabling us to infer the tree of life from molecular data. They allow us to date evolutionary events, identify sites under selection, and understand patterns of molecular evolution across genomes. In practical applications, evolutionary models help track viral evolution, predict drug resistance mutations, and identify functionally important genomic regions through comparative analysis.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Models of DNA Evolution</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/models_of_dna_evolution.html#the-fundamental-challenge-multiple-substitutions",
    "href": "chapters/bioinformatics/models_of_dna_evolution.html#the-fundamental-challenge-multiple-substitutions",
    "title": "48  Models of DNA Evolution",
    "section": "The Fundamental Challenge: Multiple Substitutions",
    "text": "The Fundamental Challenge: Multiple Substitutions\nThe central challenge in modeling DNA evolution lies in accounting for multiple substitutions at the same site. When sequences diverge over long evolutionary periods, the probability increases that any given position experiences more than one substitution. This phenomenon, known as multiple hits or homoplasy, causes the observed sequence divergence to plateau even as evolutionary time continues to increase.\nTo illustrate this concept, imagine tracking a single nucleotide position over evolutionary time. Starting with adenine (A), it might mutate to guanine (G), then to thymine (T), and finally back to adenine. An observer comparing only the initial and final states would see no change, missing the three substitutions that actually occurred. This saturation effect becomes increasingly important as sequences diverge, making simple similarity measures inadequate for estimating evolutionary distances.\nThe mathematical framework for handling multiple substitutions involves continuous-time Markov chains. These models treat nucleotide substitution as a stochastic process where the probability of change depends only on the current state, not on the sequence’s history. The Markov property—memorylessness—simplifies the mathematics while capturing the essential features of the substitution process. The models specify instantaneous rates of change between nucleotides, from which we can derive the probabilities of observing particular nucleotide pairs after any amount of evolutionary time.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Models of DNA Evolution</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/models_of_dna_evolution.html#a-model-of-dna-evolution",
    "href": "chapters/bioinformatics/models_of_dna_evolution.html#a-model-of-dna-evolution",
    "title": "48  Models of DNA Evolution",
    "section": "A model of DNA evolution",
    "text": "A model of DNA evolution\nWe need a model that decribes the evolution of DNA. If the base at some position in a DNA sequence is \\(y \\in {A, T, C, G}\\), we would like to know the probability that it is \\(x \\in {A, T, C, G}\\) after some time bas passed. This is a complicated task, so let’s break it into simpler steps.\nTo model how the base at some DNA position changes over time, we need a way to represent the probability that the base is either a \\(A\\), \\(T\\), \\(G\\), or \\(C\\) at some point in time \\(t\\). We write those four probabilities as \\(p_A(t)\\), \\(p_T(t)\\), \\(p_G(t)\\), and \\(p_C(t)\\). We also need the probabilities that one base changes into another in a tiny span of time \\(\\Delta t\\). We call these substitution rates and write the rate of change from base \\(x\\) into base \\(y\\) as \\(\\mu_{xy}\\).\nLet’s for the sake of argument pretend that we know those probabilities. Then the model DNA evolution we need is one that uses those probabilities to compute, say, \\(p_A(t+\\Delta t)\\), the probability that our base is \\(A\\) after some tiny bit of extra time \\(\\Delta t\\):\n\\[\np_A(t+\\Delta t) = p_A(t) - p_A(t)\\, \\mu_{\\bullet A} \\Delta t + \\sum_{x \\neq A} p_x(t) \\, \\mu_{xA} \\Delta t\n\\]\nwhere \\(\\mu_{\\bullet A} = \\sum_{x \\neq A} \\mu_{Ax}\\) is the total probability that a \\(T\\), \\(G\\), \\(C\\) mutates into an \\(A\\).\nThe formula has three terms:\n\nThe probability that the base was \\(A\\) at time \\(t\\): \\(p_A(t)\\)\nThe probability that the base was \\(A\\) at time \\(t\\), but changed into another base after \\(\\Delta t\\): \\(p_A(t)\\, \\mu_{\\bullet A} \\Delta t\\)\nThe probability that the base was not \\(A\\) but changed into \\(A\\) after \\(\\sum_{x \\neq A} p_x(t) \\, \\mu_{xA} \\Delta t\\)\n\nWe can write this equation for each base and merge them into a single equation: \\[\n\\mathbf{p}(t + \\Delta t) = \\mathbf{p}(t) + \\mathbf{p}(t)Q\\Delta t\n\\]\nwhere \\[\n\\mathbf{p}(t) = (p_A(t),\\ p_C(t),\\ p_G(t),\\ p_T(t))\n\\] is a vector of the four probabilities and \\(Q\\) is a matrix of all the substitution rates:\n\\[\nQ = \\begin{pmatrix}\n-\\mu_{\\bullet A} & \\mu_{AG} & \\mu_{AC} & \\mu_{AT} \\\\\n\\mu_{GA} & -\\mu_{\\bullet G} & \\mu_{GC} & \\mu_{GT} \\\\\n\\mu_{CA} & \\mu_{CG} & -\\mu_{\\bullet C} &\\mu_{CT} \\\\\n\\mu_{TA} & \\mu_{TG} & \\mu_{TC} & -\\mu_{\\bullet T}\n\\end{pmatrix}\n\\]",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Models of DNA Evolution</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/models_of_dna_evolution.html#the-jukes-cantor-model-simplicity-and-symmetry",
    "href": "chapters/bioinformatics/models_of_dna_evolution.html#the-jukes-cantor-model-simplicity-and-symmetry",
    "title": "48  Models of DNA Evolution",
    "section": "The Jukes-Cantor Model: Simplicity and Symmetry",
    "text": "The Jukes-Cantor Model: Simplicity and Symmetry\nThe Jukes-Cantor (JC69) model, proposed in 1969, represents the simplest mathematical description of DNA evolution. This model assumes that all substitutions occur at the same rate and that all nucleotides have equal equilibrium frequencies. Despite its simplicity, or perhaps because of it, the JC69 model provides fundamental insights into the relationship between observed differences and actual evolutionary distances.\n\nModel Assumptions and Structure\nThe JC69 model makes several simplifying assumptions: 1. All nucleotides (A, C, G, T) have equal equilibrium frequencies (πA = πC = πG = πT = 0.25) 2. All substitutions occur at the same rate α 3. The substitution process is time-reversible 4. Sites evolve independently 5. The substitution rate is constant over time (molecular clock)\nUnder the JC69 model, each nucleotide has an equal probability of mutating to any of the three other nucleotides.\n\n\\[\n\\alpha = \\frac{1}{4} \\times \\mu\n\\]\n\\[\nQ = \\begin{pmatrix}\n-3\\alpha & \\alpha & \\alpha & \\alpha \\\\\n  \\alpha & -3\\alpha & \\alpha & \\alpha \\\\\n  \\alpha & \\alpha & -3\\alpha & \\alpha \\\\\n  \\alpha & \\alpha & \\alpha & -3\\alpha\n\\end{pmatrix}\n\\]\n\\[\nQ = \\begin{pmatrix}\n-\\frac{3\\mu}{4} & \\frac{\\mu}{4} & \\frac{\\mu}{4} & \\frac{\\mu}{4} \\\\\n\\frac{\\mu}{4} & -\\frac{3\\mu}{4} & \\frac{\\mu}{4} & \\frac{\\mu}{4} \\\\\n\\frac{\\mu}{4} & \\frac{\\mu}{4} & -\\frac{3\\mu}{4} & \\frac{\\mu}{4} \\\\\n\\frac{\\mu}{4} & \\frac{\\mu}{4} & \\frac{\\mu}{4} & -\\frac{3\\mu}{4} \\\\\n\\end{pmatrix}\n\\]\n\\[\nP(t) = \\exp(tQ)\n\\]\n\\[\nP = \\begin{pmatrix}\n\\frac{1}{4}+\\frac{3}{4}e^{-\\mu t} & \\frac{1}{4}-\\frac{1}{4}e^{-\\mu t} & \\frac{1}{4}-\\frac{1}{4}e^{-\\mu t} & \\frac{1}{4}-\\frac{1}{4}e^{-\\mu t} \\\\\n\\frac{1}{4}-\\frac{1}{4}e^{-\\mu t} & \\frac{1}{4}+\\frac{3}{4}e^{-\\mu t} & \\frac{1}{4}-\\frac{1}{4}e^{-\\mu t} & \\frac{1}{4}-\\frac{1}{4}e^{-\\mu t} \\\\\n\\frac{1}{4}-\\frac{1}{4}e^{-\\mu t} & \\frac{1}{4}-\\frac{1}{4}e^{-\\mu t} & \\frac{1}{4}+\\frac{3}{4}e^{-\\mu t} & \\frac{1}{4}-\\frac{1}{4}e^{-\\mu t} \\\\\n\\frac{1}{4}-\\frac{1}{4}e^{-\\mu t} & \\frac{1}{4}-\\frac{1}{4}e^{-\\mu t} & \\frac{1}{4}-\\frac{1}{4}e^{-\\mu t} & \\frac{1}{4}+\\frac{3}{4}e^{-\\mu t} \\\\\n\\end{pmatrix}\n\\]\n\\[\n\\nu = \\bigg(\\frac{\\mu}{4} + \\frac{\\mu}{4} + \\frac{\\mu}{4}\\bigg)t \\implies t\\mu = \\frac{4}{3}\\nu\n\\]\nMeasuing time in actual mutations, \\(\\nu\\), we get\n\\[\nP_{ij} = \\begin{cases}\n    \\frac{1}{4}+\\frac{3}{4}e^{-\\mu t} & \\text{if} & i = j \\\\\n    \\frac{1}{4}-\\frac{1}{4}e^{-\\mu t} & \\text{if} & i \\neq j\n\\end{cases}\n\\]\nWe can estimate the total observed substitution rate, \\(p\\), as the fraction of base positions that mutated between two sequences:\n\\[\np = 3\\times \\bigg(\\frac{1}{4} - \\frac{1}{4} e^{-4\\nu/3} \\bigg)= \\frac{3}{4} - \\frac{3}{4} e^{-4\\nu/3}\n\\] —\nNow, if we isolate \\(\\nu\\), we get the formula for the Jukes-Cantor correction:\n\\[\n\\nu = -\\frac{3}{4} \\ln(1-\\frac{4}{3}p)\n\\]\nIf we denote the substitution rate as α, then the rate matrix Q describing instantaneous rates of change is:\n\\[\nQ = \\begin{pmatrix}\n-3\\alpha & \\alpha & \\alpha & \\alpha \\\\\n\\alpha & -3\\alpha & \\alpha & \\alpha \\\\\n\\alpha & \\alpha & -3\\alpha & \\alpha \\\\\n\\alpha & \\alpha & \\alpha & -3\\alpha\n\\end{pmatrix}\n\\]\nThe diagonal elements ensure that each row sums to zero, maintaining the property of a rate matrix. The negative diagonal elements represent the rate of leaving each state, while off-diagonal elements represent rates of transition to other states.\n\n\nDeriving Transition Probabilities\nThe transition probability matrix P(t), giving the probability that nucleotide i changes to nucleotide j over time t, can be derived by exponentiating the rate matrix:\n\\[\nP(t) = e^{Qt}\n\\]\nTo solve this, we use the eigenvalue decomposition of Q. The rate matrix Q can be written as:\n\\[\nQ = \\alpha \\begin{pmatrix}\n-3 & 1 & 1 & 1 \\\\\n1 & -3 & 1 & 1 \\\\\n1 & 1 & -3 & 1 \\\\\n1 & 1 & 1 & -3\n\\end{pmatrix}\n\\]\nThis matrix has two distinct eigenvalues: - λ₁ = 0 (with multiplicity 1) - λ₂ = -4α (with multiplicity 3)\nThe eigenvector for λ₁ = 0 is (1, 1, 1, 1)ᵀ, representing the equilibrium distribution. The three eigenvectors for λ₂ = -4α span the space orthogonal to the equilibrium.\nUsing spectral decomposition, we obtain the transition probability matrix:\n\\[P(t) = \\frac{1}{4}\\mathbf{1}\\mathbf{1}^T + e^{-4\\alpha t}\\left(I - \\frac{1}{4}\\mathbf{1}\\mathbf{1}^T\\right)\\]\nwhere 𝟙 is a column vector of ones and I is the identity matrix. This yields the closed-form solutions:\n\\[P_{ii}(t) = \\frac{1}{4} + \\frac{3}{4}e^{-4\\alpha t}\\] \\[P_{ij}(t) = \\frac{1}{4} - \\frac{1}{4}e^{-4\\alpha t} \\text{ for } i \\neq j\\]\n\n\nWorked Example: Computing Transition Probabilities\nLet’s work through a concrete example. Suppose α = 0.01 substitutions per site per million years, and we want to know the probabilities after t = 10 million years.\nFirst, calculate the exponent: \\[-4\\alpha t = -4 \\times 0.01 \\times 10 = -0.4\\]\nThen: \\[e^{-0.4} \\approx 0.6703\\]\nThe probability that a nucleotide remains unchanged: \\[P_{ii}(10) = \\frac{1}{4} + \\frac{3}{4} \\times 0.6703 = 0.25 + 0.5027 = 0.7527\\]\nThe probability that a nucleotide changes to any specific other nucleotide: \\[P_{ij}(10) = \\frac{1}{4} - \\frac{1}{4} \\times 0.6703 = 0.25 - 0.1676 = 0.0824\\]\nWe can verify our calculation: \\[P_{ii} + 3 \\times P_{ij} = 0.7527 + 3 \\times 0.0824 = 0.7527 + 0.2472 \\approx 1.0\\]\nThis means that after 10 million years, there’s approximately a 75% chance a nucleotide remains unchanged and about 8% chance it changes to any specific other nucleotide.\n\n\nTwo-Sequence Divergence\nWhen comparing two sequences that diverged from a common ancestor, we need to consider evolution along both branches. If each branch has evolved for time t, the total evolutionary time is 2t.\nFor a single site, the probability that both sequences have the same nucleotide is:\n\\[P_{same} = \\sum_{i=A,C,G,T} \\pi_i \\left[\\sum_{j=A,C,G,T} P_{ij}(t) \\cdot P_{ij}(t)\\right]\\]\nGiven the symmetry of the JC69 model:\n\\[P_{same} = \\frac{1}{4} \\times 4 \\times \\left[P_{ii}^2(t) + 3P_{ij}^2(t)\\right]\\]\nSubstituting our expressions:\n\\[P_{same} = \\left(\\frac{1}{4} + \\frac{3}{4}e^{-4\\alpha t}\\right)^2 + 3\\left(\\frac{1}{4} - \\frac{1}{4}e^{-4\\alpha t}\\right)^2\\]\nAfter algebraic simplification:\n\\[P_{same} = \\frac{1}{4} + \\frac{3}{4}e^{-8\\alpha t}\\]\nTherefore, the probability that sites differ is:\n\\[p = 1 - P_{same} = \\frac{3}{4}(1 - e^{-8\\alpha t})\\]\n\n\nDistance Correction Formula\nThe observed proportion of differences p underestimates the actual number of substitutions due to multiple hits. To correct for this, we solve for the evolutionary distance d = 2αt:\nFrom: \\[p = \\frac{3}{4}(1 - e^{-\\frac{4d}{3}})\\]\nSolving for d: \\[1 - \\frac{4p}{3} = e^{-\\frac{4d}{3}}\\]\nTaking the natural logarithm: \\[\\ln\\left(1 - \\frac{4p}{3}\\right) = -\\frac{4d}{3}\\]\nTherefore: \\[d = -\\frac{3}{4}\\ln\\left(1 - \\frac{4p}{3}\\right)\\]\nThis is the Jukes-Cantor distance correction formula, which estimates the expected number of substitutions per site from the observed proportion of differences.\n\n\nNumerical Example: Distance Correction\nConsider two sequences with the following alignment:\nSequence 1: ATCGATCGATCGATCGATCG (20 nucleotides)\nSequence 2: ATCGTTCCATCGAACGATGG (20 nucleotides)\nDifferences:     *  *      * *    ** (6 differences)\nThe observed proportion of differences: \\[p = \\frac{6}{20} = 0.30\\]\nApplying the JC69 correction: \\[d = -\\frac{3}{4}\\ln\\left(1 - \\frac{4 \\times 0.30}{3}\\right) = -\\frac{3}{4}\\ln(1 - 0.40) = -\\frac{3}{4}\\ln(0.60)\\]\n\\[d = -\\frac{3}{4} \\times (-0.5108) = 0.3831\\]\nSo while we observe 30% differences, the estimated number of substitutions per site is 0.383, or 38.3%. This 28% increase (from 0.30 to 0.383) accounts for multiple substitutions at the same sites.\n\n\nSaturation and Model Limitations\nAs sequences diverge, the observed differences plateau while actual substitutions continue to accumulate. The JC69 model predicts:\n\nWhen d = 0.1: p ≈ 0.097 (minimal correction needed)\nWhen d = 0.5: p ≈ 0.447 (moderate correction)\nWhen d = 1.0: p ≈ 0.681 (substantial correction)\nWhen d = 2.0: p ≈ 0.747 (near saturation)\nAs d → ∞: p → 0.75 (complete saturation)\n\nThe correction formula becomes undefined when p ≥ 0.75, as ln(1 - 4p/3) is undefined for p ≥ 0.75. This represents complete saturation—sequences are no more similar than random.\n\n\nProperties and Insights\nThe JC69 model reveals several fundamental properties of neutral evolution:\n\nExponential decay: The probability of identity decreases exponentially with time\nEquilibrium convergence: All sequences converge to 25% of each nucleotide\nReversibility: P(i→j in time t) × πᵢ = P(j→i in time t) × πⱼ\nSaturation: Observable differences plateau at 75% as time increases\nMultiple hits: The correction factor increases dramatically with divergence\n\nThese properties make the JC69 model a valuable baseline for understanding molecular evolution, even though its assumptions are oversimplified for real sequences.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Models of DNA Evolution</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/models_of_dna_evolution.html#the-kimura-two-parameter-model-transitions-and-transversions",
    "href": "chapters/bioinformatics/models_of_dna_evolution.html#the-kimura-two-parameter-model-transitions-and-transversions",
    "title": "48  Models of DNA Evolution",
    "section": "The Kimura Two-Parameter Model: Transitions and Transversions",
    "text": "The Kimura Two-Parameter Model: Transitions and Transversions\n\n\n\n\nMutation diagram\n\n\n\\[\nQ = \\begin{pmatrix}\n-2\\beta-\\alpha & \\beta & \\alpha & \\beta \\\\\n  \\beta & -2\\beta-\\alpha & \\beta & \\alpha \\\\\n  \\alpha & \\beta & -2\\beta-\\alpha & \\beta \\\\\n  \\beta & \\alpha & \\beta & -2\\beta-\\alpha\n\\end{pmatrix}\n\\]\n\\(\\alpha\\): transition rate. \\(\\beta\\): transversion rate.\n\\[\nd_{K2P} = - \\frac{1}{2} \\ln \\big( 1 - 2P - Q\\big) - \\frac{1}{4} \\ln \\big( 2Q\\big)\n\\]\n\\(P\\) is the fraction of transitions and \\(Q\\) is the fraction of transversions.\n\nEmpirical observations reveal that not all substitutions occur with equal frequency. Transitions (purine to purine or pyrimidine to pyrimidine changes) typically occur more frequently than transversions (purine to pyrimidine or vice versa). The Kimura two-parameter (K80) model, introduced in 1980, incorporates this biological reality while maintaining mathematical tractability.\nThe K80 model distinguishes between transition rate α and transversion rate β, typically with α &gt; β. The rate matrix becomes:\n\\[\nQ = \\begin{pmatrix}\n-(\\alpha + 2\\beta) & \\beta & \\alpha & \\beta \\\\\n\\beta & -(\\alpha + 2\\beta) & \\beta & \\alpha \\\\\n\\alpha & \\beta & -(\\alpha + 2\\beta) & \\beta \\\\\n\\beta & \\alpha & \\beta & -(\\alpha + 2\\beta)\n\\end{pmatrix}\n\\]\nThis structure reflects the chemical basis of mutations. Transitions involve substitutions between chemically similar bases and often result from spontaneous deamination or tautomeric shifts. Transversions require more dramatic chemical changes and occur less frequently. The transition/transversion ratio (κ = α/β) typically ranges from 2 to 10 in nuclear DNA, though it can be higher in mitochondrial sequences.\nThe transition probability functions for the K80 model are:\n\\[\nP_{ii}(t) = \\frac{1}{4} + \\frac{1}{4}e^{-4\\beta t} + \\frac{1}{2}e^{-2(\\alpha + \\beta)t}\n\\]\n\\[\n_{ij}^{ts}(t) = \\frac{1}{4} + \\frac{1}{4}e^{-4\\beta t} - \\frac{1}{2}e^{-2(\\alpha + \\beta)t}\n\\]\n(transitions)\n\\[\nP_{ij}^{tv}(t) = \\frac{1}{4} - \\frac{1}{4}e^{-4\\beta t}\n\\]\n(transversions)\nThese equations reveal how the model captures both fast (transitions) and slow (transversions) evolutionary processes. The presence of two exponential terms reflects the two-parameter nature of the model, with different decay rates for different types of substitutions.\nTo estimate evolutionary distances under the K80 model, we need to observe both the proportion of transitional differences (S) and transversional differences (V):\n\\[\nd = -\\frac{1}{2}\\ln(1 - 2S - V) - \\frac{1}{4}\\ln(1 - 2V)\n\\]\nThis formula shows how the model uses additional information about substitution types to provide more accurate distance estimates than the JC69 model.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Models of DNA Evolution</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/models_of_dna_evolution.html#the-felsenstein-81-model-unequal-base-frequencies",
    "href": "chapters/bioinformatics/models_of_dna_evolution.html#the-felsenstein-81-model-unequal-base-frequencies",
    "title": "48  Models of DNA Evolution",
    "section": "The Felsenstein 81 Model: Unequal Base Frequencies",
    "text": "The Felsenstein 81 Model: Unequal Base Frequencies\nReal DNA sequences rarely exhibit equal frequencies of all four nucleotides. The Felsenstein 81 (F81) model addresses this limitation by allowing unequal equilibrium base frequencies while maintaining equal rates for all substitution types. This model recognizes that compositional biases—such as high GC content in certain genomes—affect substitution patterns.\nIn the F81 model, the rate of substitution from nucleotide i to nucleotide j is proportional to the equilibrium frequency of j (denoted πⱼ). The rate matrix is:\n\\[Q_{ij} = \\begin{cases}\n\\beta\\pi_j & \\text{if } i \\neq j \\\\\n-\\beta(1 - \\pi_i) & \\text{if } i = j\n\\end{cases}\\]\nThis formulation ensures that the substitution process maintains the specified equilibrium frequencies. The model captures the idea that mutations to common nucleotides occur more frequently than mutations to rare nucleotides, reflecting the underlying mutational processes or selective constraints maintaining compositional biases.\nThe transition probabilities for the F81 model are:\n\\[P_{ij}(t) = \\begin{cases}\n\\pi_j + (\\delta_{ij} - \\pi_j)e^{-\\beta t} & \\text{general form} \\\\\n\\pi_j + (1 - \\pi_j)e^{-\\beta t} & \\text{if } i = j \\\\\n\\pi_j(1 - e^{-\\beta t}) & \\text{if } i \\neq j\n\\end{cases}\\]\nwhere δᵢⱼ is the Kronecker delta (1 if i=j, 0 otherwise). These equations show how sequences evolve toward equilibrium frequencies, with the rate of approach governed by the parameter β.\nThe F81 model provides insights into how compositional constraints shape molecular evolution. In GC-rich genomes, for example, AT→GC substitutions occur more frequently than GC→AT substitutions, maintaining the compositional bias. This model forms the basis for more complex models that combine unequal base frequencies with other biological features.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Models of DNA Evolution</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/models_of_dna_evolution.html#the-hky85-model-combining-biological-realism",
    "href": "chapters/bioinformatics/models_of_dna_evolution.html#the-hky85-model-combining-biological-realism",
    "title": "48  Models of DNA Evolution",
    "section": "The HKY85 Model: Combining Biological Realism",
    "text": "The HKY85 Model: Combining Biological Realism\nThe Hasegawa-Kishino-Yano (HKY85) model represents a synthesis of the K80 and F81 models, incorporating both transition/transversion bias and unequal base frequencies. This model, proposed in 1985, strikes a balance between biological realism and computational tractability, making it one of the most widely used models in phylogenetic analysis.\nThe HKY85 rate matrix combines the features of its predecessors:\n\\[Q_{ij} = \\begin{cases}\n\\beta\\pi_j & \\text{for transversions} \\\\\n\\beta\\kappa\\pi_j & \\text{for transitions} \\\\\n-\\sum_{k \\neq i} Q_{ik} & \\text{for } i = j\n\\end{cases}\\]\nwhere κ represents the transition/transversion rate ratio. This formulation captures two major features of molecular evolution: the chemical basis of mutations (through κ) and compositional constraints (through π).\nThe eigenvalue decomposition of the HKY85 rate matrix reveals its mathematical structure. While the general transition probabilities lack simple closed-form expressions, they can be computed efficiently using matrix exponentiation methods. The model has four distinct eigenvalues, reflecting its increased complexity compared to simpler models.\nDistance estimation under HKY85 requires numerical methods rather than analytical solutions. Maximum likelihood estimation provides optimal estimates of both evolutionary distances and model parameters (κ and base frequencies). The likelihood function incorporates information from all sites, weighting the contribution of each pattern of nucleotides according to its probability under the model.\nThe HKY85 model’s success stems from its ability to capture essential features of molecular evolution without excessive parameterization. It accounts for approximately 85% of the variation in substitution patterns observed in real sequences while remaining computationally feasible for large datasets.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Models of DNA Evolution</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/models_of_dna_evolution.html#the-general-time-reversible-model-maximum-flexibility",
    "href": "chapters/bioinformatics/models_of_dna_evolution.html#the-general-time-reversible-model-maximum-flexibility",
    "title": "48  Models of DNA Evolution",
    "section": "The General Time Reversible Model: Maximum Flexibility",
    "text": "The General Time Reversible Model: Maximum Flexibility\nThe General Time Reversible (GTR) model represents the most general neutral model of DNA evolution with time reversibility. This model allows all six pairs of nucleotide substitutions to have different rates while maintaining the mathematical property of time reversibility—essential for most phylogenetic methods.\nThe GTR rate matrix has the form:\n\\[Q_{ij} = \\begin{cases}\nr_{ij}\\pi_j & \\text{if } i \\neq j \\\\\n-\\sum_{k \\neq i} Q_{ik} & \\text{if } i = j\n\\end{cases}\\]\nwhere rᵢⱼ represents the exchangeability parameter between nucleotides i and j (with rᵢⱼ = rⱼᵢ for reversibility), and πⱼ is the equilibrium frequency of nucleotide j. The model has 8 free parameters: 5 exchangeability parameters (the sixth is fixed for identifiability) and 3 base frequencies (the fourth is determined by the constraint that frequencies sum to 1).\nTime reversibility means that the probability of observing sequence S₁ at time 0 and S₂ at time t equals the probability of observing S₂ at time 0 and S₁ at time t:\n\\[\\pi_i P_{ij}(t) = \\pi_j P_{ji}(t)\\]\nThis property, while biologically questionable (evolution has direction), proves mathematically convenient and appears to have little practical impact on phylogenetic inference for most datasets.\nThe GTR model encompasses all previously discussed models as special cases: - JC69: all rᵢⱼ = 1, all πᵢ = 0.25 - K80: all πᵢ = 0.25, transitions share one rate, transversions another - F81: all rᵢⱼ = 1, πᵢ estimated from data - HKY85: transitions share one rate, transversions another, πᵢ estimated\nThis hierarchy of models allows researchers to select appropriate complexity levels through statistical model selection procedures. The Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) can identify whether additional parameters significantly improve model fit.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Models of DNA Evolution</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/models_of_dna_evolution.html#rate-heterogeneity-the-gamma-distribution",
    "href": "chapters/bioinformatics/models_of_dna_evolution.html#rate-heterogeneity-the-gamma-distribution",
    "title": "48  Models of DNA Evolution",
    "section": "Rate Heterogeneity: The Gamma Distribution",
    "text": "Rate Heterogeneity: The Gamma Distribution\nReal sequences exhibit substantial variation in substitution rates across sites. Some positions evolve rapidly due to weak functional constraints, while others remain nearly invariant due to strong purifying selection. Ignoring rate heterogeneity leads to underestimation of evolutionary distances and incorrect phylogenetic inference.\nThe gamma distribution provides a flexible framework for modeling rate heterogeneity. If rates across sites follow a gamma distribution with shape parameter α, the probability density function is:\n\\[f(r) = \\frac{\\alpha^\\alpha}{\\Gamma(\\alpha)} r^{\\alpha-1} e^{-\\alpha r}\\]\nThe shape parameter α determines the extent of rate variation: - α &lt; 1: L-shaped distribution with many slowly evolving sites - α = 1: exponential distribution - α &gt; 1: bell-shaped distribution - α → ∞: no rate variation (all sites evolve at the same rate)\nEmpirical studies typically find α values between 0.1 and 2, indicating substantial rate heterogeneity in most sequences. Low α values (high heterogeneity) are characteristic of functional sequences with many conserved positions.\nThe continuous gamma distribution is often approximated using discrete categories for computational efficiency. Yang’s discrete gamma model divides sites into k categories (typically 4-8) with equal probabilities, choosing rates to preserve the mean and variance of the continuous distribution. The likelihood calculation then involves summing over all possible rate categories for each site:\n\\[L = \\prod_{i=1}^n \\sum_{j=1}^k \\frac{1}{k} P(D_i | r_j)\\]\nwhere Dᵢ represents the data at site i and rⱼ is the rate for category j.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Models of DNA Evolution</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/models_of_dna_evolution.html#invariant-sites-accounting-for-functional-constraints",
    "href": "chapters/bioinformatics/models_of_dna_evolution.html#invariant-sites-accounting-for-functional-constraints",
    "title": "48  Models of DNA Evolution",
    "section": "Invariant Sites: Accounting for Functional Constraints",
    "text": "Invariant Sites: Accounting for Functional Constraints\nSome positions in functional sequences cannot tolerate any substitutions without losing essential function. These invariant sites violate the assumptions of standard models, which predict that all sites should eventually vary given sufficient time. The proportion of invariant sites (pᵢₙᵥ) can be substantial in coding sequences, particularly at first and second codon positions.\nThe I+G model combines invariant sites with gamma-distributed rates for variable sites:\n\\[L = p_{inv} L_{inv} + (1 - p_{inv}) L_{\\Gamma}\\]\nwhere Lᵢₙᵥ is the likelihood of observing no change and L_Γ is the likelihood under the gamma model. This formulation recognizes two classes of conservation: absolute constraint (invariant sites) and strong but not absolute constraint (slowly evolving variable sites).\nParameter estimation in I+G models requires care to avoid identifiability problems. As the gamma shape parameter approaches zero, the gamma distribution places increasing weight on slowly evolving sites, potentially confounding with truly invariant sites. Maximum likelihood estimation must navigate this parameter correlation carefully.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Models of DNA Evolution</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/models_of_dna_evolution.html#codon-models-the-genetic-codes-influence",
    "href": "chapters/bioinformatics/models_of_dna_evolution.html#codon-models-the-genetic-codes-influence",
    "title": "48  Models of DNA Evolution",
    "section": "Codon Models: The Genetic Code’s Influence",
    "text": "Codon Models: The Genetic Code’s Influence\nWhen analyzing protein-coding sequences, nucleotide substitution models ignore the genetic code’s structure. Codon models explicitly incorporate the relationship between DNA changes and amino acid replacements, providing more biologically realistic descriptions of coding sequence evolution.\nThe Muse-Gaut and Goldman-Yang models represent two major approaches to codon modeling. Both distinguish between synonymous substitutions (not changing the amino acid) and non-synonymous substitutions (changing the amino acid), but differ in their parameterization.\nThe Goldman-Yang model parameterizes rates as:\n\\[q_{ij} \\propto \\begin{cases}\n\\pi_j & \\text{for synonymous transversions} \\\\\n\\kappa\\pi_j & \\text{for synonymous transitions} \\\\\n\\omega\\pi_j & \\text{for non-synonymous transversions} \\\\\n\\omega\\kappa\\pi_j & \\text{for non-synonymous transitions} \\\\\n0 & \\text{for multiple nucleotide changes}\n\\end{cases}\\]\nwhere ω = dN/dS represents the ratio of non-synonymous to synonymous substitution rates. This ratio provides information about selection: - ω &lt; 1: purifying selection (most amino acid changes are deleterious) - ω = 1: neutral evolution - ω &gt; 1: positive selection (amino acid changes are advantageous)\nCodon models can detect positive selection even when it affects only a small proportion of sites, making them powerful tools for identifying functionally important regions and adaptive evolution.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Models of DNA Evolution</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/models_of_dna_evolution.html#context-dependent-evolution-beyond-independence",
    "href": "chapters/bioinformatics/models_of_dna_evolution.html#context-dependent-evolution-beyond-independence",
    "title": "48  Models of DNA Evolution",
    "section": "Context-Dependent Evolution: Beyond Independence",
    "text": "Context-Dependent Evolution: Beyond Independence\nStandard models assume that sites evolve independently, but real sequences exhibit context-dependent substitution patterns. The most striking example is the CpG dinucleotide in vertebrate genomes. Methylated cytosines in CpG contexts spontaneously deaminate to thymine at elevated rates, causing CpG depletion except in unmethylated CpG islands.\nContext-dependent models explicitly incorporate neighbor effects:\n\\[Q_{ij|context} = f(i, j, \\text{neighboring bases})\\]\nThese models can capture: - CpG effects: elevated C→T rates in CpG contexts - Codon position effects: different rates at three codon positions - RNA structure: compensatory substitutions maintaining base pairs - Protein structure: correlated changes maintaining interactions\nWhile context-dependent models provide more realistic descriptions of sequence evolution, they require substantially more parameters and computational resources. The trade-off between model realism and practical applicability remains an active area of research.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Models of DNA Evolution</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/models_of_dna_evolution.html#model-selection-choosing-appropriate-complexity",
    "href": "chapters/bioinformatics/models_of_dna_evolution.html#model-selection-choosing-appropriate-complexity",
    "title": "48  Models of DNA Evolution",
    "section": "Model Selection: Choosing Appropriate Complexity",
    "text": "Model Selection: Choosing Appropriate Complexity\nWith numerous models available, selecting appropriate complexity for specific datasets becomes crucial. Overly simple models miss important evolutionary features, while overly complex models overfit data and provide unreliable estimates.\nStatistical model selection uses information criteria balancing fit and complexity:\nAkaike Information Criterion (AIC): \\[AIC = -2\\ln L + 2k\\]\nBayesian Information Criterion (BIC): \\[BIC = -2\\ln L + k\\ln n\\]\nwhere L is the maximum likelihood, k is the number of parameters, and n is the sample size. Lower values indicate better models, with penalties for additional parameters preventing overfitting.\nLikelihood ratio tests provide statistical assessment when models are nested: \\[LRT = 2(\\ln L_{complex} - \\ln L_{simple})\\]\nUnder the null hypothesis (simple model adequate), LRT follows a chi-square distribution with degrees of freedom equal to the parameter difference.\nHierarchical likelihood ratio tests can systematically evaluate model features: 1. Test base frequency equality (JC69 vs F81) 2. Test transition/transversion equality (F81 vs HKY85) 3. Test rate homogeneity (HKY85 vs HKY85+G) 4. Test for invariant sites (HKY85+G vs HKY85+I+G)\nThis systematic approach identifies necessary model features while avoiding unnecessary complexity.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Models of DNA Evolution</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/models_of_dna_evolution.html#practical-applications-from-theory-to-practice",
    "href": "chapters/bioinformatics/models_of_dna_evolution.html#practical-applications-from-theory-to-practice",
    "title": "48  Models of DNA Evolution",
    "section": "Practical Applications: From Theory to Practice",
    "text": "Practical Applications: From Theory to Practice\nEvolutionary models enable diverse applications in biology and medicine:\nPhylogenetic Reconstruction: Models provide the statistical framework for inferring evolutionary relationships. Maximum likelihood and Bayesian methods use substitution models to evaluate alternative trees, identifying those best explaining observed sequences. Accurate models are essential for resolving difficult phylogenetic questions, such as rapid radiations or ancient divergences.\nMolecular Dating: By relating sequence divergence to time, models enable estimation of divergence times. The molecular clock hypothesis—constant substitution rates over time—provides the simplest framework, though relaxed clock models accommodate rate variation among lineages. Calibration using fossil evidence or geological events converts genetic distances to absolute times.\nPositive Selection Detection: Codon models identify genes and sites under positive selection by comparing non-synonymous and synonymous substitution rates. This approach has revealed adaptive evolution in immune system genes, reproductive proteins, and host-pathogen interactions. Site-specific models can pinpoint individual amino acids under selection, guiding functional studies.\nAncestral Sequence Reconstruction: Models enable probabilistic reconstruction of ancestral sequences at internal tree nodes. This approach has been used to resurrect ancient proteins, study evolutionary trajectories, and understand the origins of novel functions. The accuracy of reconstruction depends critically on model appropriateness.\nEpidemiological Tracking: Fast-evolving viruses like influenza and HIV require sophisticated models accounting for rapid evolution and strong selection. Real-time phylogenetics tracks viral spread, identifies transmission chains, and predicts future evolution. Model-based analyses inform public health decisions and vaccine design.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Models of DNA Evolution</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/models_of_dna_evolution.html#limitations-and-future-directions",
    "href": "chapters/bioinformatics/models_of_dna_evolution.html#limitations-and-future-directions",
    "title": "48  Models of DNA Evolution",
    "section": "Limitations and Future Directions",
    "text": "Limitations and Future Directions\nDespite their utility, current models have important limitations:\nStationarity Assumption: Most models assume constant nucleotide frequencies over time, but compositional evolution occurs in many lineages. Non-stationary models allowing frequency changes are computationally challenging but necessary for some datasets.\nSite Independence: The assumption that sites evolve independently ignores structural and functional constraints creating correlated evolution. Incorporating dependency requires exponentially more parameters, limiting practical applications.\nSelection Modeling: Most models assume neutral evolution or simple selection schemes. Real selection varies across sites, through time, and among lineages in complex ways not captured by current models.\nInsertion-Deletion Evolution: Standard substitution models ignore insertions and deletions (indels), though these contribute substantially to sequence divergence. Integrated models of substitution and indel evolution remain computationally challenging.\nFuture developments will likely focus on: - Machine learning approaches to model selection and parameter estimation - Integration of structural and functional information - Improved computational methods for complex models - Models tailored to specific biological questions - Real-time phylogenetics for pathogen surveillance",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Models of DNA Evolution</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/models_of_dna_evolution.html#conclusion",
    "href": "chapters/bioinformatics/models_of_dna_evolution.html#conclusion",
    "title": "48  Models of DNA Evolution",
    "section": "Conclusion",
    "text": "Conclusion\nModels of DNA evolution provide the mathematical foundation for understanding how genetic sequences change over time. From the simple elegance of the Jukes-Cantor model to the biological realism of context-dependent models, this framework enables us to extract evolutionary information from sequence data.\nThe progression from simple to complex models reflects our growing understanding of molecular evolution’s intricacies. Each model captures different aspects of the substitution process: equal rates (JC69), transition/transversion bias (K80), compositional constraints (F81), combined features (HKY85, GTR), rate heterogeneity (gamma), functional constraints (invariant sites), and the genetic code (codon models).\nModel selection remains crucial for balancing biological realism with statistical reliability. Overly simple models miss important evolutionary features, while overly complex models overfit limited data. Information criteria and likelihood ratio tests guide appropriate model choice for specific datasets and questions.\nThe practical impact of evolutionary models extends throughout biology and medicine. They enable phylogenetic reconstruction, molecular dating, selection detection, ancestral reconstruction, and epidemiological tracking. As sequencing technologies generate ever-larger datasets and computational methods advance, evolutionary models will continue evolving to meet new challenges.\nUnderstanding these models—their assumptions, strengths, and limitations—is essential for anyone working with molecular sequence data. They transform raw sequence comparisons into evolutionary insights, revealing the historical processes that shaped the diversity of life. As we develop more sophisticated models capturing additional biological realism, we deepen our understanding of molecular evolution and enhance our ability to use genomic data for practical applications.\nThe future of evolutionary modeling lies in integrating multiple data types, incorporating structural and functional constraints, and developing computationally efficient methods for increasingly complex models. These advances will enable more accurate phylogenetic inference, better understanding of adaptive evolution, and improved predictions for medical and biotechnological applications. The mathematical frameworks developed for modeling DNA evolution will continue to evolve, driven by biological discoveries and computational innovations, maintaining their central role in understanding life’s molecular history.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Models of DNA Evolution</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html",
    "href": "chapters/bioinformatics/phylogenetics.html",
    "title": "49  Phylogenetics",
    "section": "",
    "text": "50 Part I: Recursion and Tree Algorithms in Phylogenetics\nPhylogenetics represents one of the most fundamental disciplines in modern biology, providing the mathematical and computational framework for understanding the evolutionary relationships among organisms. At its core, phylogenetics seeks to reconstruct the tree of life—the branching pattern of descent that connects all living things through their shared evolutionary history. This field combines elements from evolutionary biology, mathematics, statistics, and computer science to infer these relationships from molecular and morphological data. The importance of phylogenetic analysis extends far beyond academic curiosity; it plays crucial roles in understanding disease evolution, tracking pandemic spread, conservation biology, drug discovery, and our fundamental understanding of life’s diversity.\nThe modern era of phylogenetics began with the molecular revolution, when scientists gained the ability to sequence DNA and proteins. This molecular data provides a rich source of information about evolutionary relationships, as mutations accumulate over time in a roughly clock-like fashion. By comparing sequences from different organisms, we can infer their evolutionary relationships and estimate the time since they diverged from common ancestors. However, this inference is not straightforward—it requires sophisticated mathematical models and computational algorithms to account for the complex processes of molecular evolution.\nPhylogenetic trees, also known as evolutionary trees or cladograms, are the primary tools for representing evolutionary relationships. These trees consist of nodes connected by branches, where the leaves (terminal nodes) represent contemporary species or sequences, internal nodes represent hypothetical common ancestors, and branches represent the evolutionary paths connecting them. The topology of the tree—its branching pattern—encodes the relationships among organisms, while branch lengths often represent evolutionary time or the amount of evolutionary change.\nThe computational challenges in phylogenetics are substantial. For even a modest number of species, the number of possible tree topologies is astronomical. For instance, with just 10 species, there are over 34 million possible unrooted trees. This number grows super-exponentially with the number of taxa, making exhaustive searches impossible for datasets of realistic size. Consequently, phylogenetic inference relies heavily on clever algorithms, heuristic searches, and statistical approaches to navigate this vast space of possible trees.\nThis lecture note explores two fundamental aspects of computational phylogenetics. First, we examine the principle of recursion and its application to tree algorithms, demonstrating how recursive thinking naturally aligns with the hierarchical structure of phylogenetic trees. Second, we delve into the Felsenstein pruning algorithm, a cornerstone method for calculating the likelihood of sequence data given a phylogenetic tree. Together, these topics provide essential foundations for understanding modern phylogenetic analysis.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#the-principle-of-recursion",
    "href": "chapters/bioinformatics/phylogenetics.html#the-principle-of-recursion",
    "title": "49  Phylogenetics",
    "section": "The Principle of Recursion",
    "text": "The Principle of Recursion\nRecursion is a programming paradigm where a function calls itself to solve smaller instances of the same problem. This approach is particularly elegant for problems that exhibit self-similar structure, where the solution to a larger problem can be expressed in terms of solutions to smaller, similar subproblems. In phylogenetics, trees are inherently recursive structures—each subtree is itself a complete tree with the same structural properties as the whole. This makes recursion the natural choice for implementing tree algorithms.\nThe key to understanding recursion lies in recognizing two essential components: the base case and the recursive case. The base case defines the simplest version of the problem that can be solved directly without further recursion. The recursive case breaks down the problem into smaller subproblems and combines their solutions. Without a proper base case, recursion would continue indefinitely, leading to stack overflow errors.\nIn Python, recursive functions follow a standard pattern:\ndef recursive_function(input):\n    # Base case: check if we've reached the simplest form\n    if is_base_case(input):\n        return base_case_solution(input)\n\n    # Recursive case: break down the problem\n    smaller_problems = decompose(input)\n\n    # Solve smaller problems recursively\n    solutions = [recursive_function(problem) for problem in smaller_problems]\n\n    # Combine solutions\n    return combine_solutions(solutions)\nThis pattern appears repeatedly in phylogenetic algorithms. Trees naturally decompose into subtrees rooted at child nodes, making recursive algorithms both intuitive and efficient for tree-based computations.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#tree-representation-in-python",
    "href": "chapters/bioinformatics/phylogenetics.html#tree-representation-in-python",
    "title": "49  Phylogenetics",
    "section": "Tree Representation in Python",
    "text": "Tree Representation in Python\nBefore implementing recursive algorithms on phylogenetic trees, we need a suitable data structure to represent trees in Python. A simple yet effective approach uses nested dictionaries or custom classes. Here’s a basic tree node class that supports phylogenetic computations:\nclass TreeNode:\n    def __init__(self, name=None, branch_length=0.0):\n        self.name = name  # Species name for leaves, None for internal nodes\n        self.branch_length = branch_length  # Length of branch leading to this node\n        self.children = []  # List of child nodes\n        self.parent = None  # Reference to parent node\n\n    def add_child(self, child):\n        \"\"\"Add a child node to this node\"\"\"\n        self.children.append(child)\n        child.parent = self\n\n    def is_leaf(self):\n        \"\"\"Check if this node is a leaf (has no children)\"\"\"\n        return len(self.children) == 0\n\n    def is_root(self):\n        \"\"\"Check if this node is the root (has no parent)\"\"\"\n        return self.parent is None\nThis representation allows us to build trees programmatically and traverse them using recursive algorithms. For example, we can construct a simple phylogenetic tree:\n# Create a simple tree: ((A,B),C)\nroot = TreeNode()\nleft_child = TreeNode()\nright_child = TreeNode(name=\"C\", branch_length=0.5)\n\nleaf_a = TreeNode(name=\"A\", branch_length=0.3)\nleaf_b = TreeNode(name=\"B\", branch_length=0.4)\n\nroot.add_child(left_child)\nroot.add_child(right_child)\nleft_child.add_child(leaf_a)\nleft_child.add_child(leaf_b)",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#counting-leaves-a-simple-recursive-algorithm",
    "href": "chapters/bioinformatics/phylogenetics.html#counting-leaves-a-simple-recursive-algorithm",
    "title": "49  Phylogenetics",
    "section": "Counting Leaves: A Simple Recursive Algorithm",
    "text": "Counting Leaves: A Simple Recursive Algorithm\nOne of the simplest recursive algorithms on trees counts the number of leaves (terminal nodes). This algorithm demonstrates the elegance of recursive thinking for tree problems:\ndef count_leaves(node):\n    \"\"\"\n    Recursively count the number of leaves in a tree.\n\n    Parameters:\n    node: TreeNode - The root of the tree or subtree\n\n    Returns:\n    int - The number of leaves in the tree\n    \"\"\"\n    # Base case: if this is a leaf, return 1\n    if node.is_leaf():\n        return 1\n\n    # Recursive case: sum the leaves in all subtrees\n    total_leaves = 0\n    for child in node.children:\n        total_leaves += count_leaves(child)\n\n    return total_leaves\nThe algorithm works by checking if the current node is a leaf. If it is, we’ve found one leaf and return 1. Otherwise, we recursively count the leaves in each subtree and sum the results. This naturally handles trees of any size and shape.\nWe can also write this more concisely using Python’s sum function and a generator expression:\ndef count_leaves_compact(node):\n    \"\"\"Compact version of leaf counting\"\"\"\n    if node.is_leaf():\n        return 1\n    return sum(count_leaves_compact(child) for child in node.children)",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#calculating-tree-height",
    "href": "chapters/bioinformatics/phylogenetics.html#calculating-tree-height",
    "title": "49  Phylogenetics",
    "section": "Calculating Tree Height",
    "text": "Calculating Tree Height\nAnother fundamental recursive algorithm calculates the height (or depth) of a tree—the maximum distance from the root to any leaf:\ndef tree_height(node):\n    \"\"\"\n    Calculate the height of a tree (maximum depth).\n\n    Parameters:\n    node: TreeNode - The root of the tree\n\n    Returns:\n    int - The height of the tree\n    \"\"\"\n    # Base case: a leaf has height 0\n    if node.is_leaf():\n        return 0\n\n    # Recursive case: height is 1 plus maximum height of children\n    max_child_height = max(tree_height(child) for child in node.children)\n    return 1 + max_child_height\nThis algorithm demonstrates how recursion naturally handles the branching structure of trees. Each recursive call explores one branch, and the max function ensures we find the longest path from root to leaf.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#collecting-species-names",
    "href": "chapters/bioinformatics/phylogenetics.html#collecting-species-names",
    "title": "49  Phylogenetics",
    "section": "Collecting Species Names",
    "text": "Collecting Species Names\nIn phylogenetic analysis, we often need to collect all species names (leaf labels) in a tree. This is another natural application of recursion:\ndef get_species_names(node):\n    \"\"\"\n    Recursively collect all species names in a tree.\n\n    Parameters:\n    node: TreeNode - The root of the tree\n\n    Returns:\n    list - A list of all species names\n    \"\"\"\n    # Base case: if this is a leaf, return its name\n    if node.is_leaf():\n        return [node.name] if node.name else []\n\n    # Recursive case: collect names from all subtrees\n    all_names = []\n    for child in node.children:\n        all_names.extend(get_species_names(child))\n\n    return all_names",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#tree-traversal-strategies",
    "href": "chapters/bioinformatics/phylogenetics.html#tree-traversal-strategies",
    "title": "49  Phylogenetics",
    "section": "Tree Traversal Strategies",
    "text": "Tree Traversal Strategies\nTree traversal—visiting all nodes in a tree systematically—is fundamental to many phylogenetic algorithms. There are several traversal strategies, each suited to different tasks:\n\nPre-order Traversal\nIn pre-order traversal, we visit the current node before visiting its children:\ndef preorder_traversal(node, visit_function):\n    \"\"\"\n    Perform pre-order traversal of a tree.\n\n    Parameters:\n    node: TreeNode - The current node\n    visit_function: function - Function to apply to each node\n    \"\"\"\n    # Visit current node first\n    visit_function(node)\n\n    # Then visit all children\n    for child in node.children:\n        preorder_traversal(child, visit_function)\nPre-order traversal is useful when we need to process parent nodes before their children, such as when propagating information down the tree.\n\n\nPost-order Traversal\nIn post-order traversal, we visit children before the parent:\ndef postorder_traversal(node, visit_function):\n    \"\"\"\n    Perform post-order traversal of a tree.\n\n    Parameters:\n    node: TreeNode - The current node\n    visit_function: function - Function to apply to each node\n    \"\"\"\n    # Visit all children first\n    for child in node.children:\n        postorder_traversal(child, visit_function)\n\n    # Then visit current node\n    visit_function(node)\nPost-order traversal is crucial for algorithms that need to process children before parents, such as the Felsenstein pruning algorithm we’ll discuss later.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#calculating-total-branch-length",
    "href": "chapters/bioinformatics/phylogenetics.html#calculating-total-branch-length",
    "title": "49  Phylogenetics",
    "section": "Calculating Total Branch Length",
    "text": "Calculating Total Branch Length\nThe total branch length of a tree is the sum of all branch lengths, often used as a measure of the amount of evolution:\ndef total_branch_length(node):\n    \"\"\"\n    Calculate the sum of all branch lengths in a tree.\n\n    Parameters:\n    node: TreeNode - The root of the tree\n\n    Returns:\n    float - The total branch length\n    \"\"\"\n    # Start with this node's branch length (0 for root)\n    total = node.branch_length if not node.is_root() else 0\n\n    # Add branch lengths from all subtrees\n    for child in node.children:\n        total += total_branch_length(child)\n\n    return total",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#introduction-to-likelihood-based-phylogenetics",
    "href": "chapters/bioinformatics/phylogenetics.html#introduction-to-likelihood-based-phylogenetics",
    "title": "49  Phylogenetics",
    "section": "Introduction to Likelihood-Based Phylogenetics",
    "text": "Introduction to Likelihood-Based Phylogenetics\nThe Felsenstein pruning algorithm, introduced by Joseph Felsenstein in 1981, revolutionized phylogenetic inference by providing an efficient method to calculate the likelihood of observed sequence data given a phylogenetic tree and an evolutionary model. This algorithm forms the foundation of maximum likelihood and Bayesian approaches to phylogenetic inference, which are now the gold standard methods in the field.\nThe fundamental question addressed by the Felsenstein algorithm is: given a set of aligned DNA or protein sequences and a proposed phylogenetic tree (including topology and branch lengths), what is the probability of observing these sequences under a specified model of evolution? This likelihood calculation is essential for comparing different trees and finding the tree that best explains the observed data.\nThe key insight of Felsenstein’s algorithm is that the likelihood calculation, which would be computationally intractable if done naively, can be performed efficiently using dynamic programming on the tree structure. The algorithm exploits the Markov property of evolutionary models—the idea that evolution along different branches is independent given the ancestral states—to break down the calculation into manageable pieces.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#mathematical-foundation",
    "href": "chapters/bioinformatics/phylogenetics.html#mathematical-foundation",
    "title": "49  Phylogenetics",
    "section": "Mathematical Foundation",
    "text": "Mathematical Foundation\nConsider a phylogenetic tree \\(T\\) with \\(n\\) leaves (observed sequences) and a set of aligned sequences at a single site. Let the observed nucleotides at this site be denoted as \\(D = \\{d_1, d_2, ..., d_n\\}\\), where each \\(d_i \\in \\{A, C, G, T\\}\\) for DNA sequences.\nThe likelihood of the data at this site given the tree is:\n\\[L(D|T, \\theta) = \\sum_{r \\in \\{A,C,G,T\\}} \\pi_r P(D|r, T, \\theta)\\]\nwhere: - \\(\\pi_r\\) is the equilibrium frequency of nucleotide \\(r\\) at the root - \\(P(D|r, T, \\theta)\\) is the probability of observing data \\(D\\) given root state \\(r\\) - \\(\\theta\\) represents the model parameters (substitution rates, etc.)\nThe challenge is computing \\(P(D|r, T, \\theta)\\) efficiently. This requires summing over all possible internal node states, which grows exponentially with the number of internal nodes.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#the-substitution-model",
    "href": "chapters/bioinformatics/phylogenetics.html#the-substitution-model",
    "title": "49  Phylogenetics",
    "section": "The Substitution Model",
    "text": "The Substitution Model\nThe Felsenstein algorithm requires a model of sequence evolution. The simplest model is the Jukes-Cantor (JC69) model, which assumes: - Equal base frequencies: \\(\\pi_A = \\pi_C = \\pi_G = \\pi_T = 0.25\\) - Equal substitution rates between all pairs of nucleotides\nUnder the JC69 model, the probability of observing nucleotide \\(j\\) given ancestral nucleotide \\(i\\) after evolutionary time \\(t\\) is:\n\\[P_{ij}(t) = \\begin{cases}\n\\frac{1}{4} + \\frac{3}{4}e^{-\\frac{4\\mu t}{3}} & \\text{if } i = j \\\\\n\\frac{1}{4} - \\frac{1}{4}e^{-\\frac{4\\mu t}{3}} & \\text{if } i \\neq j\n\\end{cases}\\]\nwhere \\(\\mu\\) is the substitution rate.\nMore complex models like the General Time Reversible (GTR) model allow for different substitution rates and base frequencies, but the principle remains the same.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#the-pruning-algorithm",
    "href": "chapters/bioinformatics/phylogenetics.html#the-pruning-algorithm",
    "title": "49  Phylogenetics",
    "section": "The Pruning Algorithm",
    "text": "The Pruning Algorithm\nThe Felsenstein pruning algorithm uses post-order traversal to compute partial likelihoods from the leaves up to the root. For each node and each possible state, it computes the likelihood of the subtree below that node.\nLet \\(L_k(i)\\) denote the partial likelihood at node \\(k\\) for state \\(i\\)—the probability of observing all the sequence data in the subtree rooted at \\(k\\), given that node \\(k\\) has state \\(i\\).\nFor a leaf node \\(k\\) with observed state \\(d_k\\): \\[L_k(i) = \\begin{cases}\n1 & \\text{if } i = d_k \\\\\n0 & \\text{if } i \\neq d_k\n\\end{cases}\\]\nFor an internal node \\(k\\) with children \\(m\\) and \\(n\\): \\[L_k(i) = \\left(\\sum_{j} P_{ij}(t_m) L_m(j)\\right) \\times \\left(\\sum_{j} P_{ij}(t_n) L_n(j)\\right)\\]\nwhere \\(t_m\\) and \\(t_n\\) are the branch lengths leading to children \\(m\\) and \\(n\\), respectively.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#python-implementation",
    "href": "chapters/bioinformatics/phylogenetics.html#python-implementation",
    "title": "49  Phylogenetics",
    "section": "Python Implementation",
    "text": "Python Implementation\nHere’s a complete implementation of the Felsenstein pruning algorithm:\nimport numpy as np\nfrom math import exp\n\nclass FelsensteinCalculator:\n    def __init__(self, substitution_model='JC69'):\n        \"\"\"\n        Initialize the Felsenstein likelihood calculator.\n\n        Parameters:\n        substitution_model: str - The substitution model to use\n        \"\"\"\n        self.model = substitution_model\n        self.states = ['A', 'C', 'G', 'T']\n        self.state_to_index = {state: i for i, state in enumerate(self.states)}\n\n        # Set equilibrium frequencies based on model\n        if self.model == 'JC69':\n            self.pi = np.array([0.25, 0.25, 0.25, 0.25])\n            self.mu = 1.0  # Substitution rate\n\n    def transition_probability(self, branch_length):\n        \"\"\"\n        Calculate transition probability matrix for given branch length.\n\n        Parameters:\n        branch_length: float - The branch length\n\n        Returns:\n        numpy.ndarray - 4x4 transition probability matrix\n        \"\"\"\n        if self.model == 'JC69':\n            # Jukes-Cantor transition probabilities\n            p_same = 0.25 + 0.75 * exp(-4.0 * self.mu * branch_length / 3.0)\n            p_diff = 0.25 - 0.25 * exp(-4.0 * self.mu * branch_length / 3.0)\n\n            P = np.full((4, 4), p_diff)\n            np.fill_diagonal(P, p_same)\n            return P\n        else:\n            raise NotImplementedError(f\"Model {self.model} not implemented\")\n\n    def initialize_leaf_likelihood(self, observed_state):\n        \"\"\"\n        Initialize likelihood vector for a leaf node.\n\n        Parameters:\n        observed_state: str - The observed nucleotide at this leaf\n\n        Returns:\n        numpy.ndarray - Likelihood vector\n        \"\"\"\n        likelihood = np.zeros(4)\n        if observed_state in self.state_to_index:\n            likelihood[self.state_to_index[observed_state]] = 1.0\n        else:\n            # Handle ambiguous characters or gaps\n            likelihood[:] = 1.0  # All states equally likely\n\n        return likelihood\n\n    def calculate_internal_likelihood(self, node, child_likelihoods):\n        \"\"\"\n        Calculate likelihood vector for an internal node.\n\n        Parameters:\n        node: TreeNode - The internal node\n        child_likelihoods: list - List of (child_node, likelihood_vector) tuples\n\n        Returns:\n        numpy.ndarray - Likelihood vector for this node\n        \"\"\"\n        node_likelihood = np.ones(4)\n\n        for child, child_like in child_likelihoods:\n            # Get transition probability matrix for this branch\n            P = self.transition_probability(child.branch_length)\n\n            # Calculate contribution from this child\n            contribution = P.dot(child_like)\n\n            # Multiply contributions from all children\n            node_likelihood *= contribution\n\n        return node_likelihood\n\n    def calculate_site_likelihood(self, tree_root, site_data):\n        \"\"\"\n        Calculate likelihood for a single site using Felsenstein pruning.\n\n        Parameters:\n        tree_root: TreeNode - Root of the phylogenetic tree\n        site_data: dict - Dictionary mapping species names to nucleotides\n\n        Returns:\n        float - Log-likelihood for this site\n        \"\"\"\n        # First pass: post-order traversal to compute partial likelihoods\n        likelihoods = {}\n\n        def compute_likelihood(node):\n            if node.is_leaf():\n                # Initialize leaf likelihood\n                observed = site_data.get(node.name, 'N')\n                likelihoods[node] = self.initialize_leaf_likelihood(observed)\n            else:\n                # Compute likelihood for internal node\n                child_data = []\n                for child in node.children:\n                    compute_likelihood(child)  # Ensure child is processed\n                    child_data.append((child, likelihoods[child]))\n\n                likelihoods[node] = self.calculate_internal_likelihood(\n                    node, child_data\n                )\n\n        # Compute all partial likelihoods\n        compute_likelihood(tree_root)\n\n        # Calculate final likelihood at root\n        root_likelihood = likelihoods[tree_root]\n        site_likelihood = np.dot(self.pi, root_likelihood)\n\n        # Return log-likelihood to avoid underflow\n        return np.log(site_likelihood) if site_likelihood &gt; 0 else float('-inf')\n\n    def calculate_total_likelihood(self, tree_root, alignment):\n        \"\"\"\n        Calculate total likelihood for an alignment.\n\n        Parameters:\n        tree_root: TreeNode - Root of the phylogenetic tree\n        alignment: dict - Dictionary mapping species names to sequences\n\n        Returns:\n        float - Total log-likelihood\n        \"\"\"\n        # Get sequence length (assume all sequences same length)\n        seq_length = len(next(iter(alignment.values())))\n\n        total_log_likelihood = 0.0\n\n        # Calculate likelihood for each site\n        for site_idx in range(seq_length):\n            # Extract nucleotides at this site\n            site_data = {\n                species: seq[site_idx]\n                for species, seq in alignment.items()\n            }\n\n            # Add log-likelihood for this site\n            site_log_like = self.calculate_site_likelihood(tree_root, site_data)\n            total_log_likelihood += site_log_like\n\n        return total_log_likelihood",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#handling-numerical-underflow",
    "href": "chapters/bioinformatics/phylogenetics.html#handling-numerical-underflow",
    "title": "49  Phylogenetics",
    "section": "Handling Numerical Underflow",
    "text": "Handling Numerical Underflow\nOne practical challenge in implementing the Felsenstein algorithm is numerical underflow. As we multiply many small probabilities together, the values can become too small to represent in floating-point arithmetic. There are several strategies to address this:\n\nScaling Method\nThe scaling method maintains the relative proportions of partial likelihoods while keeping values in a reasonable range:\ndef calculate_internal_likelihood_scaled(self, node, child_likelihoods):\n    \"\"\"\n    Calculate likelihood with scaling to prevent underflow.\n    \"\"\"\n    node_likelihood = np.ones(4)\n    scale_factors = []\n\n    for child, child_like in child_likelihoods:\n        P = self.transition_probability(child.branch_length)\n        contribution = P.dot(child_like)\n\n        # Scale to prevent underflow\n        max_contrib = np.max(contribution)\n        if max_contrib &gt; 0:\n            contribution /= max_contrib\n            scale_factors.append(np.log(max_contrib))\n\n        node_likelihood *= contribution\n\n    # Store scale factor for this node\n    self.scale_factors[node] = sum(scale_factors)\n\n    return node_likelihood\n\n\nLog-Likelihood Method\nAn alternative is to work entirely in log space:\ndef calculate_log_likelihood(self, node, child_log_likelihoods):\n    \"\"\"\n    Calculate log-likelihood to avoid underflow.\n    \"\"\"\n    log_likelihood = np.zeros(4)\n\n    for state_parent in range(4):\n        log_sum = 0.0\n\n        for child, child_log_like in child_log_likelihoods:\n            P = self.transition_probability(child.branch_length)\n\n            # Use log-sum-exp trick for numerical stability\n            terms = []\n            for state_child in range(4):\n                if P[state_parent, state_child] &gt; 0:\n                    terms.append(\n                        np.log(P[state_parent, state_child]) +\n                        child_log_like[state_child]\n                    )\n\n            log_sum += self.log_sum_exp(terms)\n\n        log_likelihood[state_parent] = log_sum\n\n    return log_likelihood\n\ndef log_sum_exp(self, log_values):\n    \"\"\"\n    Compute log(sum(exp(log_values))) in a numerically stable way.\n    \"\"\"\n    max_val = max(log_values)\n    return max_val + np.log(sum(exp(val - max_val) for val in log_values))",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#practical-example-analyzing-a-small-alignment",
    "href": "chapters/bioinformatics/phylogenetics.html#practical-example-analyzing-a-small-alignment",
    "title": "49  Phylogenetics",
    "section": "Practical Example: Analyzing a Small Alignment",
    "text": "Practical Example: Analyzing a Small Alignment\nLet’s work through a complete example of using the Felsenstein algorithm on a small dataset:\ndef example_analysis():\n    \"\"\"\n    Complete example of phylogenetic likelihood calculation.\n    \"\"\"\n    # Create a simple tree: ((Human, Chimp), (Mouse, Rat))\n    root = TreeNode()\n    primates = TreeNode(branch_length=0.1)\n    rodents = TreeNode(branch_length=0.15)\n\n    human = TreeNode(name=\"Human\", branch_length=0.05)\n    chimp = TreeNode(name=\"Chimp\", branch_length=0.06)\n    mouse = TreeNode(name=\"Mouse\", branch_length=0.2)\n    rat = TreeNode(name=\"Rat\", branch_length=0.18)\n\n    root.add_child(primates)\n    root.add_child(rodents)\n    primates.add_child(human)\n    primates.add_child(chimp)\n    rodents.add_child(mouse)\n    rodents.add_child(rat)\n\n    # Sample alignment (5 sites)\n    alignment = {\n        \"Human\": \"ACGTG\",\n        \"Chimp\": \"ACGTA\",\n        \"Mouse\": \"CCATA\",\n        \"Rat\":   \"CCGTA\"\n    }\n\n    # Calculate likelihood\n    calc = FelsensteinCalculator()\n    log_likelihood = calc.calculate_total_likelihood(root, alignment)\n\n    print(f\"Tree: ((Human:{human.branch_length}, Chimp:{chimp.branch_length}), \"\n          f\"(Mouse:{mouse.branch_length}, Rat:{rat.branch_length}))\")\n    print(f\"Alignment length: {len(alignment['Human'])} sites\")\n    print(f\"Log-likelihood: {log_likelihood:.4f}\")\n    print(f\"Likelihood: {exp(log_likelihood):.6e}\")\n\n    # Analyze each site\n    for site_idx in range(len(alignment['Human'])):\n        site_data = {\n            species: seq[site_idx]\n            for species, seq in alignment.items()\n        }\n        site_log_like = calc.calculate_site_likelihood(root, site_data)\n        print(f\"Site {site_idx + 1}: {' '.join(site_data.values())} -&gt; \"\n              f\"Log-likelihood: {site_log_like:.4f}\")\n\n    return root, alignment, log_likelihood\n\n# Run the example\ntree, align, log_like = example_analysis()",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#optimizing-branch-lengths",
    "href": "chapters/bioinformatics/phylogenetics.html#optimizing-branch-lengths",
    "title": "49  Phylogenetics",
    "section": "Optimizing Branch Lengths",
    "text": "Optimizing Branch Lengths\nThe Felsenstein algorithm is often used within an optimization framework to find the branch lengths that maximize the likelihood:\nfrom scipy.optimize import minimize\n\ndef optimize_branch_lengths(tree_root, alignment, initial_lengths=None):\n    \"\"\"\n    Optimize branch lengths to maximize likelihood.\n\n    Parameters:\n    tree_root: TreeNode - Root of the tree\n    alignment: dict - Sequence alignment\n    initial_lengths: list - Initial branch length values\n\n    Returns:\n    list - Optimized branch lengths\n    \"\"\"\n    calc = FelsensteinCalculator()\n\n    # Collect all branches\n    branches = []\n    def collect_branches(node):\n        if not node.is_root():\n            branches.append(node)\n        for child in node.children:\n            collect_branches(child)\n\n    collect_branches(tree_root)\n\n    # Set initial lengths\n    if initial_lengths is None:\n        initial_lengths = [0.1] * len(branches)\n\n    def objective(lengths):\n        # Update branch lengths\n        for branch, length in zip(branches, lengths):\n            branch.branch_length = length\n\n        # Calculate negative log-likelihood (for minimization)\n        log_like = calc.calculate_total_likelihood(tree_root, alignment)\n        return -log_like\n\n    # Optimize with bounds (lengths must be positive)\n    result = minimize(\n        objective,\n        initial_lengths,\n        method='L-BFGS-B',\n        bounds=[(1e-6, 10.0)] * len(branches)\n    )\n\n    # Update tree with optimized lengths\n    for branch, length in zip(branches, result.x):\n        branch.branch_length = length\n\n    return result.x, -result.fun",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#applications-and-extensions",
    "href": "chapters/bioinformatics/phylogenetics.html#applications-and-extensions",
    "title": "49  Phylogenetics",
    "section": "Applications and Extensions",
    "text": "Applications and Extensions\nThe Felsenstein algorithm has numerous applications and extensions in modern phylogenetics:\n\nRate Heterogeneity\nReal sequences evolve at different rates at different sites. The gamma distribution is commonly used to model rate heterogeneity across sites, allowing some positions to evolve faster or slower than others. This is implemented by calculating the likelihood under different rate categories and averaging them according to the gamma distribution.\n\n\nAncestral State Reconstruction\nThe partial likelihoods computed during the Felsenstein algorithm can be used to reconstruct the most likely ancestral sequences at internal nodes of the tree. This involves a second pass through the tree, using the partial likelihoods to compute posterior probabilities of each state at each internal node.\n\n\nModel Selection\nThe Felsenstein algorithm enables comparison of different substitution models through likelihood ratio tests or information criteria like AIC and BIC. By calculating likelihoods under different models, we can determine which model best explains the observed data while accounting for model complexity.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#computational-complexity-and-performance",
    "href": "chapters/bioinformatics/phylogenetics.html#computational-complexity-and-performance",
    "title": "49  Phylogenetics",
    "section": "Computational Complexity and Performance",
    "text": "Computational Complexity and Performance\nThe time complexity of the Felsenstein algorithm is \\(O(n \\times m \\times s^2)\\), where: - \\(n\\) is the number of nodes in the tree - \\(m\\) is the sequence length - \\(s\\) is the number of states (4 for DNA, 20 for proteins)\nThe space complexity is \\(O(n \\times s)\\) for storing partial likelihoods at each node.\nFor large-scale analyses, several optimizations are crucial: - Site pattern compression: Many sites may have identical patterns across species, so we can compute the likelihood once per pattern and weight by frequency - Parallelization: Sites are independent, allowing parallel computation across multiple processors - Vectorization: Modern CPUs can process multiple likelihood calculations simultaneously using SIMD instructions - GPU acceleration: The matrix operations in the Felsenstein algorithm are well-suited for GPU computation",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#integration-with-tree-search",
    "href": "chapters/bioinformatics/phylogenetics.html#integration-with-tree-search",
    "title": "49  Phylogenetics",
    "section": "Integration with Tree Search",
    "text": "Integration with Tree Search\nThe Felsenstein algorithm is typically embedded within tree search algorithms that explore the space of possible topologies:\ndef hill_climbing_tree_search(alignment, starting_tree):\n    \"\"\"\n    Simple hill-climbing search for the best tree.\n    \"\"\"\n    calc = FelsensteinCalculator()\n    current_tree = starting_tree\n    current_likelihood = calc.calculate_total_likelihood(current_tree, alignment)\n\n    improved = True\n    while improved:\n        improved = False\n\n        # Try all possible NNI moves\n        for move in generate_nni_moves(current_tree):\n            # Apply move\n            modified_tree = apply_nni(current_tree, move)\n\n            # Optimize branch lengths\n            optimized_lengths, new_likelihood = optimize_branch_lengths(\n                modified_tree, alignment\n            )\n\n            # Accept if better\n            if new_likelihood &gt; current_likelihood:\n                current_tree = modified_tree\n                current_likelihood = new_likelihood\n                improved = True\n                break\n\n    return current_tree, current_likelihood",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/bioinformatics/phylogenetics.html#conclusion",
    "href": "chapters/bioinformatics/phylogenetics.html#conclusion",
    "title": "49  Phylogenetics",
    "section": "Conclusion",
    "text": "Conclusion\nThe combination of recursive algorithms and the Felsenstein pruning algorithm provides a powerful framework for phylogenetic analysis. Recursion naturally captures the hierarchical structure of evolutionary trees, enabling elegant and efficient implementations of tree algorithms. The Felsenstein algorithm, by efficiently calculating likelihoods through dynamic programming on trees, makes statistical phylogenetic inference computationally feasible even for large datasets.\nThese methods have revolutionized our understanding of evolution, from reconstructing the tree of life to tracking disease outbreaks. The recursive algorithms we explored—from simple leaf counting to complex traversal strategies—demonstrate how recursive thinking aligns perfectly with the hierarchical nature of phylogenetic trees. The Felsenstein algorithm showcases how clever algorithmic design can make seemingly intractable calculations feasible, enabling us to extract evolutionary signal from molecular sequences.\nThe principles covered in this lecture—recursive thinking for tree problems and likelihood-based inference—extend beyond phylogenetics to many areas of computational biology and computer science. Understanding these concepts provides a foundation for tackling complex problems involving hierarchical data and probabilistic inference. As sequencing technologies continue to advance and datasets grow ever larger, these fundamental algorithms remain at the heart of phylogenetic analysis, adapted and extended to meet new challenges but retaining their essential elegance and power.\nThe beauty of phylogenetic algorithms lies in their ability to reveal the hidden history written in DNA and proteins. Through recursive algorithms and statistical models like the Felsenstein algorithm, we can read this molecular record and understand the processes that have shaped life on Earth over billions of years. This synthesis of mathematics, computer science, and biology exemplifies the power of interdisciplinary approaches in modern science, providing tools that continue to yield new insights into the nature and history of life.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Phylogenetics</span>"
    ]
  },
  {
    "objectID": "chapters/web/aardwark_seqdist/index.html",
    "href": "chapters/web/aardwark_seqdist/index.html",
    "title": "50  Aardvark?",
    "section": "",
    "text": "Build a FASTA file\n“What is an Aardvark?” You may have bugged you for a while. It might have kept you up at night. You might have been twisting and turning due to the mystery that is the Aardvark. But fear not. Today, you will attempt to answer this exact question. Or at least a part of the question from a phylogenetic perspective. In the process, hopefully, you will learn something about phylogeny. Let’s start with a more straightforward question. “What does the Aardvark look like?”, Figure 50.1.\nThe Aardvark is a nocturnal mammal found in Africa, with the size of a big rottweiler. The long ears and face shape (not considering the nose) make you wonder if it is the long-lost cousin of the kangaroo. It mainly feeds on ants and termites, which might change your mind in regards to its heritage. It could be a type of anteater. Looking at the nose and considering that in Afrikaans, ‘erdvark’ means ‘earth pig’ or ‘ground pig,’ you might change your mind once again and consider it a type of pig. The Aardvark is truly a mystery. So, let’s solve the mystery of the Aardvark’s evolutionary relation to these reference animals! Feel free to look up cute pictures of these animals while you work on the exercises. I have collected a list of animals that I think of when I look at the Aardvark. Have the animals in Figure 50.2 in mind when you do the exercises.\nTo uncover the mystery of the Aardvark and its relation to the reference animals, we will be looking at the evolutionary distance between the animals listed above and the Aardvark. To do so, we need to align sequences from each of the animals. Not any gene will do. We need a highly conserved gene found in all animals, that can represent to long evolutionary distances in question.\nI suggest the COI (cytochrome c oxidase I) gene (also known as the COX1 gene). The COI gene is one of the most popular phylogenetic markers for evaluating evolutionary relationships. It is found in nearly all aerobic eukaryotes, where it encodes Cytochrome C Oxidase subunit 1, a protein involved in mitochondrial respiration.\nCOI is a part of the mitochondrial DNA, which has the added benefit that the gene is inherited from the mother and that it does not undergo recombination. Recombination would make different parts of the aligned sequences follow seperate paths through the generations. Each segment of the alignment would then have its own tree, and we would end up modeling an avarage of many trees with a model that assumes a single tree for the entire sequence. This is obviously not good, but think about how it might bias the resulting phylogeny.\nWe are not the first to think this specific gene would be neat to use for phylogeny. COI has been used for DNA barcoding, which is a method for identifying and classifying species based on their genetics. This also means that this gene has been sequenced for a lot of species, making our job more manageable. It means that we do not need to fly off to some jungle, catch animals, extract DNA, create primers for the COI gene, and sequence each one. We simply look them up in the online database.\nTo investigate the mystery of the Aardvark, you need to collect the sequences of the COI genes of all the reference animals in a way that makes it possible to compare all the gene sequences. A common way to do so is to use FASTA files. FASTA is a file format used for sequences of nucleotides or amino acids that is used by almost all bioinformatic software tools. A FASTA file can contain multiple sequences as long as they are formatted as follows: First, a line starting with “&gt;” followed by an ID and a description of the sequence. This is known as the “header” and is one line only The following lines are the sequence. This can take up as many lines as needed. The sequence is usually split across lines with 60-80 characters on each line. The example below has 70 characters per line. To start a new sequence in the file, simply add a new header, followed by a sequence. Like this:",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Aardvark?</span>"
    ]
  },
  {
    "objectID": "chapters/web/aardwark_seqdist/index.html#build-a-fasta-file",
    "href": "chapters/web/aardwark_seqdist/index.html#build-a-fasta-file",
    "title": "50  Aardvark?",
    "section": "",
    "text": "&gt; ID1 some kind of description\nATGTCTTCTATTAACAGCTCTGAATCGCTTGCTGCTTCGGGAGGAAAGCCTTCTGTTTCCCACGAGTCCT\nTGCCCTATAAAACTGTCACCTACTCCGGAGAAGGCAATGAGTATGTAATTATTGACAACAAAAAATACTT\nGAGGCACGAGTTGATGGCTGCCTTCGGTGGTACCTTCAATCCTGGTTTGGCACC\n&gt; ID2 some other kind of description\nATGTCTTCTATTAACAGCTCTGAATCGCTTGCTGCTTCGGGAGGAAAGCCTTCTGTTTCCCACGAGTCCT\nTGCCCTATAAAACTGTCACCTACTCCGGAGAAGGCAATGAGTATGTAATTATTGACAACAAAAAATACTT\nGAGGCACGAGTTGATGGCTGCCTTCGGTGGTACCTTCAATCCTGGTTTGGCACCCTTTCCTAAGCATCAG\nTTTGGTAACGCTTCTGCCCTAGGTATAGCAGCATTCGCCTTACCGCTTTTAGTGTTGGGCTTGTATAATT\nTGCAAGCCAAAGACATTACAATTCCAAATATGATTGTTGGTTTATGTTTCTTCTACGGTGGTCTTTGTCA\nATTCTTATCTGGACTCTGGGAAATGGTCATGGGAAACACCTTTGCTGCCACTTCCT\n\nExercise 50-1\nNow it is your turn! Make a FASTA file containing the nucleotide sequence of the COI (COX1) gene of all the reference animals and the Aardvark. Use the file “animals.fasta” to insert the nucleotide sequences. You can find the nucleotide sequences in the NCBI Nucleotide database. To search for genes, select “Gene” instead of nucleotide from the drop-down menu. If you where looking for the PRDM9 gene in humans you would, you search like this:\nPRDM9[Gene Name] AND Homo sapiens[Orgn]\nThe [Gene Name] and [Orgn] tags tells the database that “PRDM9” should be interpreted as a gene name, and “Homo sapiens” as an organism. This greatly limits your search and excludes database entries where the same words appear in other contexts. Have a quick look at the complete list of search terms available.\nYou are looking for COX1 and Orycteropus afer is the Latin name for Aardvark. This guides you to the page containing the database information for the COI gene for the specific organism (animal). You can find the gene sequence in a fasta format in the section “NCBI Reference Sequences (RefSeq).” Copy the gene sequence and paste it into the animals.fasta file under the appropriate header. To help you along, here are the Latin names of the animals:\n\n\n\nSpecies\nLatin name\n\n\n\n\nAardvark\nOrycteropus afer\n\n\nGrey Kangaroo\nMacropus giganteus\n\n\nGolden Mole\nEremitalpa granti\n\n\nElephant Shrew\nMacroscelides flavicaudatus\n\n\nRed River Hog\nPotamochoerus porcus\n\n\nCollared anteater\nTamandua tetradactyla\n\n\nAfrican Elephant\nLoxodonta africana\n\n\n\nWe would like to find an animal that can serve as “outgroup” in our analysis. An outgroup is a sequence so distantly related animal to all your other sequences (your “ingroup”) that you can safely assume that all your ingroup sequences find a common ancestor before before they find one with the outgroup sequence (Figure 50.3). The point where the outgroup attaches to the ingroup tree must then be the common ancestor of your ingroup sequences. Since all your subject animals are all live-bearing mammals, you can use the platypus (a monotreme)(Ornithorhynchus anatinus) as the outgroup and add it to your fasta-file.\n\n\n\n\n\n\nFigure 50.3: Ingroup and outgroup in a phylogeny",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Aardvark?</span>"
    ]
  },
  {
    "objectID": "chapters/web/aardwark_seqdist/index.html#one-click-analysis",
    "href": "chapters/web/aardwark_seqdist/index.html#one-click-analysis",
    "title": "50  Aardvark?",
    "section": "One-click analysis",
    "text": "One-click analysis\nNow that you have created a FASTA file with the COI (COX1) gene of the Aardvark and all the reference animals, it is time to look into the evolutionary distances between them. We will be using the web tool www.phylogeny.fr for this exercise. When you do your phylogenetic analyses below, you need to screengrab/save your phylogenetic trees along the way. You should paste them into a document with notes on which models and parameters used for each tree. That way you can compare them all at the end.\n\nExercise 50-2\nGo to www.phylogeny.fr and get aquianted with the web interface. Then navigate to the “one-click” analysis under the “phylogeny analysis” tab. The “one-click” analysis runs an easy initial analysis of your sequences, automating the four steps below with default models and parameters:\n\nMultiple sequences alignment of all the genes in your newly made FASTA file.\nCuration of the aligned sequences in order to eliminate poorly aligned positions (such as trailing nucleotides in cases where one sequence is much longer than the others).\nBuilding of the phylogenetic tree according to evolutionary distances calculated from the curated multiple sequence alignment.\nVisualization of the tree.\n\n\n\nExercise 50-3\nNow that you have navigated to the “one-click” analysis page, you can upload the FASTA file you created in the previous task. Then click submit. The result is a phylogenetic tree describing the evolutionary distances between COI genes of the different animals. Take a good look at it. Does it look anything like what you expected?\n\n\nExercise 50-4\nIf you want to make the tree easier to interpret or to highlight specific relations, you can change the visualization without changing the information in the tree. One way is to reroot the tree. Let’s try it out. First, click on the “Reroot (outgroup)” button below the tree under the section “Select an action and click leaf or internal branch.” Then click on the name “Platypus” and wait for the tree to re-render. Now, look at it again. You can also manipulate the visualization of the tree by, instead of clicking “reroot,” you can click on “Flip” or “Swap.”\n\nWhat does Flip do?\nWhat does Swap do?\nWhat does Reroot do?\n\n\n\nExercise 50-5\nYou have played around with the tree a bit, and now you are ready address what it tells you. Start by clicking on “Reset (cancel all changes)” in the section “Select an action” and follow up by rerooting the tree with Platypus as the outgroup. Now answer the following questions:\n\nHow much does re-rooting change the tree?\nHow many terminal nodes (leaves) and how many internal nodes are there in the tree?\nWhich node represents the most recent common ancestor between the Aardvark and the African Elephant? Which one is the common ancestor between the Aardvark and the Elephant Shrew? Does this hold no matter whether you forced the platypus as the outgroup?\nWhich of the reference animals are closest related to the Aardvark according to this model? Does the tree look like you expected?\n\n\n\nDifferent 50-6\n\n\nExercise 50-7\nThe “one-click” analysis uses the HKY85 substitution model to calculate the phylogenetic distances, but you can choose other models as well. Here is a rundown of the most important ones:\nThe Jukes-Cantor model is one of the simplest nucleotide substitution models. It assumes that all types of nucleotide substitutions (transitions and transversions) occur at an equal rate, meaning that there’s a single rate parameter for all types of substitutions. This is a highly simplified model and is often used as a baseline for comparison with more complex models.\nThe Kimura two-parameter model is a simple model that takes into account two types of substitutions: transitions (purine-to-purine or pyrimidine-to-pyrimidine changes) and transversions (purine-to-pyrimidine or vice versa). It assumes that transitions occur at a different rate (\\(\\alpha\\)) than transversions (\\(\\beta\\)), reflecting the fact that transitions are often more common in DNA evolution.\nThe HKY85 model is a relatively simple model that takes into account two major factors in the evolution of nucleotide sequences: transitions (purine-to-purine or pyrimidine-to-pyrimidine changes) and transversions (purine-to-pyrimidine or vice versa). It assumes that transitions and transversions occur at different rates, which makes it more biologically realistic compared to some simpler models like the Jukes-Cantor model.\nThe GTR model is a more complex and flexible model compared to HKY85. It allows for different substitution rates between all possible pairs of nucleotides, making it a highly general model. This means that it can accommodate variations in the substitution rates of all six possible types of nucleotide changes (A\\(\\leftrightarrow\\)C, A\\(\\leftrightarrow\\)G, A\\(\\leftrightarrow\\)T, C\\(\\leftrightarrow\\)G, C\\(\\leftrightarrow\\)T, G\\(\\leftrightarrow\\)T).\nThe Hamming distance is not a traditional substitution model used for phylogenetics. It is a simple method for comparing sequences of equal length, where it counts the number of positions at which two sequences differ (i.e., the number of substitutions needed to convert one sequence into another). It does not consider the specific types of substitutions (transitions or transversions).\nWhat might be the reason for not alway choosing the model with the largest number of parameters?\n\n\nExercise 50-8\nLet’s explore how the evolutionary distances change when you use other models and also how the different tree construction methods make a difference. You can du this by running the steps from the “one-step” analysis, but change the model of DNA evolution to explore how this affects your results.\nYou will start at step 1: Multiple sequence alignment. You need a multiple alignments of all your sequences. Lucky for you, www.phylogeny.fr has already done that in order to make the tree. Here, the MUSCLE program has been used (MUSCLE is conceptually close to ClustalW). We will not change the alignment method. To access the multiple alignments, click on the tab “3. Alignment”. To see the curated that, click on the tab “4. Curation”. In this tab, the alignment has been curated by the program Gblocks. Gblocks has identified the portions of the alignment suitable for distance calculation and tree-building and has underlined these portions with a dark blue box. You can read more about Gblocks in the Gblocks documentation. Go read the introduction in the Gblocks documentation. What kind of positions are excluded after curation?\nDownload the curated alignment by clicking on “Cured alignment in Phylip format” under the Outputs section. This gives you the equal-length aligned sequences resulting from curation by Gblocks in a phy-file. Go read the introduction in the Gblocks documentation. What kind of positions are NOT included after curation?\n\n\nExercise 50-9\nNow you can see what happens if we use the same alignment same curation, but different evolutionary models to calculate the evolutionary distances. To help you calculate the evolutionary distances, www.phylogeny.fr has some nice options for calculating distance matrices for you. Let’s start by building phylogenetic trees using PhyML.\nPhyML (Phylogenetic Maximum Likelihood) employs a statistical approach (maximum likelihood) to estimate the most likely tree given the input alignment. To Use PhyML, navigate to the top, tap “Online Programmes,” and choose PhyML. Here you can upload your curated alignment in phylip format (the one you downloaded from the curation tab).\nPhyML now lets you choose a substitution model. A substitution model is a mathematical model that describes how genetic sequences change over time. So now you will see how these different models actually affect your tree. Try the two DNA/RNA substitution models for PhyML (HKY58 and GTR). Remember to save your tree by clicking “Tree in newick format” and save the file. These tree files can be visualised by using the program “TreeDyn” under the top tab “Online Programmes”. Do the trees formed by GTR and HKY85 look alike? What are the Aardvark’s closest relatives according to the two trees?",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Aardvark?</span>"
    ]
  },
  {
    "objectID": "chapters/web/aardwark_seqdist/index.html#explore-phylogeny---part-3",
    "href": "chapters/web/aardwark_seqdist/index.html#explore-phylogeny---part-3",
    "title": "50  Aardvark?",
    "section": "Explore phylogeny - part 3",
    "text": "Explore phylogeny - part 3\nNot only can you change the substitution model used in calculating the evolutionary distances. You can also use different tree-builders. You just tried PhyML, which uses a maximum likelihood approach. Next, try to use BioNJ. BioNJ stands for “Biased Neighbor Joining,” which is an adaptation of the neighbor-joining algorithm (not currently a part of www.phylogeny.fr). Neighbour Joining is a distance-based tree-building method, and BioNJ uses the same approach, with a slight bias, to avoid errors with particularly long branches. Try the three DNA/RNA substitution models for BioNJ (kimura, jukes-cantor, hamming ). Remember to save your trees. Do these three trees look alike? Do they look like the trees formed by PhyML? What are the Aardvark’s closest relatives according to the three trees?",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Aardvark?</span>"
    ]
  },
  {
    "objectID": "chapters/web/aardwark_seqdist/index.html#food-for-thought",
    "href": "chapters/web/aardwark_seqdist/index.html#food-for-thought",
    "title": "50  Aardvark?",
    "section": "Food for thought",
    "text": "Food for thought\nOkay, step back from the Aardvark for a moment. What about the other animals we are looking at? Are their relations always the same? Are some always grouped together while others are always grouped far apart? If you are particularly curious, try to add other types of elephants to the FASTA-file and see how other types of elephants group when doing phylogeny.\nNow, you have looked at the Aardvark and its evolutionary relations with other animals. You have tried many different ways. Are you more confident in your knowledge about the Aardvark, or are you feeling more confused than ever? What of the animals in your analysis is the closest relative of the Aardvark? Does Wikipedia agree with you? (https://en.wikipedia.org/wiki/Aardvark) (Read from the Introduction, Name, and Taxonomy). Do you trust Wikipedia? See if you can find the Aardvark ( Orycteropus afer) in NCBI’s Taxnomy Browser. What does that report its taxonomy?",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Aardvark?</span>"
    ]
  },
  {
    "objectID": "chapters/web/aardwark_seqdist/index.html#project-files",
    "href": "chapters/web/aardwark_seqdist/index.html#project-files",
    "title": "50  Aardvark?",
    "section": "Project files",
    "text": "Project files\nDownload the files you need for this project:\nhttps://munch-group.org/bioinformatics/supplementary/project_files",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Aardvark?</span>"
    ]
  },
  {
    "objectID": "chapters/web/orf_finding/index.html",
    "href": "chapters/web/orf_finding/index.html",
    "title": "51  Plasmid ORFs",
    "section": "",
    "text": "Exercise 51-1\nGene prediction in bacteria is a crucial step in understanding the genetic makeup of these microorganisms. It involves identifying the locations and boundaries of genes within bacterial genomes, essential for studying bacterial physiology and pathogenicity and developing targeted therapies. The main component of bacterial genes is the open reading frame (ORFs), which translates into protein. So, identifying ORFs is the main task in “de novo” prediction of bacterial genes. The most commonly used start codon is ATG, and less frequently, GTG and TTG. Stop codons are either one of TAA, TAG, or TGA. Although the three “reading frames” on each strand allow ORFs to overlap, this is mainly a feature of highly compacted viral genomes and not often observed in bacteria. Promoter and terminator DNA motifs provide additional information usually built into hidden Markov models or neural networks. Such models also asses the coding potential, codon usage, and length of ORFs to distinguish randomly occurring pairs of start and stop codons from those representing genes.\nA bacterial plasmid is a circular, double-stranded DNA molecule separate from the genome found in the cytoplasm of bacteria. It is relatively small, ranging from a few kilobases to several hundred kilobases in size, and replicates autonomously, passing to daughter cells at cell division. Although they are usually not essential to the bacterium’s survival, plasmids often carry genes that can confer selective advantages to the bacteria under certain conditions. Such conditions include exposure to antibacterial agents like the antibiotics used to combat infections, and some plasmids carry genes making the bacterium resistant to such drugs.\nIn this exercise, the bacterial DNA you will investigate is a plasmid extracted from an Enterococcus faecium strain to test if it is responsible for its documented resistance to vancomycin. Vancomycin is used to treat infections of bacteria strains resistant to other antibiotics. Methicillin-resistant Staphylococcus aureus (MRSA) is one such strain. Because vancomycin is considered a “last resort” antibiotic, resistance to this drug is dangerous, and it must monitored carefully.\nYou will use the NCBI’s ORF Finder tool to identify ORFs in the plasmid, and then use various online tools to narrow down the ORFs to a set of likely resistance genes. You can download a fasta file with the plasmid sequence from Brightspace.\nWhen you open up ORF Finder, start by pasting the fasta file into the “Query Sequence” field. Look at the search parameters, but do not change them yet. One parameter controls the minimum ORF length. We can change the expected genetic code, specify alternate start codons, and choose to ignore nested ORFs.\nRun ORF Finder with the default parameters.",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Plasmid ORFs</span>"
    ]
  },
  {
    "objectID": "chapters/web/orf_finding/index.html#project-files",
    "href": "chapters/web/orf_finding/index.html#project-files",
    "title": "51  Plasmid ORFs",
    "section": "Project files",
    "text": "Project files\nDownload the files you need for this project:\n \nhttps://munch-group.org/bioinformatics/supplementary/project_files",
    "crumbs": [
      "Bioinformatics",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Plasmid ORFs</span>"
    ]
  },
  {
    "objectID": "supplementary/project_files.html",
    "href": "supplementary/project_files.html",
    "title": "Project files",
    "section": "",
    "text": "Python projects\nIn each sections below, you will find download links to the files you need for each python project.",
    "crumbs": [
      "Supplementary",
      "Project files"
    ]
  },
  {
    "objectID": "supplementary/project_files.html#web-exercises",
    "href": "supplementary/project_files.html#web-exercises",
    "title": "Project files",
    "section": "Web exercises",
    "text": "Web exercises\nIn each sections below, you will find download links to the files you need for each web exercise.\n\nGWAS\nNone\n\n\nCCR5\n CCR5_CDS.fa   CCR5_CDS_delta32.fa   CCR5_mRNA.fa   CCR5_mRNA_delta32.fa \n\n\nMRSA\nNone\n\n\nAardwark\n animals.fasta \n\n\nORF finding\n Enterococcusfaeciumresistanceplasmidsequence.fa \n\n\nLong reads\n\ncontig_1.bam\ncontig_1.bam.bai\ncontig_1.fa\ncontig_1.fa.fai\n\n\nNeural networks\nNone",
    "crumbs": [
      "Supplementary",
      "Project files"
    ]
  },
  {
    "objectID": "supplementary/recordings2025.html",
    "href": "supplementary/recordings2025.html",
    "title": "Lecture recordings",
    "section": "",
    "text": "Week 1",
    "crumbs": [
      "Supplementary",
      "Lecture recordings"
    ]
  },
  {
    "objectID": "supplementary/recordings2025.html#week-1",
    "href": "supplementary/recordings2025.html#week-1",
    "title": "Lecture recordings",
    "section": "",
    "text": "Monday\nUnfortunately, the Monday recording failed :/\n\n\nWednesday\nUnfortunately, the Monday recording failed :/",
    "crumbs": [
      "Supplementary",
      "Lecture recordings"
    ]
  },
  {
    "objectID": "supplementary/recordings2025.html#week-2",
    "href": "supplementary/recordings2025.html#week-2",
    "title": "Lecture recordings",
    "section": "Week 2",
    "text": "Week 2\n\nMonday\n\n\n\nWednesday",
    "crumbs": [
      "Supplementary",
      "Lecture recordings"
    ]
  },
  {
    "objectID": "supplementary/recordings2025.html#week-3",
    "href": "supplementary/recordings2025.html#week-3",
    "title": "Lecture recordings",
    "section": "Week 3",
    "text": "Week 3\n\nMonday\n\n\n\nWednesday",
    "crumbs": [
      "Supplementary",
      "Lecture recordings"
    ]
  },
  {
    "objectID": "supplementary/recordings2025.html#week-4",
    "href": "supplementary/recordings2025.html#week-4",
    "title": "Lecture recordings",
    "section": "Week 4",
    "text": "Week 4\n\nMonday\n\n\n\n\nWednesday",
    "crumbs": [
      "Supplementary",
      "Lecture recordings"
    ]
  },
  {
    "objectID": "supplementary/recordings2025.html#week-5",
    "href": "supplementary/recordings2025.html#week-5",
    "title": "Lecture recordings",
    "section": "Week 5",
    "text": "Week 5\n\nMonday\n\n\n\n\nWednesday",
    "crumbs": [
      "Supplementary",
      "Lecture recordings"
    ]
  },
  {
    "objectID": "supplementary/recordings2025.html#week-6",
    "href": "supplementary/recordings2025.html#week-6",
    "title": "Lecture recordings",
    "section": "Week 6",
    "text": "Week 6\n\nMonday\n\n\n\nWednesday",
    "crumbs": [
      "Supplementary",
      "Lecture recordings"
    ]
  },
  {
    "objectID": "supplementary/recordings2025.html#week-7",
    "href": "supplementary/recordings2025.html#week-7",
    "title": "Lecture recordings",
    "section": "Week 7",
    "text": "Week 7\n\nMonday\n\n\n\nWednesday",
    "crumbs": [
      "Supplementary",
      "Lecture recordings"
    ]
  },
  {
    "objectID": "supplementary/recordings2025.html#week-8",
    "href": "supplementary/recordings2025.html#week-8",
    "title": "Lecture recordings",
    "section": "Week 8",
    "text": "Week 8\n\nMonday\n\n\n\nWednesday",
    "crumbs": [
      "Supplementary",
      "Lecture recordings"
    ]
  },
  {
    "objectID": "supplementary/recordings2025.html#week-9",
    "href": "supplementary/recordings2025.html#week-9",
    "title": "Lecture recordings",
    "section": "Week 9",
    "text": "Week 9\n\nMonday\n\n\n\nWednesday",
    "crumbs": [
      "Supplementary",
      "Lecture recordings"
    ]
  },
  {
    "objectID": "supplementary/recordings2025.html#week-10",
    "href": "supplementary/recordings2025.html#week-10",
    "title": "Lecture recordings",
    "section": "Week 10",
    "text": "Week 10\n\nMonday\n\n\n\nWednesday",
    "crumbs": [
      "Supplementary",
      "Lecture recordings"
    ]
  },
  {
    "objectID": "supplementary/recordings2025.html#week-11",
    "href": "supplementary/recordings2025.html#week-11",
    "title": "Lecture recordings",
    "section": "Week 11",
    "text": "Week 11\n\nMonday\n\n\n\nWednesday",
    "crumbs": [
      "Supplementary",
      "Lecture recordings"
    ]
  },
  {
    "objectID": "supplementary/recordings2025.html#week-12",
    "href": "supplementary/recordings2025.html#week-12",
    "title": "Lecture recordings",
    "section": "Week 12",
    "text": "Week 12\n\nMonday\n\n\n\nWednesday",
    "crumbs": [
      "Supplementary",
      "Lecture recordings"
    ]
  },
  {
    "objectID": "supplementary/recordings2025.html#week-13",
    "href": "supplementary/recordings2025.html#week-13",
    "title": "Lecture recordings",
    "section": "Week 13",
    "text": "Week 13\n\nMonday",
    "crumbs": [
      "Supplementary",
      "Lecture recordings"
    ]
  },
  {
    "objectID": "supplementary/slides2024.html",
    "href": "supplementary/slides2024.html",
    "title": "Lecture slides",
    "section": "",
    "text": "NB: You need to be logged into Brightspace to download the slides.\n\nWeek 1\n\nMonday\nWednesday\n\nWeek 2\n\nMonday\nWednesday\n\nWeek 3\n\nMonday\nWednesday\n\nWeek 4\n\nMonday\nWednesday\n\nWeek 5\n\nMonday\nWednesday\n\nWeek 6\n\nMonday\nWednesday\n\nWeek 7\n\nMonday\nWednesday\n\nWeek 8\n\nMonday\nThe Wednesday lecture was canceled.\n\nWeek 9\n\nMonday\nWednesday\n\nWeek 10\n\nMonday\nWednesday\n\nWeek 11\n\nMonday\nWednesday\n\nWeek 12\n\nMonday\nWednesday\n\nWeek 13\n\nMonday\nWednesday\n\nWeek 14\n\nMonday\nWednesday",
    "crumbs": [
      "Supplementary",
      "Lecture slides"
    ]
  },
  {
    "objectID": "supplementary/bioinformatics_resources.html",
    "href": "supplementary/bioinformatics_resources.html",
    "title": "Databases and resources",
    "section": "",
    "text": "Knowledege data bases",
    "crumbs": [
      "Supplementary",
      "Databases and resources"
    ]
  },
  {
    "objectID": "supplementary/bioinformatics_resources.html#do-it-yourself",
    "href": "supplementary/bioinformatics_resources.html#do-it-yourself",
    "title": "Databases and resources",
    "section": "Do it yourself:",
    "text": "Do it yourself:",
    "crumbs": [
      "Supplementary",
      "Databases and resources"
    ]
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Mandatory assignments are handed in through assignments in Brightspace\nInformation about assignments can be found under “Schedule” in the menu bar.",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "exam.html",
    "href": "exam.html",
    "title": "Exam info",
    "section": "",
    "text": "Download the course homepage and Python documentation\nDownload the HTML course website for offline viewing at the exam (unzip once downloaded and double-click index.html).\nDownload the HTML Python documentation for offline viewing at the exam (unzip once downloaded and double-click index.html).",
    "crumbs": [
      "Exam info"
    ]
  },
  {
    "objectID": "exam.html#example-of-exam-assignment",
    "href": "exam.html#example-of-exam-assignment",
    "title": "Exam info",
    "section": "Example of exam assignment",
    "text": "Example of exam assignment\nThere is no test script for this assignment. You need a data file for this assignment, which you can also get below. Your exam assignment will not need any data from input files.",
    "crumbs": [
      "Exam info"
    ]
  },
  {
    "objectID": "exam.html#about-the-written-exam",
    "href": "exam.html#about-the-written-exam",
    "title": "Exam info",
    "section": "About the written exam",
    "text": "About the written exam\n\nInformationen på denne side fremgår også af eksamensopgaven.\n\nEksamensopgave består af to dele, som vægtes lige i bedømmelsen:\n\nEt sæt programmeringsopgaver.\nEt sæt bioinformatikopgaver.",
    "crumbs": [
      "Exam info"
    ]
  },
  {
    "objectID": "exam.html#filer-til-brug-ved-eksamen",
    "href": "exam.html#filer-til-brug-ved-eksamen",
    "title": "Exam info",
    "section": "Filer til brug ved eksamen",
    "text": "Filer til brug ved eksamen\nUdover denne PDF-fil som indeholder eksamensopgaverne, har du også downloaded tre andre filer fra eksamenssystemet, som du skal bruge til at løse eksamensopgaven:\n\nprogexam.py: Det er i denne fil, du skal skrive de Python funktioner, der bedes om i eksamensopgavens programmeringsdel.\ntest_progexam.py: Det er denne fil, du kan bruge til at teste de funktioner du skriver i progexam.py.\nbioinfexam.py: Det er i denne fil, du skriver svarene på eksamensopgavens bioinformatik-del.",
    "crumbs": [
      "Exam info"
    ]
  },
  {
    "objectID": "exam.html#sådan-løser-du-programmeringsopgaverne",
    "href": "exam.html#sådan-løser-du-programmeringsopgaverne",
    "title": "Exam info",
    "section": "Sådan løser du programmeringsopgaverne",
    "text": "Sådan løser du programmeringsopgaverne\nStart med at åbne din terminal og naviger ind i den folder, som eksamenssystemet har lavet på din computer. Der er muligt at folderens navn indeholder mellemrum. For at navigere ind i en folder der indeholder et mellemrum vha. terminalen, kan man skrive starten af folder-navnet og så trykke på Tab. Så fuldendes navnet automatisk. F.eks.: Hvis folderen hedder “Eksamen Bioinf”, kan man skrive: “cd Eksamen og så trykke Tab. Så fuldendes navnet og man kan trykke Enter.\nSom i programmeringsprojekterne fra kurset skriver du din kode i progexam.py og kører koden sådan her:\npython progexam.py\nSom i programmeringsprojekterne i kurset kan du teste din kode sådan her:\npython test_progexam.py\nTest scriptet er tilgængeligt som en hjælp til at teste din kode, men du har selv det fulde ansvar for rigtigheden af din kode.\nDet er tilladt at bruge løsninger af opgaver til at løse senere opgaver. Man må altså gerne kalde tidligere definerede funktioner inde i andre funktioner, man senere bliver bedt om at skrive.\nFølgende er afgørende for at din eksamensbesvarelse kan evalueres korrekt:\n\nHver funktion skal navngives præcis som angivet i opgaven. Funktioner der ikke er navngivet korrekt, regnes som ikke besvarede.\nDet er ikke tilladt importere kode fra andre filer, du har skrevet eller installeret. Det vil sige, at du ikke må bruge import statements i din fil.\nNår du afleverer progexam.py må den kun indeholde definitioner af de funktioner, der er beskrevet i eksamensopgaven. Al kode udenfor funktionsdefinitioner skal slettes inden du afleverer, så sørg for at teste i god tid inden aflevering, om dine funktioner stadig virker, når du sletter sådan ekstra kode.\n\nAllervigtigst: Funktioner der ikke fuldstænding opfylder opgavens beskrivelse regnes som ikke besvarede. Så sørg for at lave dine funktioner færdige, så de klarer alle tests. Hvis ingen af dine funktioner er helt rigtigt besvaret får du ingen point.",
    "crumbs": [
      "Exam info"
    ]
  },
  {
    "objectID": "exam.html#sådan-løser-du-bioinformatikopgaverne",
    "href": "exam.html#sådan-løser-du-bioinformatikopgaverne",
    "title": "Exam info",
    "section": "Sådan løser du bioinformatikopgaverne",
    "text": "Sådan løser du bioinformatikopgaverne\nBioinformatikdelen af eksamensopgaven består af et sæt af opgaver, der hver dækker et emne. Hver opgave indeholder flere delopgaver. Der er tre typer delopgaver:\n\nUdsagn der enten er sande eller falske og som skal besvares med True eller False.\nSpørgsmål der skal besvares med et tal (int eller float).\nSpørgsmål der skal besvares med en tekst streng (f.eks. 'Dette er mit bedste svar')\n\nFilen bioinfexam.py er en Python fil og indeholder en variabel for hver delopgave. For eksempel: den variabel der hører til delopgave tre i emne syv hedder emne_7_del_3. Hver variabel har en default værdi som enten er None eller en tom streng (' '):\nemne_7_del_3 = None\nemne_7_del_4 = ' '\n\nDu besvarer sandt/falsk udsagn ved at udskifte None med enten True eller False.\nDu besvarer tal-spørgsmål ved at udskifte None med et tal.\nDu besvarer tekst-opgaver ved at fylde tekst i den tomme streng.\n\nDet er anført i bioinfexam.py om en delopgave skal besvares med True/False, et tal, eller tekst.\nFølgende er afgørende for at din eksamensbesvarelse kan evalueres korrekt: Delopgaver som ikke er besvaret betragtes som forkert besvarede, så du er bedst tjent med at gætte fremfor ikke at svare.\nI nogle af opgaveemnerne refererer statements til en vist illustration. Alt efter størrelsen på din skærm kan du være nødt til at “zoome ind” på illustrationerne i det program du bruger til at vise denne PDF, ellers kan der være detaljer du ikke kan se.",
    "crumbs": [
      "Exam info"
    ]
  },
  {
    "objectID": "exam.html#sådan-afleverer-du-din-eksamensopgave-i-wiseflow",
    "href": "exam.html#sådan-afleverer-du-din-eksamensopgave-i-wiseflow",
    "title": "Exam info",
    "section": "Sådan afleverer du din eksamensopgave i Wiseflow",
    "text": "Sådan afleverer du din eksamensopgave i Wiseflow\nInden du afleverer, skal du tjekke at progexam.py kun indeholder definitioner af de funktioner der er beskrevet i eksamensopgaven. Hvis du har skrevet yderligere kode for at teste dine funktioner, skal du slette den inden du oplader din fil.\nDu afleverer din eksamensbesvarelse ved at uploade disse to filer til Wiseflow:\n\nprogexam.py skal afleveres som hoveddokument\nbioinfexam.py skal afleveres som bilag.",
    "crumbs": [
      "Exam info"
    ]
  },
  {
    "objectID": "chapters/trouble_shooting.html",
    "href": "chapters/trouble_shooting.html",
    "title": "Trouble shooting",
    "section": "",
    "text": "Hej Kasper\nJeg følger bioinformatik og programmering og havde problemer med at kunne køre scripts i powershell.\nLøsningen er at køre powershell som admin og tjekke execution policy:\nGet-ExecutionPolicy\nOg hvis den er restricted bruge\nSet-ExecutionPolicy -Scope CurrentUser\nUnrestricted\nOg vælge Y - man er helt sikker på man godt tør.",
    "crumbs": [
      "Trouble shooting"
    ]
  }
]